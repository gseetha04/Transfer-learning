{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gseetha04/Transfer-learning/blob/main/Transferlearning_experiments_spanBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLswPG1ou122"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############Fine-tune on SCITE and test on SCITE####################################"
      ],
      "metadata": {
        "id": "ucW7_lDSu61K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqszTLDrvc0-",
        "outputId": "2cb21067-cdea-4458-af3b-3c497f6197c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 15 03:55:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qd_BAiBvdTa",
        "outputId": "61248972-01a2-4de1-9772-a41a1c9597ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 170237, done.\u001b[K\n",
            "remote: Counting objects: 100% (453/453), done.\u001b[K\n",
            "remote: Compressing objects: 100% (241/241), done.\u001b[K\n",
            "remote: Total 170237 (delta 238), reused 348 (delta 179), pack-reused 169784\u001b[K\n",
            "Receiving objects: 100% (170237/170237), 170.00 MiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (128681/128681), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/transformers/examples/pytorch/token-classification/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGufvXENvfei",
        "outputId": "6c322f7e-fa5e-4841-e1fd-fd6c4b4ad1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers/examples/pytorch/token-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzaDiLHnvhk-",
        "outputId": "2b2d0b2e-b09f-41bf-eeb7-8b39cdbe4c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
            "  Downloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, responses, multiprocess, huggingface-hub, datasets, evaluate\n",
            "Successfully installed datasets-2.14.7 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.19.3 multiprocess-0.70.15 pyarrow-hotfix-0.5 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQkyirDovnHw",
        "outputId": "46b27f4e-b19a-4622-c391-6038db0d5502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-2yt88jhg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-2yt88jhg\n",
            "  Resolved https://github.com/huggingface/transformers to commit 2e72bbab2cd169903b1e77b439718c1bdc5d50b2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0.dev0)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.36.0.dev0)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8000870 sha256=83415dfc00663643d709e10589fbfb0e1b625b781e6a5530e91b7993797f4dfc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cqotn8gh/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: safetensors, tokenizers, transformers\n",
            "Successfully installed safetensors-0.4.0 tokenizers-0.15.0 transformers-4.36.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwZlDx2gvrWt",
        "outputId": "a93f4fee-7362-464a-e210-1f4a169d9b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=714e994fb874da5ff55600fe6315ec52ced558d03f42cb238d181603824fe937\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV8G1fz2xz-M",
        "outputId": "d8035922-2769-4d35-e69e-15091fba3bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LQjQZpFJvVnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_excel('/content/train_data.xlsx',sheet_name = \"Sheet5\")\n",
        "data_train.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "pZxSaDkevA8e",
        "outputId": "3d616833-0e31-4c6b-83af-e48ec8399d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentences  \\\n",
              "0  [\"A\",\"rare\",\"and\",\"incurable\",\"congenital\",\"di...   \n",
              "1  [\"The\",\"soft\",\"brown\",\"and\",\"beige\",\"tones\",\"i...   \n",
              "2  [\"The\",\"JFK\",\"baggage\",\"system\",\"malfunction\",...   \n",
              "3  [\"Cobalt\",\"metal\",\"fume\",\"and\",\"dust\",\"cause\",...   \n",
              "\n",
              "                                           Bio_label  \n",
              "0  [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"E\",\"O\",\"O\",\"...  \n",
              "1  [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"O\",\"O\",\"E\",\"...  \n",
              "2  [\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...  \n",
              "3  [\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\"]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64c876c2-e882-4daa-bf41-bf04bb57c353\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>Bio_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"A\",\"rare\",\"and\",\"incurable\",\"congenital\",\"di...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"E\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"The\",\"soft\",\"brown\",\"and\",\"beige\",\"tones\",\"i...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"O\",\"O\",\"E\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"The\",\"JFK\",\"baggage\",\"system\",\"malfunction\",...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"Cobalt\",\"metal\",\"fume\",\"and\",\"dust\",\"cause\",...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\"]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64c876c2-e882-4daa-bf41-bf04bb57c353')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-64c876c2-e882-4daa-bf41-bf04bb57c353 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-64c876c2-e882-4daa-bf41-bf04bb57c353');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-31c60207-4726-4e6f-a076-eec2c3cbd3a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31c60207-4726-4e6f-a076-eec2c3cbd3a5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-31c60207-4726-4e6f-a076-eec2c3cbd3a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_train[[\"sentences\", \"Bio_label\"]].rename(columns={\"sentences\": \"A\", \"Bio_label\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw_train, y_raw_train = [json.loads(tokens_test) for tokens_test in dataset_test.A.values], [json.loads(labels_test) for labels_test in dataset_test.B.values]"
      ],
      "metadata": {
        "id": "Lejc8Zi4wPtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "RANDOM_SEED = 42\n",
        "# Our data is split into sentences\n",
        "\n",
        "\n",
        "# FIXME: we also need test data!\n",
        "X_train_raw, X_dev_raw, y_train_raw, y_dev_raw = train_test_split(X_raw_train, y_raw_train, test_size=.2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "0XjFWbNxw4zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_train_raw,y_train_raw):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "v8c9eSglw_al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners1 = []\n",
        "for i,j in zip(X_dev_raw,y_dev_raw):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners1.append(_new_ner)"
      ],
      "metadata": {
        "id": "eSVQ7HoPxCIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_json_lines(jsons, output_dir):\n",
        "    print(output_dir)\n",
        "    with open(output_dir, 'w') as outfile:\n",
        "        for entry in jsons:\n",
        "            json.dump(entry, outfile)\n",
        "            outfile.write('\\n')"
      ],
      "metadata": {
        "id": "MVLuByACxIRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(train,  '/content/train.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j45DApJvxJJW",
        "outputId": "a5e0d029-62c9-4087-f90a-dfb760e137a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev = [new_ners1[i] for i in range(len(new_ners1))]\n",
        "dump_json_lines(dev,  '/content/dev.json')"
      ],
      "metadata": {
        "id": "aQmTVTvIxM1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe303a22-1514-4a2c-ea17-f7e036159da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eykfPAbIxSk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_excel('/content/test_data.xlsx',sheet_name = \"Sheet3\")\n",
        "data_test.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "sLC85r5WvSam",
        "outputId": "86fee3db-f795-49fb-ec0d-3db37587feac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentences  \\\n",
              "0  [\"Various\",\"hormonal\",\",\",\"bacterial\",\"and\",\"i...   \n",
              "1  [\"A\",\"stereo\",\"buss\",\"outputs\",\"the\",\"stereo\",...   \n",
              "2  [\"The\",\"incoming\",\"water\",\"caused\",\"a\",\"stain\"...   \n",
              "3  [\"The\",\"genreal\",\"anesthetic\",\"cause\",\"unconsc...   \n",
              "\n",
              "                                           Bio_label  \n",
              "0              [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\"]  \n",
              "1  [\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"E\",\"E\",\"O\",\"C\",\"C\",\"...  \n",
              "2  [\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\",\"...  \n",
              "3  [\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff4556d7-a650-4844-bcb6-4a6f6f490578\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>Bio_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"Various\",\"hormonal\",\",\",\"bacterial\",\"and\",\"i...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"A\",\"stereo\",\"buss\",\"outputs\",\"the\",\"stereo\",...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"E\",\"E\",\"O\",\"C\",\"C\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"The\",\"incoming\",\"water\",\"caused\",\"a\",\"stain\"...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"The\",\"genreal\",\"anesthetic\",\"cause\",\"unconsc...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff4556d7-a650-4844-bcb6-4a6f6f490578')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff4556d7-a650-4844-bcb6-4a6f6f490578 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff4556d7-a650-4844-bcb6-4a6f6f490578');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-296b9e3e-b743-4ec8-9a6b-4cbc81c2fae2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-296b9e3e-b743-4ec8-9a6b-4cbc81c2fae2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-296b9e3e-b743-4ec8-9a6b-4cbc81c2fae2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_test[[\"sentences\", \"Bio_label\"]].rename(columns={\"sentences\": \"A\", \"Bio_label\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw_test, y_raw_test = [json.loads(tokens_test) for tokens_test in dataset_test.A.values], [json.loads(labels_test) for labels_test in dataset_test.B.values]"
      ],
      "metadata": {
        "id": "vmeDY7JivvDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_raw_test,y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "JdCPgbcKv8dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8-K5VCjwBrM",
        "outputId": "ebcbd73b-89d7-491c-b13e-f04594bc3d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased   \\\n",
        "           --train_file /content/train.json  \\\n",
        "           --validation_file /content/dev.json  \\\n",
        "           --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqtUS09pwLsP",
        "outputId": "7e3adc01-6de5-4b7c-abcd-bb6727ebf43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-31 16:30:24.382763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/31/2023 16:30:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/31/2023 16:30:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Jul31_16-30-26_0dd88bc1aaee,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-b0eb2745653d22a4\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Using custom data configuration default-b0eb2745653d22a4\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "07/31/2023 16:30:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 2596.56it/s]\n",
            "Downloading took 0.0 min\n",
            "07/31/2023 16:30:27 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "07/31/2023 16:30:27 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2270.46it/s]\n",
            "Generating train split\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 696 examples [00:00, 36911.23 examples/s]\n",
            "Generating validation split\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 175 examples [00:00, 91716.01 examples/s]\n",
            "Generating test split\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 191 examples [00:00, 98853.91 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "07/31/2023 16:30:27 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "07/31/2023 16:30:27 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading (…)lve/main/config.json: 100% 414/414 [00:00<00:00, 2.39MB/s]\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 16:30:28,412 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 16:30:28,416 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:523] 2023-07-31 16:30:28,642 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 16:30:28,866 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 16:30:28,867 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 508kB/s]\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 16:30:30,670 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 16:30:30,671 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 16:30:30,671 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 16:30:30,671 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 16:30:30,671 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 16:30:30,671 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 16:30:30,672 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 16:30:30,695 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 16:30:30,696 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 665M/665M [00:02<00:00, 305MB/s]\n",
            "[INFO|modeling_utils.py:2638] 2023-07-31 16:30:33,716 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3370] 2023-07-31 16:30:39,152 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3372] 2023-07-31 16:30:39,153 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/696 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2488] 2023-07-31 16:30:39,211 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45a79f29e37ba962.arrow\n",
            "07/31/2023 16:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45a79f29e37ba962.arrow\n",
            "Running tokenizer on train dataset: 100% 696/696 [00:00<00:00, 2808.20 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/175 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-db38f16bdc6b6d92.arrow\n",
            "07/31/2023 16:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-db38f16bdc6b6d92.arrow\n",
            "Running tokenizer on validation dataset: 100% 175/175 [00:00<00:00, 2883.94 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/191 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4f244a8b012ce84b.arrow\n",
            "07/31/2023 16:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0eb2745653d22a4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4f244a8b012ce84b.arrow\n",
            "Running tokenizer on prediction dataset: 100% 191/191 [00:00<00:00, 2696.73 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 24.1MB/s]\n",
            "[INFO|trainer.py:749] 2023-07-31 16:30:46,294 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1681] 2023-07-31 16:30:46,307 >> ***** Running training *****\n",
            "[INFO|trainer.py:1682] 2023-07-31 16:30:46,307 >>   Num examples = 696\n",
            "[INFO|trainer.py:1683] 2023-07-31 16:30:46,307 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1684] 2023-07-31 16:30:46,307 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1687] 2023-07-31 16:30:46,307 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1688] 2023-07-31 16:30:46,307 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1689] 2023-07-31 16:30:46,307 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1690] 2023-07-31 16:30:46,308 >>   Number of trainable parameters = 332,533,764\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:284] 2023-07-31 16:30:46,317 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.1007, 'learning_rate': 3.106060606060606e-05, 'epoch': 11.36}\n",
            " 38% 500/1320 [04:07<06:45,  2.02it/s][INFO|trainer.py:2809] 2023-07-31 16:34:53,777 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-500\n",
            "[INFO|configuration_utils.py:460] 2023-07-31 16:34:53,779 >> Configuration saved in /content/test-ner-2022/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-07-31 16:34:58,422 >> Model weights saved in /content/test-ner-2022/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-07-31 16:34:58,423 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-07-31 16:34:58,423 >> Special tokens file saved in /content/test-ner-2022/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.009, 'learning_rate': 1.2121212121212122e-05, 'epoch': 22.73}\n",
            " 76% 1000/1320 [08:44<02:29,  2.14it/s][INFO|trainer.py:2809] 2023-07-31 16:39:30,348 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1000\n",
            "[INFO|configuration_utils.py:460] 2023-07-31 16:39:30,349 >> Configuration saved in /content/test-ner-2022/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-07-31 16:39:39,129 >> Model weights saved in /content/test-ner-2022/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-07-31 16:39:39,131 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-07-31 16:39:39,131 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1000/special_tokens_map.json\n",
            "100% 1320/1320 [11:43<00:00,  1.97it/s][INFO|trainer.py:1929] 2023-07-31 16:42:29,619 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 703.4276, 'train_samples_per_second': 29.683, 'train_steps_per_second': 1.877, 'train_loss': 0.04220154935663396, 'epoch': 30.0}\n",
            "100% 1320/1320 [11:43<00:00,  1.88it/s]\n",
            "[INFO|trainer.py:2809] 2023-07-31 16:42:29,745 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:460] 2023-07-31 16:42:29,746 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-07-31 16:42:39,898 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-07-31 16:42:39,899 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-07-31 16:42:39,899 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0422\n",
            "  train_runtime            = 0:11:43.42\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =     29.683\n",
            "  train_steps_per_second   =      1.877\n",
            "07/31/2023 16:42:39 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:749] 2023-07-31 16:42:39,944 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3083] 2023-07-31 16:42:39,966 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3085] 2023-07-31 16:42:39,966 >>   Num examples = 175\n",
            "[INFO|trainer.py:3088] 2023-07-31 16:42:39,966 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 16.66it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 15.63it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9676\n",
            "  eval_f1                 =     0.9274\n",
            "  eval_loss               =     0.1637\n",
            "  eval_precision          =     0.9169\n",
            "  eval_recall             =     0.9381\n",
            "  eval_runtime            = 0:00:01.55\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    112.627\n",
            "  eval_steps_per_second   =     14.159\n",
            "07/31/2023 16:42:41 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:749] 2023-07-31 16:42:41,522 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3083] 2023-07-31 16:42:41,523 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3085] 2023-07-31 16:42:41,523 >>   Num examples = 191\n",
            "[INFO|trainer.py:3088] 2023-07-31 16:42:41,523 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 14.06it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9724\n",
            "  predict_f1                 =     0.9469\n",
            "  predict_loss               =      0.149\n",
            "  predict_precision          =     0.9591\n",
            "  predict_recall             =      0.935\n",
            "  predict_runtime            = 0:00:01.75\n",
            "  predict_samples_per_second =    108.905\n",
            "  predict_steps_per_second   =     13.684\n",
            "[INFO|modelcard.py:452] 2023-07-31 16:42:43,659 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9168765743073047}, {'name': 'Recall', 'type': 'recall', 'value': 0.9381443298969072}, {'name': 'F1', 'type': 'f1', 'value': 0.927388535031847}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9676074406670943}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])"
      ],
      "metadata": {
        "id": "56obozYTyYDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "YNJBYAqUzeDb",
        "outputId": "50e767a4-d61b-4ff6-f5ed-5702a2159eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 preds\n",
              "0                                    C C C C C C C O E\n",
              "1                    C C C O E E E E O O C C O E E E E\n",
              "2    C C C O E E O O O O O O O O O O O O O O O O O ...\n",
              "3                      C C C O E O E O O O O O O O O O\n",
              "4                                C C C O O E O E O O O\n",
              "..                                                 ...\n",
              "186  O O O O O O O O O O O O O O O O O O O O C C O ...\n",
              "187  O O O O O O O O O O O O O O O O O O O O E E O ...\n",
              "188  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
              "189  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
              "190  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
              "\n",
              "[191 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-487854c6-3810-4e34-a01e-75e4835c1aa6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C C C C C C C O E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C C C O E E E E O O C C O E E E E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C C C O E E O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C C C O E O E O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C C C O O E O E O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O C C O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O E E O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-487854c6-3810-4e34-a01e-75e4835c1aa6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-703a71c8-81bb-433e-9c62-c73502aee3e3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-703a71c8-81bb-433e-9c62-c73502aee3e3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-703a71c8-81bb-433e-9c62-c73502aee3e3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-487854c6-3810-4e34-a01e-75e4835c1aa6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-487854c6-3810-4e34-a01e-75e4835c1aa6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))"
      ],
      "metadata": {
        "id": "QI7H9c0P2OvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = {'preds':data_preds}"
      ],
      "metadata": {
        "id": "9FlYI_Bk0ZkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibi6jLuR0aNH",
        "outputId": "e50c3850-a1ac-4214-ea0c-087fa0af3b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'preds':                                                  preds\n",
              " 0                                    C,C,C,C,C,C,C,O,E\n",
              " 1                    C,C,C,O,E,E,E,E,O,O,C,C,O,E,E,E,E\n",
              " 2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
              " 3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
              " 4                                C,C,C,O,O,E,O,E,O,O,O\n",
              " ..                                                 ...\n",
              " 186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
              " 187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
              " 188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
              " 189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
              " 190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
              " \n",
              " [191 rows x 1 columns]}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "FH_yE3JY0cyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ukw2b2jw6Qtv",
        "outputId": "8171198d-9ba2-49ca-d72e-b08771a3d4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [Various, hormonal, ,, bacterial, and, inflamm...   \n",
              "1    [A, stereo, buss, outputs, the, stereo, buss, ...   \n",
              "2    [The, incoming, water, caused, a, stain, on, t...   \n",
              "3    [The, genreal, anesthetic, cause, unconsciousn...   \n",
              "4    [The, alleged, abuse, resulted, in, bruises, a...   \n",
              "..                                                 ...   \n",
              "186  [Thus, ,, evaluating, capital, punishment, as,...   \n",
              "187  [About, 30, ducks, were, found, dead, in, Klam...   \n",
              "188  [He, created, and, advocated, -, flower, power...   \n",
              "189  [Method, according, to, claim, 1, ,, character...   \n",
              "190  [After, the, war, ,, as, the, Midway, was, pre...   \n",
              "\n",
              "                                                   ner  \n",
              "0                          [C, C, C, C, C, C, C, O, E]  \n",
              "1    [C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...  \n",
              "2    [C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...  \n",
              "3     [C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]  \n",
              "4                    [C, C, C, O, O, E, O, E, O, O, O]  \n",
              "..                                                 ...  \n",
              "186  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "187  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "188  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "189  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "190  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "\n",
              "[191 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-5a129eb4-9b20-4e02-8dbd-4b59a1988c25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Various, hormonal, ,, bacterial, and, inflamm...</td>\n",
              "      <td>[C, C, C, C, C, C, C, O, E]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, stereo, buss, outputs, the, stereo, buss, ...</td>\n",
              "      <td>[C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, incoming, water, caused, a, stain, on, t...</td>\n",
              "      <td>[C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, genreal, anesthetic, cause, unconsciousn...</td>\n",
              "      <td>[C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, alleged, abuse, resulted, in, bruises, a...</td>\n",
              "      <td>[C, C, C, O, O, E, O, E, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>[Thus, ,, evaluating, capital, punishment, as,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>[About, 30, ducks, were, found, dead, in, Klam...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>[He, created, and, advocated, -, flower, power...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>[Method, according, to, claim, 1, ,, character...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>[After, the, war, ,, as, the, Midway, was, pre...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a129eb4-9b20-4e02-8dbd-4b59a1988c25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-738da927-6ada-4c42-82ab-a3e7da37031d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-738da927-6ada-4c42-82ab-a3e7da37031d')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-738da927-6ada-4c42-82ab-a3e7da37031d button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a129eb4-9b20-4e02-8dbd-4b59a1988c25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a129eb4-9b20-4e02-8dbd-4b59a1988c25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_final = pd.concat([data_test,data_preds],axis=1)"
      ],
      "metadata": {
        "id": "OPwlUJbS0f0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sGQ1wVYk0i8G",
        "outputId": "68b61eaf-2ae3-4b94-d2ed-b14560e2dbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [Various, hormonal, ,, bacterial, and, inflamm...   \n",
              "1    [A, stereo, buss, outputs, the, stereo, buss, ...   \n",
              "2    [The, incoming, water, caused, a, stain, on, t...   \n",
              "3    [The, genreal, anesthetic, cause, unconsciousn...   \n",
              "4    [The, alleged, abuse, resulted, in, bruises, a...   \n",
              "..                                                 ...   \n",
              "186  [Thus, ,, evaluating, capital, punishment, as,...   \n",
              "187  [About, 30, ducks, were, found, dead, in, Klam...   \n",
              "188  [He, created, and, advocated, -, flower, power...   \n",
              "189  [Method, according, to, claim, 1, ,, character...   \n",
              "190  [After, the, war, ,, as, the, Midway, was, pre...   \n",
              "\n",
              "                                                   ner  \\\n",
              "0                          [C, C, C, C, C, C, C, O, E]   \n",
              "1    [C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...   \n",
              "2    [C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...   \n",
              "3     [C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]   \n",
              "4                    [C, C, C, O, O, E, O, E, O, O, O]   \n",
              "..                                                 ...   \n",
              "186  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "187  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "188  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "189  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "190  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "\n",
              "                                                 preds  \n",
              "0                                    C,C,C,C,C,C,C,O,E  \n",
              "1                    C,C,C,O,E,E,E,E,O,O,C,C,O,E,E,E,E  \n",
              "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
              "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
              "..                                                 ...  \n",
              "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
              "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
              "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "\n",
              "[191 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-7263b435-c66a-4fda-8b3f-489260890fe5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Various, hormonal, ,, bacterial, and, inflamm...</td>\n",
              "      <td>[C, C, C, C, C, C, C, O, E]</td>\n",
              "      <td>C,C,C,C,C,C,C,O,E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, stereo, buss, outputs, the, stereo, buss, ...</td>\n",
              "      <td>[C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...</td>\n",
              "      <td>C,C,C,O,E,E,E,E,O,O,C,C,O,E,E,E,E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, incoming, water, caused, a, stain, on, t...</td>\n",
              "      <td>[C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, genreal, anesthetic, cause, unconsciousn...</td>\n",
              "      <td>[C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, alleged, abuse, resulted, in, bruises, a...</td>\n",
              "      <td>[C, C, C, O, O, E, O, E, O, O, O]</td>\n",
              "      <td>C,C,C,O,O,E,O,E,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>[Thus, ,, evaluating, capital, punishment, as,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>[About, 30, ducks, were, found, dead, in, Klam...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>[He, created, and, advocated, -, flower, power...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>[Method, according, to, claim, 1, ,, character...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>[After, the, war, ,, as, the, Midway, was, pre...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7263b435-c66a-4fda-8b3f-489260890fe5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-2c9bc1fd-0998-4bd0-92cf-d4dbb4b03e23\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c9bc1fd-0998-4bd0-92cf-d4dbb4b03e23')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-2c9bc1fd-0998-4bd0-92cf-d4dbb4b03e23 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7263b435-c66a-4fda-8b3f-489260890fe5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7263b435-c66a-4fda-8b3f-489260890fe5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_final.to_excel('/content/spanbert_gold_preds.xlsx')"
      ],
      "metadata": {
        "id": "DIG_SkNt0k9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_spanbert_gold_preds = pd.read_excel('/content/spanbert_gold_preds.xlsx')"
      ],
      "metadata": {
        "id": "oNSoKiDf0nkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_list = [i.strip() for i in data_spanbert_gold_preds['ner'] for i in i.split(',')]"
      ],
      "metadata": {
        "id": "XiTCqaIS0rT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]"
      ],
      "metadata": {
        "id": "u3PKGOVA0uNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]"
      ],
      "metadata": {
        "id": "TDy545gn09Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.shape(flat_list))\n",
        "print(np.shape(flat_list_preds))\n",
        "print(np.shape(flat_list_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9uQJ7zU1H-w",
        "outputId": "d740f1fc-308a-4a30-8b07-74e9211fd1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3621,)\n",
            "(3621,)\n",
            "(3748,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        "df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])"
      ],
      "metadata": {
        "id": "MNlNpjHg1KKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combine = pd.concat([df_finallist_words,df_flatlist,df_finallist],axis=1)"
      ],
      "metadata": {
        "id": "-UIchDDC1Nf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "print(data_preds)\n",
        "\n",
        "preds = {'preds':data_preds}\n",
        "print(preds)\n",
        "\n",
        "data_test = pd.read_json('/content/test.json', lines = True)\n",
        "\n",
        "data_test_final = pd.concat([data_test,data_preds],axis=1)\n",
        "print(data_test_final)\n",
        "\n",
        "data_test_final.to_excel('/content/spanbert_gold_preds.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfi_VgyY9AKM",
        "outputId": "8215e793-8cb7-4172-cfa2-ec98810f666b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 preds\n",
            "0                                    C C C C C C C O E\n",
            "1                    C C C O E E E E O O C C O E E E E\n",
            "2    C C C O E E O O O O O O O O O O O O O O O O O ...\n",
            "3                      C C C O E O E O O O O O O O O O\n",
            "4                                C C C O O E O E O O O\n",
            "..                                                 ...\n",
            "186  O O O O O O O O O O O O O O O O O O O O C C O ...\n",
            "187  O O O O O O O O O O O O O O O O O O O O E E O ...\n",
            "188  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "189  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "190  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "\n",
            "[191 rows x 1 columns]\n",
            "{'preds':                                                  preds\n",
            "0                                    C C C C C C C O E\n",
            "1                    C C C O E E E E O O C C O E E E E\n",
            "2    C C C O E E O O O O O O O O O O O O O O O O O ...\n",
            "3                      C C C O E O E O O O O O O O O O\n",
            "4                                C C C O O E O E O O O\n",
            "..                                                 ...\n",
            "186  O O O O O O O O O O O O O O O O O O O O C C O ...\n",
            "187  O O O O O O O O O O O O O O O O O O O O E E O ...\n",
            "188  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "189  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "190  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                                 words  \\\n",
            "0    [Various, hormonal, ,, bacterial, and, inflamm...   \n",
            "1    [A, stereo, buss, outputs, the, stereo, buss, ...   \n",
            "2    [The, incoming, water, caused, a, stain, on, t...   \n",
            "3    [The, genreal, anesthetic, cause, unconsciousn...   \n",
            "4    [The, alleged, abuse, resulted, in, bruises, a...   \n",
            "..                                                 ...   \n",
            "186  [Thus, ,, evaluating, capital, punishment, as,...   \n",
            "187  [About, 30, ducks, were, found, dead, in, Klam...   \n",
            "188  [He, created, and, advocated, -, flower, power...   \n",
            "189  [Method, according, to, claim, 1, ,, character...   \n",
            "190  [After, the, war, ,, as, the, Midway, was, pre...   \n",
            "\n",
            "                                                   ner  \\\n",
            "0                          [C, C, C, C, C, C, C, O, E]   \n",
            "1    [C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...   \n",
            "2    [C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...   \n",
            "3     [C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]   \n",
            "4                    [C, C, C, O, O, E, O, E, O, O, O]   \n",
            "..                                                 ...   \n",
            "186  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
            "187  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
            "188  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
            "189  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
            "190  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C C C C C C C O E  \n",
            "1                    C C C O E E E E O O C C O E E E E  \n",
            "2    C C C O E E O O O O O O O O O O O O O O O O O ...  \n",
            "3                      C C C O E O E O O O O O O O O O  \n",
            "4                                C C C O O E O E O O O  \n",
            "..                                                 ...  \n",
            "186  O O O O O O O O O O O O O O O O O O O O C C O ...  \n",
            "187  O O O O O O O O O O O O O O O O O O O O E E O ...  \n",
            "188  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "189  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "190  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "\n",
            "[191 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_spanbert_gold_preds = pd.read_excel('/content/spanbert_gold_preds.xlsx')\n",
        "\n",
        "print(data_spanbert_gold_preds)\n",
        "\n",
        "flat_list = [i.strip() for i in data_spanbert_gold_preds['ner'] for i in i.split(',')]\n",
        "flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(' ')]\n",
        "flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "import numpy as np\n",
        "print(np.shape(flat_list))\n",
        "print(np.shape(flat_list_preds))\n",
        "print(np.shape(flat_list_words))\n",
        "\n",
        "df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        "df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "df_combine = pd.concat([df_finallist_words,df_flatlist,df_finallist],axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YldKQflf_QQe",
        "outputId": "d893485d-2c43-4419-c64d-04ed1ed2fe12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Unnamed: 0                                              words  \\\n",
            "0             0  ['Various', 'hormonal', ',', 'bacterial', 'and...   \n",
            "1             1  ['A', 'stereo', 'buss', 'outputs', 'the', 'ste...   \n",
            "2             2  ['The', 'incoming', 'water', 'caused', 'a', 's...   \n",
            "3             3  ['The', 'genreal', 'anesthetic', 'cause', 'unc...   \n",
            "4             4  ['The', 'alleged', 'abuse', 'resulted', 'in', ...   \n",
            "..          ...                                                ...   \n",
            "186         186  ['Thus', ',', 'evaluating', 'capital', 'punish...   \n",
            "187         187  ['About', '30', 'ducks', 'were', 'found', 'dea...   \n",
            "188         188  ['He', 'created', 'and', 'advocated', '-', 'fl...   \n",
            "189         189  ['Method', 'according', 'to', 'claim', '1', ',...   \n",
            "190         190  ['After', 'the', 'war', ',', 'as', 'the', 'Mid...   \n",
            "\n",
            "                                                   ner  \\\n",
            "0                            C, C, C, C, C, C, C, O, E   \n",
            "1    C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, E, E   \n",
            "2    C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, O...   \n",
            "3       C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O   \n",
            "4                      C, C, C, O, O, E, O, E, O, O, O   \n",
            "..                                                 ...   \n",
            "186  O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
            "187  O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
            "188  O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
            "189  O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
            "190  O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C C C C C C C O E  \n",
            "1                    C C C O E E E E O O C C O E E E E  \n",
            "2    C C C O E E O O O O O O O O O O O O O O O O O ...  \n",
            "3                      C C C O E O E O O O O O O O O O  \n",
            "4                                C C C O O E O E O O O  \n",
            "..                                                 ...  \n",
            "186  O O O O O O O O O O O O O O O O O O O O C C O ...  \n",
            "187  O O O O O O O O O O O O O O O O O O O O E E O ...  \n",
            "188  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "189  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "190  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "\n",
            "[191 rows x 4 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "(3748,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISLDMKtk_StZ",
        "outputId": "c1c491fa-b47a-49a8-c326-ed8ea045cffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.94      0.93       465\n",
            "          CE       0.73      0.44      0.55        25\n",
            "           E       0.96      0.94      0.95       468\n",
            "           O       0.98      0.99      0.99      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.90      0.83      0.86      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########10 runs######################"
      ],
      "metadata": {
        "id": "Bn0M4v1N9BrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ZhUQgzOWit9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "P5QjHvy46d15",
        "outputId": "2b1ad562-e7ab-4804-d5c3-4375b3ef768b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [Various, hormonal, ,, bacterial, and, inflamm...   \n",
              "1    [A, stereo, buss, outputs, the, stereo, buss, ...   \n",
              "2    [The, incoming, water, caused, a, stain, on, t...   \n",
              "3    [The, genreal, anesthetic, cause, unconsciousn...   \n",
              "4    [The, alleged, abuse, resulted, in, bruises, a...   \n",
              "..                                                 ...   \n",
              "186  [Thus, ,, evaluating, capital, punishment, as,...   \n",
              "187  [About, 30, ducks, were, found, dead, in, Klam...   \n",
              "188  [He, created, and, advocated, -, flower, power...   \n",
              "189  [Method, according, to, claim, 1, ,, character...   \n",
              "190  [After, the, war, ,, as, the, Midway, was, pre...   \n",
              "\n",
              "                                                   ner  \n",
              "0                          [C, C, C, C, C, C, C, O, E]  \n",
              "1    [C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...  \n",
              "2    [C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...  \n",
              "3     [C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]  \n",
              "4                    [C, C, C, O, O, E, O, E, O, O, O]  \n",
              "..                                                 ...  \n",
              "186  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "187  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "188  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "189  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "190  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "\n",
              "[191 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a79d3867-a878-4e9f-a02e-2f9ed6c90ea9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Various, hormonal, ,, bacterial, and, inflamm...</td>\n",
              "      <td>[C, C, C, C, C, C, C, O, E]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, stereo, buss, outputs, the, stereo, buss, ...</td>\n",
              "      <td>[C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, incoming, water, caused, a, stain, on, t...</td>\n",
              "      <td>[C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, genreal, anesthetic, cause, unconsciousn...</td>\n",
              "      <td>[C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, alleged, abuse, resulted, in, bruises, a...</td>\n",
              "      <td>[C, C, C, O, O, E, O, E, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>[Thus, ,, evaluating, capital, punishment, as,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>[About, 30, ducks, were, found, dead, in, Klam...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>[He, created, and, advocated, -, flower, power...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>[Method, according, to, claim, 1, ,, character...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>[After, the, war, ,, as, the, Midway, was, pre...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a79d3867-a878-4e9f-a02e-2f9ed6c90ea9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a79d3867-a878-4e9f-a02e-2f9ed6c90ea9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a79d3867-a878-4e9f-a02e-2f9ed6c90ea9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3de8ecc9-2b3b-415d-914b-8e0141f315ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3de8ecc9-2b3b-415d-914b-8e0141f315ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3de8ecc9-2b3b-415d-914b-8e0141f315ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_542f0f8e-1cf9-4c3c-9a27-f3cbf7b08b9c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_542f0f8e-1cf9-4c3c-9a27-f3cbf7b08b9c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "wScq6MTd6vP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "QBuENF_b7MkY",
        "outputId": "733dbbca-3d08-4b25-c751-71352173dc3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [Various, hormonal, ,, bacterial, and, inflamm...   \n",
              "1    [A, stereo, buss, outputs, the, stereo, buss, ...   \n",
              "2    [The, incoming, water, caused, a, stain, on, t...   \n",
              "3    [The, genreal, anesthetic, cause, unconsciousn...   \n",
              "4    [The, alleged, abuse, resulted, in, bruises, a...   \n",
              "..                                                 ...   \n",
              "186  [Thus, ,, evaluating, capital, punishment, as,...   \n",
              "187  [About, 30, ducks, were, found, dead, in, Klam...   \n",
              "188  [He, created, and, advocated, -, flower, power...   \n",
              "189  [Method, according, to, claim, 1, ,, character...   \n",
              "190  [After, the, war, ,, as, the, Midway, was, pre...   \n",
              "\n",
              "                                                   ner  \\\n",
              "0                          [C, C, C, C, C, C, C, O, E]   \n",
              "1    [C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...   \n",
              "2    [C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...   \n",
              "3     [C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]   \n",
              "4                    [C, C, C, O, O, E, O, E, O, O, O]   \n",
              "..                                                 ...   \n",
              "186  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "187  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "188  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "189  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "190  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "\n",
              "                                            liststring  \n",
              "0                                    C,C,C,C,C,C,C,O,E  \n",
              "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E  \n",
              "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
              "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
              "..                                                 ...  \n",
              "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
              "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
              "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "\n",
              "[191 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3071a52e-5134-433f-a3f1-ceb92a9cb722\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>liststring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Various, hormonal, ,, bacterial, and, inflamm...</td>\n",
              "      <td>[C, C, C, C, C, C, C, O, E]</td>\n",
              "      <td>C,C,C,C,C,C,C,O,E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, stereo, buss, outputs, the, stereo, buss, ...</td>\n",
              "      <td>[C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...</td>\n",
              "      <td>C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, incoming, water, caused, a, stain, on, t...</td>\n",
              "      <td>[C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, genreal, anesthetic, cause, unconsciousn...</td>\n",
              "      <td>[C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, alleged, abuse, resulted, in, bruises, a...</td>\n",
              "      <td>[C, C, C, O, O, E, O, E, O, O, O]</td>\n",
              "      <td>C,C,C,O,O,E,O,E,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>[Thus, ,, evaluating, capital, punishment, as,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>[About, 30, ducks, were, found, dead, in, Klam...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>[He, created, and, advocated, -, flower, power...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>[Method, according, to, claim, 1, ,, character...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>[After, the, war, ,, as, the, Midway, was, pre...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3071a52e-5134-433f-a3f1-ceb92a9cb722')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3071a52e-5134-433f-a3f1-ceb92a9cb722 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3071a52e-5134-433f-a3f1-ceb92a9cb722');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-06bda988-73b6-4454-92b6-99f63948e1bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06bda988-73b6-4454-92b6-99f63948e1bd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-06bda988-73b6-4454-92b6-99f63948e1bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_08c22161-06ca-4399-a7f8-7ed0c3c613b0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_08c22161-06ca-4399-a7f8-7ed0c3c613b0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "hynvjNyo9brN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mGtkLtNiHar",
        "outputId": "95aa7fad-fd2d-4704-8992-019a44116793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 19:15:46.209793: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:15:46.209849: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:15:46.209890: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:15:47.743515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:15:51 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:15:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-15-51_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:15:51 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:15:51 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:15:51 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:15:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:15:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:15:51 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:15:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:15:51,819 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:15:51,827 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:15:51,921 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:15:52,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:15:52,008 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:15:52,180 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:15:52,181 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:15:52,181 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:15:52,181 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:15:52,181 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:15:52,181 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:15:52,182 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:15:52,215 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:15:52,216 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:15:52,311 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:15:58,696 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:15:58,696 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:15:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:15:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:15:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:16:02,294 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:16:08,129 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:16:20,217 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:16:20,217 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:16:20,217 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:16:20,217 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:16:20,217 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:16:20,217 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:16:20,217 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:16:20,220 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:16:20,221 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:16:20,221 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:16:20,221 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:16:20,221 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:16:20,240 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 19:19:01,781 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.6605, 'train_samples_per_second': 129.16, 'train_steps_per_second': 8.165, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.17it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:19:01,883 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:19:01,885 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:19:11,569 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:19:11,591 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:19:11,591 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.66\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =     129.16\n",
            "  train_steps_per_second   =      8.165\n",
            "11/12/2023 19:19:11 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:19:11,612 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:19:11,617 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:19:11,617 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:19:11,617 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.13it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.52it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.65\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    106.006\n",
            "  eval_steps_per_second   =     13.327\n",
            "11/12/2023 19:19:13 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:19:13,272 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:19:13,274 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:19:13,274 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:19:13,274 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 14.38it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.71\n",
            "  predict_samples_per_second =    111.305\n",
            "  predict_steps_per_second   =     13.986\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:19:15,084 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:19:24.934637: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:19:24.934694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:19:24.934732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:19:26.476050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:19:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:19:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-19-29_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:19:30 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:19:30 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:19:30 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:19:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:19:30 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:19:30 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:19:30 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:19:30,639 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:19:30,647 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:19:30,732 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:19:30,817 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:19:30,818 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:19:30,990 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:19:30,990 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:19:30,990 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:19:30,990 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:19:30,990 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:19:30,990 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:19:30,991 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:19:31,022 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:19:31,023 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:19:31,111 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:19:37,572 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:19:37,572 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:19:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:19:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:19:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:19:41,443 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:19:47,291 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:19:59,270 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:19:59,270 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:19:59,270 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:19:59,271 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:19:59,271 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:19:59,271 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:19:59,271 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:19:59,281 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:19:59,282 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:19:59,283 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:19:59,283 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:19:59,283 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:19:59,349 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 19:22:40,580 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.3955, 'train_samples_per_second': 129.372, 'train_steps_per_second': 8.179, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:22:40,680 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:22:40,681 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:22:49,012 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:22:49,014 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:22:49,014 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.39\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.372\n",
            "  train_steps_per_second   =      8.179\n",
            "11/12/2023 19:22:49 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:22:49,031 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:22:49,034 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:22:49,034 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:22:49,034 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.04it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.36it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.66\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    105.039\n",
            "  eval_steps_per_second   =     13.205\n",
            "11/12/2023 19:22:50 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:22:50,703 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:22:50,706 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:22:50,706 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:22:50,706 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 14.19it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.73\n",
            "  predict_samples_per_second =     109.94\n",
            "  predict_steps_per_second   =     13.814\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:22:52,545 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:23:02.059850: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:23:02.059902: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:23:02.059955: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:23:03.636766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:23:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:23:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-23-07_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:23:07 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:23:07 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:23:07 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:23:07 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:23:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:23:07 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:23:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:23:07,871 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:23:07,879 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:23:07,963 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:23:08,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:23:08,049 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:23:08,229 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:23:08,229 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:23:08,229 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:23:08,229 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:23:08,229 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:23:08,229 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:23:08,230 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:23:08,265 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:23:08,265 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:23:08,359 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:23:15,344 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:23:15,344 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:23:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:23:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:23:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:23:20,458 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:23:26,479 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:23:38,468 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:23:38,468 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:23:38,468 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:23:38,468 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:23:38,469 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:23:38,469 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:23:38,469 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:23:38,470 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:23:38,471 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:23:38,471 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:23:38,471 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:23:38,471 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:23:38,499 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 19:26:19,806 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.4336, 'train_samples_per_second': 129.341, 'train_steps_per_second': 8.177, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:26:19,907 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:26:19,909 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:26:30,961 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:26:30,963 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:26:30,963 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.43\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.341\n",
            "  train_steps_per_second   =      8.177\n",
            "11/12/2023 19:26:31 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:26:31,032 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:26:31,035 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:26:31,036 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:26:31,036 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.54it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.91it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.72\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    101.696\n",
            "  eval_steps_per_second   =     12.785\n",
            "11/12/2023 19:26:32 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:26:32,760 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:26:32,767 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:26:32,767 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:26:32,767 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.58it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.82\n",
            "  predict_samples_per_second =    104.931\n",
            "  predict_steps_per_second   =     13.185\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:26:34,693 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:26:43.348509: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:26:43.348565: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:26:43.348602: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:26:45.128223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:26:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:26:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-26-47_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:26:48 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:26:48 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:26:48 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:26:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:26:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:26:48 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:26:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:26:48,605 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:26:48,608 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:26:48,708 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:26:48,798 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:26:48,800 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:26:48,975 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:26:48,975 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:26:48,975 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:26:48,975 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:26:48,975 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:26:48,975 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:26:48,977 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:26:49,033 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:26:49,034 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:26:49,087 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:26:54,481 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:26:54,481 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:26:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:26:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:26:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:26:57,653 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:27:03,628 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:27:16,528 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:27:16,528 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:27:16,528 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:27:16,528 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:27:16,528 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:27:16,528 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:27:16,528 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:27:16,530 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:27:16,531 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:27:16,531 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:27:16,531 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:27:16,531 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:27:16,556 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 19:29:57,609 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.1837, 'train_samples_per_second': 129.542, 'train_steps_per_second': 8.189, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.19it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:29:57,716 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:29:57,717 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:30:07,873 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:30:07,874 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:30:07,874 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.18\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.542\n",
            "  train_steps_per_second   =      8.189\n",
            "11/12/2023 19:30:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:30:07,930 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:30:07,933 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:30:07,933 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:30:07,933 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.39it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.15it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.68\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =     103.65\n",
            "  eval_steps_per_second   =      13.03\n",
            "11/12/2023 19:30:09 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:30:09,625 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:30:09,627 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:30:09,627 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:30:09,628 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.61it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.81\n",
            "  predict_samples_per_second =    105.351\n",
            "  predict_steps_per_second   =     13.238\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:30:11,541 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:30:17.884109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:30:17.884180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:30:17.884220: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:30:19.349836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:30:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:30:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-30-21_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:30:22 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:30:22 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:30:22 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:30:22 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:30:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:30:22 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:30:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:30:22,767 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:30:22,771 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:30:22,858 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:30:22,944 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:30:22,945 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:30:23,114 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:30:23,114 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:30:23,114 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:30:23,114 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:30:23,115 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:30:23,115 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:30:23,116 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:30:23,148 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:30:23,150 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:30:23,182 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:30:35,052 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:30:35,053 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:30:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:30:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:30:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:30:37,789 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:30:44,091 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:30:57,774 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:30:57,774 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:30:57,774 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:30:57,774 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:30:57,775 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:30:57,775 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:30:57,775 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:30:57,777 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:30:57,778 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:30:57,779 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:30:57,779 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:30:57,779 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:30:57,813 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  2.00it/s][INFO|trainer.py:1956] 2023-11-12 19:33:38,963 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.2899, 'train_samples_per_second': 129.456, 'train_steps_per_second': 8.184, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:33:39,070 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:33:39,071 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:33:56,681 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:33:56,683 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:33:56,683 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.28\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.456\n",
            "  train_steps_per_second   =      8.184\n",
            "11/12/2023 19:33:56 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:33:56,758 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:33:56,761 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:33:56,761 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:33:56,761 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.86it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.20it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.68\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    103.894\n",
            "  eval_steps_per_second   =     13.061\n",
            "11/12/2023 19:33:58 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:33:58,449 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:33:58,452 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:33:58,452 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:33:58,452 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.70it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.80\n",
            "  predict_samples_per_second =    105.994\n",
            "  predict_steps_per_second   =     13.319\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:34:00,351 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:34:07.994686: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:34:07.994743: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:34:07.994792: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:34:09.471110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:34:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:34:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-34-12_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:34:12 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:34:12 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:34:12 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:34:12 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:34:12 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:34:12 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:34:12 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:34:12,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:34:12,915 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:34:13,006 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:34:13,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:34:13,109 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:34:13,304 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:34:13,304 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:34:13,305 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:34:13,305 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:34:13,305 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:34:13,305 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:34:13,306 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:34:13,338 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:34:13,339 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:34:13,372 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:34:25,135 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:34:25,135 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:34:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:34:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:34:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:34:28,943 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:34:35,313 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:34:48,049 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:34:48,049 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:34:48,049 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:34:48,049 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:34:48,049 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:34:48,049 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:34:48,049 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:34:48,052 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:34:48,053 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:34:48,053 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:34:48,053 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:34:48,053 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:34:48,085 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  2.00it/s][INFO|trainer.py:1956] 2023-11-12 19:37:29,278 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.3306, 'train_samples_per_second': 129.424, 'train_steps_per_second': 8.182, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:37:29,385 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:37:29,386 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:37:40,863 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:37:40,864 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:37:40,865 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.33\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.424\n",
            "  train_steps_per_second   =      8.182\n",
            "11/12/2023 19:37:40 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:37:40,931 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:37:40,933 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:37:40,934 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:37:40,934 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.22it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.74it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.62\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    107.715\n",
            "  eval_steps_per_second   =     13.541\n",
            "11/12/2023 19:37:42 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:37:42,561 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:37:42,562 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:37:42,563 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:37:42,563 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 14.54it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.69\n",
            "  predict_samples_per_second =    112.495\n",
            "  predict_steps_per_second   =     14.135\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:37:44,369 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:37:51.643313: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:37:51.643372: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:37:51.643412: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:37:53.660471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:37:57 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:37:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-37-57_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:37:57 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:37:57 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:37:57 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:37:57 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:37:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:37:57 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:37:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:37:57,820 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:37:57,825 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:37:57,912 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:37:57,999 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:37:58,000 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:37:58,172 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:37:58,172 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:37:58,172 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:37:58,173 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:37:58,173 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:37:58,173 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:37:58,174 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:37:58,206 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:37:58,206 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:37:58,239 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:38:11,590 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:38:11,590 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:38:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:38:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:38:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:38:15,981 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:38:22,081 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:38:34,571 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:38:34,571 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:38:34,571 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:38:34,571 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:38:34,571 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:38:34,571 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:38:34,571 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:38:34,573 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:38:34,574 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:38:34,574 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:38:34,574 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:38:34,574 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:38:34,599 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 19:41:15,618 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.1442, 'train_samples_per_second': 129.573, 'train_steps_per_second': 8.191, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.19it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:41:15,720 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:41:15,722 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:41:25,536 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:41:25,537 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:41:25,538 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.14\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.573\n",
            "  train_steps_per_second   =      8.191\n",
            "11/12/2023 19:41:25 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:41:25,599 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:41:25,601 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:41:25,601 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:41:25,601 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.33it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.74it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.62\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    107.567\n",
            "  eval_steps_per_second   =     13.523\n",
            "11/12/2023 19:41:27 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:41:27,231 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:41:27,233 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:41:27,233 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:41:27,233 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 14.23it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.73\n",
            "  predict_samples_per_second =    109.853\n",
            "  predict_steps_per_second   =     13.803\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:41:29,073 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:41:37.905619: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:41:37.905678: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:41:37.905804: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:41:39.309758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:41:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:41:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-41-41_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:41:42 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:41:42 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:41:42 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:41:42 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:41:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:41:42 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:41:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:41:42,698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:41:42,702 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:41:42,789 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:41:42,883 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:41:42,884 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:41:43,051 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:41:43,052 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:41:43,052 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:41:43,052 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:41:43,052 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:41:43,052 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:41:43,053 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:41:43,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:41:43,090 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:41:43,122 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:41:55,324 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:41:55,325 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:41:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:41:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:41:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:41:58,258 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:42:04,374 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:42:16,288 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:42:16,288 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:42:16,289 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:42:16,289 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:42:16,289 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:42:16,289 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:42:16,289 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:42:16,290 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:42:16,292 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:42:16,292 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:42:16,292 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:42:16,292 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:42:16,317 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 19:44:57,358 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.1721, 'train_samples_per_second': 129.551, 'train_steps_per_second': 8.19, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.19it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:44:57,465 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:44:57,466 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:45:07,829 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:45:07,831 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:45:07,831 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.17\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.551\n",
            "  train_steps_per_second   =       8.19\n",
            "11/12/2023 19:45:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:45:07,863 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:45:07,866 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:45:07,866 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:45:07,866 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 14.93it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.37it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.78\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =     98.027\n",
            "  eval_steps_per_second   =     12.323\n",
            "11/12/2023 19:45:09 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:45:09,657 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:45:09,660 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:45:09,660 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:45:09,660 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.78it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.96\n",
            "  predict_samples_per_second =     97.193\n",
            "  predict_steps_per_second   =     12.213\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:45:11,729 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:45:19.583596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:45:19.583657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:45:19.583722: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:45:21.199533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:45:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:45:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-45-23_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:45:24 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:45:24 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:45:24 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:45:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:45:24 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:45:24,663 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:45:24,667 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:45:24,774 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:45:24,867 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:45:24,868 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:45:25,049 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:45:25,049 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:45:25,049 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:45:25,049 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:45:25,049 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:45:25,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:45:25,050 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:45:25,085 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:45:25,086 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:45:25,120 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:45:35,907 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:45:35,907 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:45:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:45:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:45:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:45:38,738 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:45:45,195 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:45:57,081 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:45:57,081 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:45:57,081 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:45:57,081 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:45:57,081 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:45:57,081 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:45:57,081 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:45:57,083 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:45:57,084 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:45:57,084 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:45:57,084 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:45:57,084 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:45:57,109 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  2.00it/s][INFO|trainer.py:1956] 2023-11-12 19:48:38,343 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.3645, 'train_samples_per_second': 129.396, 'train_steps_per_second': 8.18, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:48:38,450 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:48:38,451 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:48:48,223 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:48:48,225 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:48:48,225 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.36\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.396\n",
            "  train_steps_per_second   =       8.18\n",
            "11/12/2023 19:48:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:48:48,256 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:48:48,259 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:48:48,259 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:48:48,259 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 14.96it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.75it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.73\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    100.814\n",
            "  eval_steps_per_second   =     12.674\n",
            "11/12/2023 19:48:49 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:48:49,999 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:48:50,002 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:48:50,003 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:48:50,003 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.54it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.82\n",
            "  predict_samples_per_second =    104.435\n",
            "  predict_steps_per_second   =     13.123\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:48:51,932 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n",
            "2023-11-12 19:48:59.095860: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 19:48:59.095919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 19:48:59.095956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 19:49:00.675450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 19:49:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 19:49:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_19-49-03_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 19:49:03 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-d44892216b787231\n",
            "11/12/2023 19:49:04 - INFO - datasets.builder - Using custom data configuration default-d44892216b787231\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 19:49:04 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 19:49:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:49:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 19:49:04 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 19:49:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:49:04,563 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:49:04,570 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 19:49:04,664 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:49:04,754 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:49:04,756 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:49:04,935 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:49:04,935 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:49:04,936 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:49:04,936 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 19:49:04,936 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:49:04,936 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:49:04,938 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 19:49:04,997 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 19:49:04,999 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 19:49:05,057 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 19:49:16,945 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 19:49:16,945 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "11/12/2023 19:49:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e9059a372bbf4af.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "11/12/2023 19:49:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a6614a90f266622b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "11/12/2023 19:49:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d44892216b787231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ade2b31d1ad00856.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 19:49:20,037 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 19:49:26,187 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 19:49:37,901 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 19:49:37,901 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 19:49:37,901 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 19:49:37,901 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 19:49:37,901 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 19:49:37,901 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 19:49:37,901 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 19:49:37,904 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 19:49:37,905 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 19:49:37,905 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 19:49:37,905 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 19:49:37,905 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 19:49:37,939 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  2.00it/s][INFO|trainer.py:1956] 2023-11-12 19:52:19,634 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.8349, 'train_samples_per_second': 129.02, 'train_steps_per_second': 8.156, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.16it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 19:52:19,741 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 19:52:19,742 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 19:52:36,440 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 19:52:36,441 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 19:52:36,441 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.83\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =     129.02\n",
            "  train_steps_per_second   =      8.156\n",
            "11/12/2023 19:52:36 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:52:36,459 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:52:36,462 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:52:36,462 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:52:36,462 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.56it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.87it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.65\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    105.832\n",
            "  eval_steps_per_second   =     13.305\n",
            "11/12/2023 19:52:38 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 19:52:38,118 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 19:52:38,120 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 19:52:38,121 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-12 19:52:38,121 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 14.53it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9716\n",
            "  predict_f1                 =     0.9463\n",
            "  predict_loss               =     0.1527\n",
            "  predict_precision          =     0.9513\n",
            "  predict_recall             =     0.9413\n",
            "  predict_runtime            = 0:00:01.69\n",
            "  predict_samples_per_second =     112.36\n",
            "  predict_steps_per_second   =     14.119\n",
            "[INFO|modelcard.py:452] 2023-11-12 19:52:39,918 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E\n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O\n",
            "4                                C,C,C,O,O,E,O,E,O,O,O\n",
            "..                                                 ...\n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...\n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...\n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,O,E,E,E,E,O,O,O,O,O,O,E,E,E  \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O  \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O  \n",
            "..                                                 ...  \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...  \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...  \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.92      0.95      0.93       465\n",
            "          CE       0.83      0.76      0.79        25\n",
            "           E       0.96      0.95      0.95       468\n",
            "           O       0.98      0.98      0.98      2663\n",
            "\n",
            "    accuracy                           0.97      3621\n",
            "   macro avg       0.92      0.91      0.92      3621\n",
            "weighted avg       0.97      0.97      0.97      3621\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################SCITE as train and Organizational data as test###########################"
      ],
      "metadata": {
        "id": "gdd5juMm2fLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# data location\n",
        "#parent_dir = \"/content/drive/MyDrive\"\n",
        "parent_dir = \"/content/drive/MyDrive/\"\n",
        "mk_data_path = lambda filename: os.path.join(parent_dir, filename)\n",
        "#data_path = f\"{parent_dir}/BERT_data_final.xlsx\"\n",
        "data_path = mk_data_path(\"BERT_data_final.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc98MY5D2sGI",
        "outputId": "e7b44901-8d74-418e-82c6-2c0db6c6234d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "RANDOM_SEED=42"
      ],
      "metadata": {
        "id": "e0KoqP1A27H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel(data_path) #'/content/BERT_data_final.xlsx',sheet_name='Sheet2'\n",
        "data.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "gCDb3HEk29t6",
        "outputId": "8c6177fe-182c-4a90-d37d-8c43e1a559ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                Causal-relation text  \\\n",
              "0  [\"When \",\"a \",\"policyholder \",\"or \",\"insured \"...   \n",
              "1  [\"During \",\"2020 \",\"in \",\"response \",\"to \",\"th...   \n",
              "2  [\"Prolonged \",\"periods \",\"of \",\"low \",\"interes...   \n",
              "3  [\"Conversely \",\"a \",\"rise \",\"in \",\"interest \",...   \n",
              "\n",
              "                                            BIO Code  \\\n",
              "0  [\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"...   \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"...   \n",
              "2  [\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT...   \n",
              "3  [\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT\",\"O...   \n",
              "\n",
              "       {\"words\":Causal-relation text,\"ner\":BIO Code}  \\\n",
              "0  {\"words\":[\"When \",\"a \",\"policyholder \",\"or \",\"...   \n",
              "1  {\"words\":[\"During \",\"2020 \",\"in \",\"response \",...   \n",
              "2  {\"words\":[\"Prolonged \",\"periods \",\"of \",\"low \"...   \n",
              "3  {\"words\":[\"Conversely \",\"a \",\"rise \",\"in \",\"in...   \n",
              "\n",
              "                                    BIO Code_changed  \\\n",
              "0  [\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"...   \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"...   \n",
              "2  [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"E\",\"E\",\"O\",...   \n",
              "3  [\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"O\",\"O\",\"O\",...   \n",
              "\n",
              "                                BIO Code-changed-NER  \n",
              "0  [\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...  \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-...  \n",
              "2  [\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"...  \n",
              "3  [\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd98d505-6f58-4796-ad42-f07ee9ab4fe9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Causal-relation text</th>\n",
              "      <th>BIO Code</th>\n",
              "      <th>{\"words\":Causal-relation text,\"ner\":BIO Code}</th>\n",
              "      <th>BIO Code_changed</th>\n",
              "      <th>BIO Code-changed-NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"When \",\"a \",\"policyholder \",\"or \",\"insured \"...</td>\n",
              "      <td>[\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"...</td>\n",
              "      <td>{\"words\":[\"When \",\"a \",\"policyholder \",\"or \",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"During \",\"2020 \",\"in \",\"response \",\"to \",\"th...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"...</td>\n",
              "      <td>{\"words\":[\"During \",\"2020 \",\"in \",\"response \",...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"Prolonged \",\"periods \",\"of \",\"low \",\"interes...</td>\n",
              "      <td>[\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT...</td>\n",
              "      <td>{\"words\":[\"Prolonged \",\"periods \",\"of \",\"low \"...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"E\",\"E\",\"O\",...</td>\n",
              "      <td>[\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"Conversely \",\"a \",\"rise \",\"in \",\"interest \",...</td>\n",
              "      <td>[\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT\",\"O...</td>\n",
              "      <td>{\"words\":[\"Conversely \",\"a \",\"rise \",\"in \",\"in...</td>\n",
              "      <td>[\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"O\",\"O\",\"O\",...</td>\n",
              "      <td>[\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd98d505-6f58-4796-ad42-f07ee9ab4fe9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cd98d505-6f58-4796-ad42-f07ee9ab4fe9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cd98d505-6f58-4796-ad42-f07ee9ab4fe9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-557587fb-2ecc-4d24-aedc-2bf74f32fd39\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-557587fb-2ecc-4d24-aedc-2bf74f32fd39')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-557587fb-2ecc-4d24-aedc-2bf74f32fd39 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset = data[[\"Causal-relation text\", \"BIO Code_changed\"]].rename(columns={\"Causal-relation text\": \"X\", \"BIO Code_changed\": \"y\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw_test, y_raw_test = [json.loads(tokens) for tokens in dataset.X.values], [json.loads(labels) for labels in dataset.y.values]"
      ],
      "metadata": {
        "id": "l5Q94Spc293h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(y_raw_test):\n",
        "    for j, a in enumerate(x):\n",
        "        if 'CT' in a:\n",
        "            y_raw_test[i][j] = a.replace('CT', 'CE')"
      ],
      "metadata": {
        "id": "KhIX63LhJXIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners2 = []\n",
        "for i,j in zip(X_raw_test, y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners2.append(_new_ner)"
      ],
      "metadata": {
        "id": "15AGYlJrAk1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners2[i] for i in range(len(new_ners2))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqL-zXddA1Pw",
        "outputId": "39f8b893-d6e6-4155-a838-6eb973d0c1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VIk_ZNMI6ux",
        "outputId": "03e4541a-7f6d-4e2c-f16f-cb9ca2029fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2234,)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased   \\\n",
        "           --train_file /content/train.json  \\\n",
        "           --validation_file /content/dev.json  \\\n",
        "           --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2voEn97BBhd",
        "outputId": "16ddfd46-2172-4f26-c3e6-1473a2552c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-31 17:49:22.598251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/31/2023 17:49:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/31/2023 17:49:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Jul31_17-49-25_0dd88bc1aaee,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/31/2023 17:49:25 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-6d4f9b0da6d7a098\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Using custom data configuration default-6d4f9b0da6d7a098\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "07/31/2023 17:49:26 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 1852.61it/s]\n",
            "Downloading took 0.0 min\n",
            "07/31/2023 17:49:26 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "07/31/2023 17:49:26 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1564.07it/s]\n",
            "Generating train split\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 696 examples [00:00, 80798.11 examples/s]\n",
            "Generating validation split\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 175 examples [00:00, 56466.13 examples/s]\n",
            "Generating test split\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 2234 examples [00:00, 56314.95 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "07/31/2023 17:49:26 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "07/31/2023 17:49:26 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 17:49:27,043 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 17:49:27,049 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:523] 2023-07-31 17:49:27,274 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 17:49:27,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 17:49:27,496 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 17:49:27,942 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 17:49:27,942 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 17:49:27,942 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 17:49:27,943 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1844] 2023-07-31 17:49:27,943 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 17:49:27,943 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 17:49:27,944 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-07-31 17:49:27,991 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:771] 2023-07-31 17:49:27,992 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.32.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2638] 2023-07-31 17:49:28,042 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3370] 2023-07-31 17:49:31,475 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3372] 2023-07-31 17:49:31,475 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/696 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2488] 2023-07-31 17:49:31,510 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-55e66ae5e475adf1.arrow\n",
            "07/31/2023 17:49:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-55e66ae5e475adf1.arrow\n",
            "Running tokenizer on train dataset: 100% 696/696 [00:00<00:00, 5338.24 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/175 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-122a12becdee00d4.arrow\n",
            "07/31/2023 17:49:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-122a12becdee00d4.arrow\n",
            "Running tokenizer on validation dataset: 100% 175/175 [00:00<00:00, 4910.05 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/2234 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4f6e92667bfda6ff.arrow\n",
            "07/31/2023 17:49:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-6d4f9b0da6d7a098/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4f6e92667bfda6ff.arrow\n",
            "Running tokenizer on prediction dataset: 100% 2234/2234 [00:00<00:00, 3285.38 examples/s]\n",
            "[INFO|trainer.py:2015] 2023-07-31 17:49:39,123 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:749] 2023-07-31 17:49:46,243 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1681] 2023-07-31 17:50:00,722 >> ***** Running training *****\n",
            "[INFO|trainer.py:1682] 2023-07-31 17:50:00,722 >>   Num examples = 696\n",
            "[INFO|trainer.py:1683] 2023-07-31 17:50:00,722 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1684] 2023-07-31 17:50:00,722 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1687] 2023-07-31 17:50:00,722 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1688] 2023-07-31 17:50:00,722 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1689] 2023-07-31 17:50:00,722 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1690] 2023-07-31 17:50:00,724 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1710] 2023-07-31 17:50:00,725 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1711] 2023-07-31 17:50:00,725 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1712] 2023-07-31 17:50:00,725 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1714] 2023-07-31 17:50:00,725 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:284] 2023-07-31 17:50:00,768 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:35<00:00,  2.02it/s][INFO|trainer.py:1929] 2023-07-31 17:52:36,434 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 155.8251, 'train_samples_per_second': 133.996, 'train_steps_per_second': 8.471, 'train_loss': 0.0006528706261605927, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:35<00:00,  8.47it/s]\n",
            "[INFO|trainer.py:2809] 2023-07-31 17:52:36,552 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:460] 2023-07-31 17:52:36,556 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:1874] 2023-07-31 17:52:42,236 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2227] 2023-07-31 17:52:42,278 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2234] 2023-07-31 17:52:42,278 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:35.82\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    133.996\n",
            "  train_steps_per_second   =      8.471\n",
            "07/31/2023 17:52:42 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:749] 2023-07-31 17:52:42,338 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3083] 2023-07-31 17:52:42,341 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3085] 2023-07-31 17:52:42,341 >>   Num examples = 175\n",
            "[INFO|trainer.py:3088] 2023-07-31 17:52:42,341 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 17.08it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 16.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9676\n",
            "  eval_f1                 =     0.9274\n",
            "  eval_loss               =     0.1637\n",
            "  eval_precision          =     0.9169\n",
            "  eval_recall             =     0.9381\n",
            "  eval_runtime            = 0:00:01.51\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    115.453\n",
            "  eval_steps_per_second   =     14.514\n",
            "07/31/2023 17:52:43 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:749] 2023-07-31 17:52:43,860 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3083] 2023-07-31 17:52:43,862 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3085] 2023-07-31 17:52:43,862 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3088] 2023-07-31 17:52:43,862 >>   Batch size = 8\n",
            "100% 280/280 [00:30<00:00,  9.33it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2602\n",
            "  predict_f1                 =     0.2058\n",
            "  predict_loss               =     5.3754\n",
            "  predict_precision          =     0.6544\n",
            "  predict_recall             =     0.1221\n",
            "  predict_runtime            = 0:00:30.28\n",
            "  predict_samples_per_second =     73.758\n",
            "  predict_steps_per_second   =      9.245\n",
            "[INFO|modelcard.py:452] 2023-07-31 17:53:14,749 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9168765743073047}, {'name': 'Recall', 'type': 'recall', 'value': 0.9381443298969072}, {'name': 'F1', 'type': 'f1', 'value': 0.927388535031847}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9676074406670943}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "print(data_preds)\n",
        "\n",
        "preds = {'preds':data_preds}\n",
        "print(preds)\n",
        "\n",
        "data_test = pd.read_json('/content/test.json', lines = True)\n",
        "\n",
        "data_test_final = pd.concat([data_test,data_preds],axis=1)\n",
        "print(data_test_final)\n",
        "\n",
        "data_test_final.to_excel('/content/spanbert_gold_preds.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s6AK8HKCN-O",
        "outputId": "cb809d7b-b1e0-4491-e2ef-07a8cb7ed15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  preds\n",
            "0               O O O O O O E O E O O O O O O O O O O O\n",
            "1     O O O O O O C O C O O O O O O O O O O O O O O ...\n",
            "2     O O O O O O O O E E O O E E O O O O O O O O O ...\n",
            "3     O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "4     O O O C C O O O O O O O O O C O O O O O O O O ...\n",
            "...                                                 ...\n",
            "2229   E E O O O O O O O O CE CE CE O O C C O C C O O O\n",
            "2230                          E E E E O O C C O O O O O\n",
            "2231                    E E O E E O O O O O C C C C C C\n",
            "2232                    E E E E O O C C O O O O C O O O\n",
            "2233  E E E E O O C C O E E O E E E E O O O O O C C C C\n",
            "\n",
            "[2234 rows x 1 columns]\n",
            "{'preds':                                                   preds\n",
            "0               O O O O O O E O E O O O O O O O O O O O\n",
            "1     O O O O O O C O C O O O O O O O O O O O O O O ...\n",
            "2     O O O O O O O O E E O O E E O O O O O O O O O ...\n",
            "3     O O O O O O O O O O O O O O O O O O O O O O O ...\n",
            "4     O O O C C O O O O O O O O O C O O O O O O O O ...\n",
            "...                                                 ...\n",
            "2229   E E O O O O O O O O CE CE CE O O C C O C C O O O\n",
            "2230                          E E E E O O C C O O O O O\n",
            "2231                    E E O E E O O O O O C C C C C C\n",
            "2232                    E E E E O O C C O O O O C O O O\n",
            "2233  E E E E O O C C O E E O E E E E O O O O O C C C C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                                  words  \\\n",
            "0     [When , a , policyholder , or , insured , gets...   \n",
            "1     [During , 2020 , in , response , to , the , on...   \n",
            "2     [Prolonged , periods , of , low , interest , r...   \n",
            "3     [Conversely , a , rise , in , interest , rates...   \n",
            "4     [Further , because , of , the , concentration ...   \n",
            "...                                                 ...   \n",
            "2229  [The , increase , in , adjusted , PPNR , was ,...   \n",
            "2230  [Modest , noninterest , income , growth , due ...   \n",
            "2231  [Wealth , management , and , trust , fees , in...   \n",
            "2232  [The , 2019 , provision , increased , due , to...   \n",
            "2233  [The , 2019 , provision , increased , due , to...   \n",
            "\n",
            "                                                    ner  \\\n",
            "0     [O, O, C, C, C, C, C, C, C, O, O, CE, E, E, E,...   \n",
            "1     [O, O, O, O, O, C, C, C, C, C, C, C, C, C, C, ...   \n",
            "2     [C, C, C, C, C, C, O, CE, E, E, O, O, O, CE, O...   \n",
            "3     [O, O, C, C, C, C, O, CE, O, O, O, O, O, O, E,...   \n",
            "4     [O, CE, O, O, C, C, C, C, C, C, C, C, C, C, C,...   \n",
            "...                                                 ...   \n",
            "2229  [O, E, E, E, E, E, E, E, E, E, E, E, E, CE, CE...   \n",
            "2230          [E, E, E, E, CE, CE, C, C, C, C, C, C, C]   \n",
            "2231  [E, E, E, E, E, E, CE, CE, CE, O, C, C, C, C, ...   \n",
            "2232  [O, O, E, E, CE, CE, C, C, C, C, C, O, C, C, C...   \n",
            "2233  [O, O, E, E, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O O O O O O E O E O O O O O O O O O O O  \n",
            "1     O O O O O O C O C O O O O O O O O O O O O O O ...  \n",
            "2     O O O O O O O O E E O O E E O O O O O O O O O ...  \n",
            "3     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "4     O O O C C O O O O O O O O O C O O O O O O O O ...  \n",
            "...                                                 ...  \n",
            "2229   E E O O O O O O O O CE CE CE O O C C O C C O O O  \n",
            "2230                          E E E E O O C C O O O O O  \n",
            "2231                    E E O E E O O O O O C C C C C C  \n",
            "2232                    E E E E O O C C O O O O C O O O  \n",
            "2233  E E E E O O C C O E E O E E E E O O O O O C C C C  \n",
            "\n",
            "[2234 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_spanbert_gold_preds = pd.read_excel('/content/spanbert_gold_preds.xlsx')\n",
        "\n",
        "print(data_spanbert_gold_preds)\n",
        "\n",
        "flat_list = [i.strip() for i in data_spanbert_gold_preds['ner'] for i in i.split(',')]\n",
        "flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(' ')]\n",
        "flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "import numpy as np\n",
        "print(np.shape(flat_list))\n",
        "print(np.shape(flat_list_preds))\n",
        "print(np.shape(flat_list_words))\n",
        "\n",
        "df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        "df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "df_combine = pd.concat([df_finallist_words,df_flatlist,df_finallist],axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p041BIP7EdlA",
        "outputId": "727203e2-b600-49cb-9a06-45afbc13e92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0                                              words  \\\n",
            "0              0  ['When ', 'a ', 'policyholder ', 'or ', 'insur...   \n",
            "1              1  ['During ', '2020 ', 'in ', 'response ', 'to '...   \n",
            "2              2  ['Prolonged ', 'periods ', 'of ', 'low ', 'int...   \n",
            "3              3  ['Conversely ', 'a ', 'rise ', 'in ', 'interes...   \n",
            "4              4  ['Further ', 'because ', 'of ', 'the ', 'conce...   \n",
            "...          ...                                                ...   \n",
            "2229        2229  ['The ', 'increase ', 'in ', 'adjusted ', 'PPN...   \n",
            "2230        2230  ['Modest ', 'noninterest ', 'income ', 'growth...   \n",
            "2231        2231  ['Wealth ', 'management ', 'and ', 'trust ', '...   \n",
            "2232        2232  ['The ', '2019 ', 'provision ', 'increased ', ...   \n",
            "2233        2233  ['The ', '2019 ', 'provision ', 'increased ', ...   \n",
            "\n",
            "                                                    ner  \\\n",
            "0     O, O, C, C, C, C, C, C, C, O, O, CE, E, E, E, ...   \n",
            "1     O, O, O, O, O, C, C, C, C, C, C, C, C, C, C, O...   \n",
            "2     C, C, C, C, C, C, O, CE, E, E, O, O, O, CE, O,...   \n",
            "3     O, O, C, C, C, C, O, CE, O, O, O, O, O, O, E, ...   \n",
            "4     O, CE, O, O, C, C, C, C, C, C, C, C, C, C, C, ...   \n",
            "...                                                 ...   \n",
            "2229  O, E, E, E, E, E, E, E, E, E, E, E, E, CE, CE,...   \n",
            "2230            E, E, E, E, CE, CE, C, C, C, C, C, C, C   \n",
            "2231  E, E, E, E, E, E, CE, CE, CE, O, C, C, C, C, C, C   \n",
            "2232   O, O, E, E, CE, CE, C, C, C, C, C, O, C, C, C, C   \n",
            "2233  O, O, E, E, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O O O O O O E O E O O O O O O O O O O O  \n",
            "1     O O O O O O C O C O O O O O O O O O O O O O O ...  \n",
            "2     O O O O O O O O E E O O E E O O O O O O O O O ...  \n",
            "3     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "4     O O O C C O O O O O O O O O C O O O O O O O O ...  \n",
            "...                                                 ...  \n",
            "2229   E E O O O O O O O O CE CE CE O O C C O C C O O O  \n",
            "2230                          E E E E O O C C O O O O O  \n",
            "2231                    E E O E E O O O O O C C C C C C  \n",
            "2232                    E E E E O O C C O O O O C O O O  \n",
            "2233  E E E E O O C C O E E O E E E E O O O O O C C C C  \n",
            "\n",
            "[2234 rows x 4 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "(73283,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIfb27rREg3M",
        "outputId": "efc86611-8eba-4c80-df36-d616c8a8a0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.16      0.27     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.68      0.13      0.22     28549\n",
            "           O       0.17      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.26     73283\n",
            "   macro avg       0.40      0.30      0.20     73283\n",
            "weighted avg       0.58      0.26      0.24     73283\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######10 runs"
      ],
      "metadata": {
        "id": "7gcMSDVeIF95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "Rtd7imYSILL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "M6xCrpBAIRAK",
        "outputId": "678c3a83-c0aa-4668-95ff-9bec74730dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  words  \\\n",
              "0     [When , a , policyholder , or , insured , gets...   \n",
              "1     [During , 2020 , in , response , to , the , on...   \n",
              "2     [Prolonged , periods , of , low , interest , r...   \n",
              "3     [Conversely , a , rise , in , interest , rates...   \n",
              "4     [Further , because , of , the , concentration ...   \n",
              "...                                                 ...   \n",
              "2229  [The , increase , in , adjusted , PPNR , was ,...   \n",
              "2230  [Modest , noninterest , income , growth , due ...   \n",
              "2231  [Wealth , management , and , trust , fees , in...   \n",
              "2232  [The , 2019 , provision , increased , due , to...   \n",
              "2233  [The , 2019 , provision , increased , due , to...   \n",
              "\n",
              "                                                    ner  \n",
              "0     [O, O, C, C, C, C, C, C, C, O, O, CE, E, E, E,...  \n",
              "1     [O, O, O, O, O, C, C, C, C, C, C, C, C, C, C, ...  \n",
              "2     [C, C, C, C, C, C, O, CE, E, E, O, O, O, CE, O...  \n",
              "3     [O, O, C, C, C, C, O, CE, O, O, O, O, O, O, E,...  \n",
              "4     [O, CE, O, O, C, C, C, C, C, C, C, C, C, C, C,...  \n",
              "...                                                 ...  \n",
              "2229  [O, E, E, E, E, E, E, E, E, E, E, E, E, CE, CE...  \n",
              "2230          [E, E, E, E, CE, CE, C, C, C, C, C, C, C]  \n",
              "2231  [E, E, E, E, E, E, CE, CE, CE, O, C, C, C, C, ...  \n",
              "2232  [O, O, E, E, CE, CE, C, C, C, C, C, O, C, C, C...  \n",
              "2233  [O, O, E, E, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "\n",
              "[2234 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1962311b-5691-42c6-bd44-64d074b4c62e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[When , a , policyholder , or , insured , gets...</td>\n",
              "      <td>[O, O, C, C, C, C, C, C, C, O, O, CE, E, E, E,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[During , 2020 , in , response , to , the , on...</td>\n",
              "      <td>[O, O, O, O, O, C, C, C, C, C, C, C, C, C, C, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Prolonged , periods , of , low , interest , r...</td>\n",
              "      <td>[C, C, C, C, C, C, O, CE, E, E, O, O, O, CE, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Conversely , a , rise , in , interest , rates...</td>\n",
              "      <td>[O, O, C, C, C, C, O, CE, O, O, O, O, O, O, E,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Further , because , of , the , concentration ...</td>\n",
              "      <td>[O, CE, O, O, C, C, C, C, C, C, C, C, C, C, C,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2229</th>\n",
              "      <td>[The , increase , in , adjusted , PPNR , was ,...</td>\n",
              "      <td>[O, E, E, E, E, E, E, E, E, E, E, E, E, CE, CE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2230</th>\n",
              "      <td>[Modest , noninterest , income , growth , due ...</td>\n",
              "      <td>[E, E, E, E, CE, CE, C, C, C, C, C, C, C]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231</th>\n",
              "      <td>[Wealth , management , and , trust , fees , in...</td>\n",
              "      <td>[E, E, E, E, E, E, CE, CE, CE, O, C, C, C, C, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2232</th>\n",
              "      <td>[The , 2019 , provision , increased , due , to...</td>\n",
              "      <td>[O, O, E, E, CE, CE, C, C, C, C, C, O, C, C, C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2233</th>\n",
              "      <td>[The , 2019 , provision , increased , due , to...</td>\n",
              "      <td>[O, O, E, E, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2234 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1962311b-5691-42c6-bd44-64d074b4c62e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1962311b-5691-42c6-bd44-64d074b4c62e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1962311b-5691-42c6-bd44-64d074b4c62e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cfe050f0-0def-452b-8498-25c291469bab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cfe050f0-0def-452b-8498-25c291469bab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cfe050f0-0def-452b-8498-25c291469bab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c96456ce-31a1-45a2-844c-2f30669c864e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c96456ce-31a1-45a2-844c-2f30669c864e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "W7bov9BBIHMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg1NhQqAIGEg",
        "outputId": "08b2b196-89d7-4318-c45c-ec68ce146f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 20:09:13.694125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:09:13.694199: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:09:13.694243: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:09:15.047657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:09:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:09:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-09-17_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:09:17 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:09:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 16448.25it/s]\n",
            "Downloading took 0.0 min\n",
            "11/12/2023 20:09:18 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/12/2023 20:09:18 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1770.74it/s]\n",
            "Generating train split\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 696 examples [00:00, 79400.41 examples/s]\n",
            "Generating validation split\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 175 examples [00:00, 63854.13 examples/s]\n",
            "Generating test split\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 2234 examples [00:00, 53032.36 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/12/2023 20:09:18 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/12/2023 20:09:18 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:09:18,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:09:18,447 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:09:18,535 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:09:18,624 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:09:18,625 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:09:18,799 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:09:18,799 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:09:18,799 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:09:18,799 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:09:18,799 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:09:18,799 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:09:18,801 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:09:18,852 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:09:18,854 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:09:18,905 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:09:24,078 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:09:24,078 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/696 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2692] 2023-11-12 20:09:24,142 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:09:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Running tokenizer on train dataset: 100% 696/696 [00:00<00:00, 2893.38 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/175 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-cf2b54c471bf2b08.arrow\n",
            "11/12/2023 20:09:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-cf2b54c471bf2b08.arrow\n",
            "Running tokenizer on validation dataset: 100% 175/175 [00:00<00:00, 2797.36 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/2234 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f004795e0a7d19.arrow\n",
            "11/12/2023 20:09:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f004795e0a7d19.arrow\n",
            "Running tokenizer on prediction dataset: 100% 2234/2234 [00:01<00:00, 1871.80 examples/s]\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:09:28,416 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:09:34,594 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:09:47,063 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:09:47,063 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:09:47,063 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:09:47,063 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:09:47,063 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:09:47,063 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:09:47,063 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:09:47,065 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:09:47,066 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:09:47,066 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:09:47,066 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:09:47,066 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:09:47,132 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:38<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 20:12:25,288 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 158.3215, 'train_samples_per_second': 131.884, 'train_steps_per_second': 8.337, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:38<00:00,  8.34it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:12:25,389 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:12:25,391 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:12:33,279 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:12:33,283 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:12:33,283 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:38.32\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    131.884\n",
            "  train_steps_per_second   =      8.337\n",
            "11/12/2023 20:12:33 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:12:33,309 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:12:33,315 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:12:33,315 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:12:33,315 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.29it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.49it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.65\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    105.616\n",
            "  eval_steps_per_second   =     13.278\n",
            "11/12/2023 20:12:34 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:12:34,975 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:12:34,977 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:12:34,977 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:12:34,977 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.29it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:34.04\n",
            "  predict_samples_per_second =     65.615\n",
            "  predict_steps_per_second   =      8.224\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:13:09,363 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:13:21.115459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:13:21.115513: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:13:21.115553: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:13:23.002060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:13:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:13:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-13-27_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:13:27 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:13:27 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:13:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:13:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:13:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:13:27 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:13:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:13:28,118 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:13:28,128 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:13:28,219 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:13:28,344 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:13:28,345 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:13:28,518 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:13:28,519 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:13:28,519 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:13:28,519 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:13:28,519 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:13:28,519 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:13:28,520 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:13:28,570 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:13:28,572 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:13:28,676 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:13:36,162 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:13:36,162 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:13:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/175 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2692] 2023-11-12 20:13:36,227 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:13:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Running tokenizer on validation dataset: 100% 175/175 [00:00<00:00, 3453.73 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f004795e0a7d19.arrow\n",
            "11/12/2023 20:13:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f004795e0a7d19.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:13:41,505 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:13:46,898 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:14:00,416 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:14:00,416 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:14:00,416 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:14:00,416 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:14:00,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:14:00,416 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:14:00,416 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:14:00,418 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:14:00,419 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:14:00,419 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:14:00,419 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:14:00,419 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:14:00,443 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  2.00it/s][INFO|trainer.py:1956] 2023-11-12 20:16:41,660 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.3475, 'train_samples_per_second': 129.41, 'train_steps_per_second': 8.181, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:16:41,771 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:16:41,773 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:16:48,115 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:16:48,117 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:16:48,117 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.34\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =     129.41\n",
            "  train_steps_per_second   =      8.181\n",
            "11/12/2023 20:16:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:16:48,142 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:16:48,145 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:16:48,148 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:16:48,148 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.29it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.73\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    100.705\n",
            "  eval_steps_per_second   =      12.66\n",
            "11/12/2023 20:16:49 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:16:49,887 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:16:49,890 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:16:49,890 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:16:49,890 >>   Batch size = 8\n",
            "100% 280/280 [00:31<00:00,  8.75it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:32.27\n",
            "  predict_samples_per_second =     69.213\n",
            "  predict_steps_per_second   =      8.675\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:17:22,362 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:17:32.283201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:17:32.283266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:17:32.283314: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:17:34.555151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:17:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:17:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-17-39_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:17:39 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:17:39 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:17:39 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:17:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:17:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:17:39 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:17:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:17:40,105 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:17:40,115 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:17:40,205 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:17:40,294 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:17:40,295 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:17:40,469 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:17:40,469 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:17:40,469 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:17:40,469 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:17:40,469 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:17:40,470 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:17:40,471 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:17:40,504 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:17:40,504 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:17:40,607 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:17:49,073 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:17:49,073 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:17:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:17:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/2234 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2692] 2023-11-12 20:17:49,259 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:17:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "Running tokenizer on prediction dataset: 100% 2234/2234 [00:01<00:00, 1675.18 examples/s]\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:17:56,841 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:18:00,196 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:18:11,875 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:18:11,875 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:18:11,875 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:18:11,875 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:18:11,875 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:18:11,875 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:18:11,875 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:18:11,877 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:18:11,877 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:18:11,877 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:18:11,877 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:18:11,877 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:18:11,933 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 20:20:54,828 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 163.0499, 'train_samples_per_second': 128.059, 'train_steps_per_second': 8.096, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:43<00:00,  8.10it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:20:54,930 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:20:54,934 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:21:02,987 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:21:03,021 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:21:03,021 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:43.04\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.059\n",
            "  train_steps_per_second   =      8.096\n",
            "11/12/2023 20:21:03 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:21:03,048 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:21:03,052 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:21:03,052 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:21:03,052 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.28it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.55it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.76\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =     98.954\n",
            "  eval_steps_per_second   =      12.44\n",
            "11/12/2023 20:21:04 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:21:04,824 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:21:04,827 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:21:04,828 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:21:04,828 >>   Batch size = 8\n",
            "100% 280/280 [00:31<00:00,  8.84it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:31.96\n",
            "  predict_samples_per_second =     69.889\n",
            "  predict_steps_per_second   =       8.76\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:21:36,979 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:21:48.520029: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:21:48.520133: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:21:48.520214: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:21:51.071330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:21:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:21:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-21-55_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:21:55 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:21:55 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:21:55 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:21:55 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:21:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:21:55 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:21:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:21:55,906 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:21:55,914 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:21:56,007 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:21:56,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:21:56,095 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:21:56,402 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:21:56,402 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:21:56,403 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:21:56,403 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:21:56,403 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:21:56,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:21:56,404 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:21:56,435 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:21:56,436 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:21:56,527 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:22:06,000 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:22:06,001 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:22:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:22:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:22:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:22:10,640 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:22:15,731 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:22:29,204 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:22:29,204 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:22:29,204 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:22:29,204 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:22:29,205 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:22:29,205 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:22:29,205 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:22:29,208 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:22:29,212 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:22:29,212 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:22:29,212 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:22:29,212 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:22:29,260 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 20:25:11,334 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 162.2223, 'train_samples_per_second': 128.712, 'train_steps_per_second': 8.137, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:42<00:00,  8.14it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:25:11,435 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:25:11,437 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:25:25,763 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:25:25,765 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:25:25,765 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:42.22\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.712\n",
            "  train_steps_per_second   =      8.137\n",
            "11/12/2023 20:25:25 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:25:25,780 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:25:25,782 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:25:25,782 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:25:25,782 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.75it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 15.00it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.61\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    108.617\n",
            "  eval_steps_per_second   =     13.655\n",
            "11/12/2023 20:25:27 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:25:27,396 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:25:27,398 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:25:27,398 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:25:27,398 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.32it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.92\n",
            "  predict_samples_per_second =     65.857\n",
            "  predict_steps_per_second   =      8.254\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:26:01,903 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:26:10.626066: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:26:10.626117: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:26:10.626154: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:26:12.747031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:26:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:26:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-26-17_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:26:17 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:26:17 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:26:17 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:26:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:26:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:26:17 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:26:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:26:18,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:26:18,213 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:26:18,305 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:26:18,394 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:26:18,396 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:26:18,580 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:26:18,580 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:26:18,580 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:26:18,580 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:26:18,580 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:26:18,580 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:26:18,582 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:26:18,632 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:26:18,633 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:26:18,742 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:26:24,857 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:26:24,857 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:26:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:26:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:26:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:26:30,237 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:26:35,785 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:26:48,943 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:26:48,943 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:26:48,943 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:26:48,943 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:26:48,943 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:26:48,943 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:26:48,943 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:26:48,945 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:26:48,946 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:26:48,946 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:26:48,946 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:26:48,946 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:26:48,972 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 20:29:30,510 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.6608, 'train_samples_per_second': 129.159, 'train_steps_per_second': 8.165, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.17it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:29:30,609 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:29:30,611 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:29:37,027 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:29:37,028 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:29:37,028 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.66\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.159\n",
            "  train_steps_per_second   =      8.165\n",
            "11/12/2023 20:29:37 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:29:37,070 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:29:37,072 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:29:37,072 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:29:37,072 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 14.98it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.48it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.65\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    105.716\n",
            "  eval_steps_per_second   =      13.29\n",
            "11/12/2023 20:29:38 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:29:38,730 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:29:38,732 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:29:38,732 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:29:38,732 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.41it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.57\n",
            "  predict_samples_per_second =     66.547\n",
            "  predict_steps_per_second   =      8.341\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:30:12,582 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:30:21.781363: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:30:21.781418: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:30:21.781458: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:30:23.653934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:30:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:30:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-30-27_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:30:28 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:30:28 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:30:28 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:30:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:30:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:30:28 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:30:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:30:28,838 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:30:28,849 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:30:28,936 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:30:29,024 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:30:29,026 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:30:29,201 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:30:29,201 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:30:29,201 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:30:29,201 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:30:29,201 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:30:29,201 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:30:29,203 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:30:29,256 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:30:29,257 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:30:29,369 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:30:36,639 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:30:36,639 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:30:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:30:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:30:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:30:39,709 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:30:44,440 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:30:59,410 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:30:59,410 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:30:59,410 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:30:59,410 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:30:59,411 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:30:59,411 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:30:59,411 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:30:59,412 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:30:59,413 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:30:59,413 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:30:59,413 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:30:59,414 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:30:59,438 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 20:33:40,780 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.4651, 'train_samples_per_second': 129.316, 'train_steps_per_second': 8.175, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:33:40,881 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:33:40,882 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:33:51,609 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:33:51,610 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:33:51,610 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.46\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.316\n",
            "  train_steps_per_second   =      8.175\n",
            "11/12/2023 20:33:51 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:33:51,625 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:33:51,628 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:33:51,628 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:33:51,628 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.26it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.67it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.63\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    106.962\n",
            "  eval_steps_per_second   =     13.447\n",
            "11/12/2023 20:33:53 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:33:53,267 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:33:53,269 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:33:53,269 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:33:53,270 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.34it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.86\n",
            "  predict_samples_per_second =     65.968\n",
            "  predict_steps_per_second   =      8.268\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:34:27,460 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:34:36.780898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:34:36.780957: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:34:36.780995: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:34:38.575058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:34:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:34:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-34-42_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:34:42 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:34:43 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:34:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:34:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:34:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:34:43 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:34:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:34:43,711 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:34:43,722 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:34:43,819 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:34:43,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:34:43,909 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:34:44,088 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:34:44,088 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:34:44,088 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:34:44,088 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:34:44,088 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:34:44,089 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:34:44,090 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:34:44,137 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:34:44,139 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:34:44,247 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:34:51,448 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:34:51,448 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:34:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:34:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:34:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:34:54,549 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:34:59,370 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:35:14,820 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:35:14,820 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:35:14,820 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:35:14,820 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:35:14,821 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:35:14,821 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:35:14,821 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:35:14,822 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:35:14,823 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:35:14,823 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:35:14,823 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:35:14,823 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:35:14,848 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 20:37:57,269 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 162.5431, 'train_samples_per_second': 128.458, 'train_steps_per_second': 8.121, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:42<00:00,  8.12it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:37:57,369 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:37:57,374 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:38:07,539 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:38:07,542 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:38:07,542 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:42.54\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.458\n",
            "  train_steps_per_second   =      8.121\n",
            "11/12/2023 20:38:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:38:07,566 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:38:07,568 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:38:07,569 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:38:07,569 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.15it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.51it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.64\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    106.099\n",
            "  eval_steps_per_second   =     13.338\n",
            "11/12/2023 20:38:09 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:38:09,220 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:38:09,222 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:38:09,222 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:38:09,223 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.37it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.74\n",
            "  predict_samples_per_second =     66.195\n",
            "  predict_steps_per_second   =      8.297\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:38:43,248 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:38:53.011756: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:38:53.011814: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:38:53.011852: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:38:55.371202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:39:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:39:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-39-00_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:39:00 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:39:00 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:39:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:39:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:39:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:39:00 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:39:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:39:01,150 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:39:01,160 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:39:01,266 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:39:01,354 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:39:01,355 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:39:01,532 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:39:01,532 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:39:01,532 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:39:01,532 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:39:01,532 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:39:01,532 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:39:01,533 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:39:01,581 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:39:01,583 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:39:01,695 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:39:09,098 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:39:09,099 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:39:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:39:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:39:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:39:12,418 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:39:18,003 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:39:32,512 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:39:32,512 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:39:32,512 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:39:32,512 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:39:32,512 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:39:32,512 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:39:32,512 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:39:32,515 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:39:32,520 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:39:32,520 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:39:32,520 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:39:32,520 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:39:32,553 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 20:42:15,235 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 162.8177, 'train_samples_per_second': 128.242, 'train_steps_per_second': 8.107, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:42<00:00,  8.11it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:42:15,336 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:42:15,338 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:42:21,535 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:42:21,537 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:42:21,537 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:42.81\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.242\n",
            "  train_steps_per_second   =      8.107\n",
            "11/12/2023 20:42:21 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:42:21,556 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:42:21,565 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:42:21,565 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:42:21,565 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.07it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.57it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.64\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    106.209\n",
            "  eval_steps_per_second   =     13.352\n",
            "11/12/2023 20:42:23 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:42:23,215 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:42:23,217 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:42:23,217 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:42:23,217 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.41it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.56\n",
            "  predict_samples_per_second =     66.567\n",
            "  predict_steps_per_second   =      8.343\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:42:57,069 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:43:07.230844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:43:07.230898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:43:07.230962: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:43:09.154663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:43:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:43:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-43-13_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:43:14 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:43:14 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:43:14 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:43:14 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:43:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:43:14 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:43:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:43:14,847 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:43:14,857 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:43:14,947 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:43:15,035 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:43:15,036 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:43:15,238 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:43:15,238 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:43:15,238 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:43:15,238 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:43:15,238 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:43:15,239 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:43:15,240 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:43:15,295 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:43:15,297 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:43:15,417 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:43:21,758 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:43:21,759 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:43:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:43:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:43:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:43:27,174 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:43:32,864 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:43:46,945 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:43:46,946 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:43:46,946 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:43:46,946 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:43:46,946 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:43:46,946 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:43:46,946 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:43:46,947 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:43:46,948 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:43:46,948 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:43:46,948 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:43:46,948 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:43:46,973 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 20:46:28,468 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.624, 'train_samples_per_second': 129.189, 'train_steps_per_second': 8.167, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.17it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:46:28,574 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:46:28,575 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:46:34,826 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:46:34,827 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:46:34,828 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.62\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.189\n",
            "  train_steps_per_second   =      8.167\n",
            "11/12/2023 20:46:34 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:46:34,843 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:46:34,846 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:46:34,846 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:46:34,846 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.04it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.37it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.66\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    104.835\n",
            "  eval_steps_per_second   =     13.179\n",
            "11/12/2023 20:46:36 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:46:36,519 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:46:36,522 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:46:36,522 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:46:36,522 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.41it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.56\n",
            "  predict_samples_per_second =     66.558\n",
            "  predict_steps_per_second   =      8.342\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:47:10,402 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 20:47:19.838992: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 20:47:19.839056: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 20:47:19.839104: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 20:47:21.807995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 20:47:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 20:47:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_20-47-26_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 20:47:26 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-5803853b015220ff\n",
            "11/12/2023 20:47:26 - INFO - datasets.builder - Using custom data configuration default-5803853b015220ff\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 20:47:26 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 20:47:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:47:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 20:47:27 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 20:47:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:47:27,269 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:47:27,279 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 20:47:27,368 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:47:27,453 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:47:27,454 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:47:27,647 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:47:27,647 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:47:27,647 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:47:27,647 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 20:47:27,647 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:47:27,648 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:47:27,649 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 20:47:27,714 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 20:47:27,715 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 20:47:27,832 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 20:47:34,147 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 20:47:34,147 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "11/12/2023 20:47:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-559d499490b28917.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "11/12/2023 20:47:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19afe7db3ebbd084.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "11/12/2023 20:47:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5803853b015220ff/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e666f05903888cdd.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 20:47:37,864 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 20:47:42,379 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 20:47:57,035 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 20:47:57,035 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 20:47:57,035 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 20:47:57,035 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 20:47:57,035 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 20:47:57,035 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 20:47:57,035 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 20:47:57,037 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 20:47:57,038 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 20:47:57,038 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 20:47:57,038 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 20:47:57,038 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 20:47:57,062 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  1.97it/s][INFO|trainer.py:1956] 2023-11-12 20:50:39,555 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 162.6157, 'train_samples_per_second': 128.401, 'train_steps_per_second': 8.117, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:42<00:00,  8.12it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 20:50:39,656 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 20:50:39,661 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 20:50:46,124 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 20:50:46,128 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 20:50:46,128 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:42.61\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.401\n",
            "  train_steps_per_second   =      8.117\n",
            "11/12/2023 20:50:46 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:50:46,155 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:50:46,157 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:50:46,157 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:50:46,157 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.47it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.48it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.64\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =      106.1\n",
            "  eval_steps_per_second   =     13.338\n",
            "11/12/2023 20:50:47 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 20:50:47,809 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 20:50:47,811 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 20:50:47,811 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 20:50:47,811 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.33it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.89\n",
            "  predict_samples_per_second =     65.915\n",
            "  predict_steps_per_second   =      8.262\n",
            "[INFO|modelcard.py:452] 2023-11-12 20:51:22,011 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################SCITE as train and fincausal data as test###########################"
      ],
      "metadata": {
        "id": "ZF2EFLmpOLke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip '/content/data.zip' -d '/content/'"
      ],
      "metadata": {
        "id": "KDAanyoGObyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/content/data/fin_causal_2022/new_data/task2_2022_addition-dev.csv\n",
        "#data_test = pd.read_json('/content/task2_2022_addition-dev.json',lines=True)   #/content/data/fin_causal_2022/new_data"
      ],
      "metadata": {
        "id": "somF0psURMgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feedback_dict = {'B-Effect': 'E', 'I-Effect': 'E', 'B-Cause': 'C', 'I-Cause': 'C'}\n",
        "# data_test['ner_changed'] = data_test['ner'].apply(lambda x : [feedback_dict.get(i,i)  for i in list(x)])\n",
        "#data_test['ner_changed'].replace(, regex=True)"
      ],
      "metadata": {
        "id": "_pcJiA8cRNE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Luu9YWU6R6mi",
        "outputId": "27069399-a6e0-4101-cd63-683cea60026f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [During, 2020, ,, the, Company, also, issued, ...   \n",
              "1    [The, significant, increases, in, earnings, as...   \n",
              "2    [The, provision, for, credit, losses, was, $, ...   \n",
              "3    [Higher, net, charge-offs, also, contributed, ...   \n",
              "4    [In, addition, to, the, impacts, from, the, Me...   \n",
              "..                                                 ...   \n",
              "260  [In, addition, ,, corporate, and, other, gener...   \n",
              "261  [The, provision, for, credit, losses, increase...   \n",
              "262  [Depreciation, and, amortization, increased, p...   \n",
              "263  [We, recorded, a, loss, of, $, 13.1, million, ...   \n",
              "264  [During, the, year, ended, December, 31, ,, 20...   \n",
              "\n",
              "                                                   ner     id  \\\n",
              "0    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    NaN   \n",
              "1    [E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...    4.0   \n",
              "2    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    7.0   \n",
              "3    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    8.0   \n",
              "4    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    9.0   \n",
              "..                                                 ...    ...   \n",
              "260  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...  526.0   \n",
              "261  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  528.0   \n",
              "262  [E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...  531.0   \n",
              "263  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  534.0   \n",
              "264  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  537.0   \n",
              "\n",
              "                                               postfix  \\\n",
              "0                                                  NaN   \n",
              "1    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...   \n",
              "2    [ ,  ,  ,  ,  ,  , ,  , ,  ,  ,  , ,  ,  ,  , ...   \n",
              "3    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...   \n",
              "4    [ ,  ,  ,  ,  ,  ,  , ,  ,  ,  ,  , ,  ,  ,  ,...   \n",
              "..                                                 ...   \n",
              "260  [ , ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  , ,  ,  , ...   \n",
              "261  [ ,  ,  ,  ,  ,  , ,  , ,  , ,  , ,  , ,  , , ...   \n",
              "262  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...   \n",
              "263  [ ,  ,  ,  ,  , ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,...   \n",
              "264  [ ,  ,  ,  ,  , ,  , ,  ,  , ,  ,  , ,  ,  , ,...   \n",
              "\n",
              "                                           ner_changed  \n",
              "0    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "1    [E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...  \n",
              "2    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "3    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "4    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "..                                                 ...  \n",
              "260  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...  \n",
              "261  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "262  [E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...  \n",
              "263  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "264  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "\n",
              "[265 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-59479552-48d7-45a9-a114-88c42b4e2a9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>id</th>\n",
              "      <th>postfix</th>\n",
              "      <th>ner_changed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[During, 2020, ,, the, Company, also, issued, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, significant, increases, in, earnings, as...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, provision, for, credit, losses, was, $, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  , ,  , ,  ,  ,  , ,  ,  ,  , ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Higher, net, charge-offs, also, contributed, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[In, addition, to, the, impacts, from, the, Me...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  , ,  ,  ,  ,  , ,  ,  ,  ,...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>[In, addition, ,, corporate, and, other, gener...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...</td>\n",
              "      <td>526.0</td>\n",
              "      <td>[ , ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  , ,  ,  , ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>[The, provision, for, credit, losses, increase...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>528.0</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  , ,  , ,  , ,  , ,  , ,  , , ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>[Depreciation, and, amortization, increased, p...</td>\n",
              "      <td>[E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...</td>\n",
              "      <td>531.0</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...</td>\n",
              "      <td>[E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>[We, recorded, a, loss, of, $, 13.1, million, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>534.0</td>\n",
              "      <td>[ ,  ,  ,  ,  , ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>[During, the, year, ended, December, 31, ,, 20...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>537.0</td>\n",
              "      <td>[ ,  ,  ,  ,  , ,  , ,  ,  , ,  ,  , ,  ,  , ,...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>265 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59479552-48d7-45a9-a114-88c42b4e2a9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-e4e973af-5a55-4cd5-8888-cefc3aacd230\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4e973af-5a55-4cd5-8888-cefc3aacd230')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-e4e973af-5a55-4cd5-8888-cefc3aacd230 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59479552-48d7-45a9-a114-88c42b4e2a9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59479552-48d7-45a9-a114-88c42b4e2a9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_test.to_excel('/content/fincausal_testdata.xlsx')"
      ],
      "metadata": {
        "id": "dNgti3VUR-fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_fincausal_test = pd.read_json('/content/Fincausal-test.json', lines=True)"
      ],
      "metadata": {
        "id": "QrGGWBXcSIP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fincausal_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RzeXNZbpSYjb",
        "outputId": "f41b0e7a-72c8-471c-ed44-1fcf01ad8997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [During, 2020, ,, the, Company, also, issued, ...   \n",
              "1    [The, significant, increases, in, earnings, as...   \n",
              "2    [The, provision, for, credit, losses, was, $, ...   \n",
              "3    [Higher, net, charge-offs, also, contributed, ...   \n",
              "4    [In, addition, to, the, impacts, from, the, Me...   \n",
              "..                                                 ...   \n",
              "260  [In, addition, ,, corporate, and, other, gener...   \n",
              "261  [The, provision, for, credit, losses, increase...   \n",
              "262  [Depreciation, and, amortization, increased, p...   \n",
              "263  [We, recorded, a, loss, of, $, 13.1, million, ...   \n",
              "264  [During, the, year, ended, December, 31, ,, 20...   \n",
              "\n",
              "                                                   ner   id  \\\n",
              "0    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    3   \n",
              "1    [E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...    4   \n",
              "2    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    7   \n",
              "3    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    8   \n",
              "4    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...    9   \n",
              "..                                                 ...  ...   \n",
              "260  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...  526   \n",
              "261  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  528   \n",
              "262  [E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...  531   \n",
              "263  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  534   \n",
              "264  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  537   \n",
              "\n",
              "                                               postfix  \n",
              "0    [ , ,  ,  ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,  , , ...  \n",
              "1    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...  \n",
              "2    [ ,  ,  ,  ,  ,  , ,  , ,  ,  ,  , ,  ,  ,  , ...  \n",
              "3    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...  \n",
              "4    [ ,  ,  ,  ,  ,  ,  , ,  ,  ,  ,  , ,  ,  ,  ,...  \n",
              "..                                                 ...  \n",
              "260  [ , ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  , ,  ,  , ...  \n",
              "261  [ ,  ,  ,  ,  ,  , ,  , ,  , ,  , ,  , ,  , , ...  \n",
              "262  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...  \n",
              "263  [ ,  ,  ,  ,  , ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,...  \n",
              "264  [ ,  ,  ,  ,  , ,  , ,  ,  , ,  ,  , ,  ,  , ,...  \n",
              "\n",
              "[265 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9cd0cee-7ccd-42f7-b3dd-689abdb602c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>id</th>\n",
              "      <th>postfix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[During, 2020, ,, the, Company, also, issued, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>3</td>\n",
              "      <td>[ , ,  ,  ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,  , , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, significant, increases, in, earnings, as...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...</td>\n",
              "      <td>4</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, provision, for, credit, losses, was, $, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>7</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  , ,  , ,  ,  ,  , ,  ,  ,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Higher, net, charge-offs, also, contributed, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>8</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[In, addition, to, the, impacts, from, the, Me...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  , ,  ,  ,  ,  , ,  ,  ,  ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>[In, addition, ,, corporate, and, other, gener...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...</td>\n",
              "      <td>526</td>\n",
              "      <td>[ , ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  , ,  ,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>[The, provision, for, credit, losses, increase...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>528</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  , ,  , ,  , ,  , ,  , ,  , , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>[Depreciation, and, amortization, increased, p...</td>\n",
              "      <td>[E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...</td>\n",
              "      <td>531</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>[We, recorded, a, loss, of, $, 13.1, million, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>534</td>\n",
              "      <td>[ ,  ,  ,  ,  , ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>[During, the, year, ended, December, 31, ,, 20...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>537</td>\n",
              "      <td>[ ,  ,  ,  ,  , ,  , ,  ,  , ,  ,  , ,  ,  , ,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>265 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9cd0cee-7ccd-42f7-b3dd-689abdb602c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9cd0cee-7ccd-42f7-b3dd-689abdb602c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9cd0cee-7ccd-42f7-b3dd-689abdb602c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-db0b6db9-27af-43eb-b387-903900772ca3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-db0b6db9-27af-43eb-b387-903900772ca3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-db0b6db9-27af-43eb-b387-903900772ca3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4b50e143-33e9-4329-b9df-18a32c866308\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_fincausal_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4b50e143-33e9-4329-b9df-18a32c866308 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_fincausal_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_test[[\"words\", \"ner\"]].rename(columns={\"words\": \"A\", \"ner\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "s1, s2 = [json.dumps(tokens_test) for tokens_test in dataset_test.A.values], [json.dumps(labels_test) for labels_test in dataset_test.B.values]\n",
        "\n",
        "X_raw_test, y_raw_test = [json.loads(token_test) for token_test in s1], [json.loads(label_test) for label_test in s2]"
      ],
      "metadata": {
        "id": "aC1SxyJqRQPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners2 = []\n",
        "for i,j in zip(X_raw_test, y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners2.append(_new_ner)"
      ],
      "metadata": {
        "id": "Sz23x82eVNKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners2[i] for i in range(len(new_ners2))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0VUzAh0VPxU",
        "outputId": "733446cf-c07c-422f-d4b6-c532166088d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "Xq9-tSD0VKFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "h3D2mlAXjeq7",
        "outputId": "762770d4-d6f5-4173-d46e-a84eb90df40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  words  \\\n",
              "0     [When , a , policyholder , or , insured , gets...   \n",
              "1     [During , 2020 , in , response , to , the , on...   \n",
              "2     [Prolonged , periods , of , low , interest , r...   \n",
              "3     [Conversely , a , rise , in , interest , rates...   \n",
              "4     [Further , because , of , the , concentration ...   \n",
              "...                                                 ...   \n",
              "2229  [The , increase , in , adjusted , PPNR , was ,...   \n",
              "2230  [Modest , noninterest , income , growth , due ...   \n",
              "2231  [Wealth , management , and , trust , fees , in...   \n",
              "2232  [The , 2019 , provision , increased , due , to...   \n",
              "2233  [The , 2019 , provision , increased , due , to...   \n",
              "\n",
              "                                                    ner  \\\n",
              "0     [O, O, C, C, C, C, C, C, C, O, O, CE, E, E, E,...   \n",
              "1     [O, O, O, O, O, C, C, C, C, C, C, C, C, C, C, ...   \n",
              "2     [C, C, C, C, C, C, O, CE, E, E, O, O, O, CE, O...   \n",
              "3     [O, O, C, C, C, C, O, CE, O, O, O, O, O, O, E,...   \n",
              "4     [O, CE, O, O, C, C, C, C, C, C, C, C, C, C, C,...   \n",
              "...                                                 ...   \n",
              "2229  [O, E, E, E, E, E, E, E, E, E, E, E, E, CE, CE...   \n",
              "2230          [E, E, E, E, CE, CE, C, C, C, C, C, C, C]   \n",
              "2231  [E, E, E, E, E, E, CE, CE, CE, O, C, C, C, C, ...   \n",
              "2232  [O, O, E, E, CE, CE, C, C, C, C, C, O, C, C, C...   \n",
              "2233  [O, O, E, E, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "\n",
              "                                             liststring  \n",
              "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E  \n",
              "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...  \n",
              "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...  \n",
              "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...  \n",
              "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...  \n",
              "...                                                 ...  \n",
              "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C  \n",
              "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C  \n",
              "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C  \n",
              "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C  \n",
              "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...  \n",
              "\n",
              "[2234 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88288d4f-a6b6-45f0-8dde-a1527b1f600e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>liststring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[When , a , policyholder , or , insured , gets...</td>\n",
              "      <td>[O, O, C, C, C, C, C, C, C, O, O, CE, E, E, E,...</td>\n",
              "      <td>O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[During , 2020 , in , response , to , the , on...</td>\n",
              "      <td>[O, O, O, O, O, C, C, C, C, C, C, C, C, C, C, ...</td>\n",
              "      <td>O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Prolonged , periods , of , low , interest , r...</td>\n",
              "      <td>[C, C, C, C, C, C, O, CE, E, E, O, O, O, CE, O...</td>\n",
              "      <td>C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Conversely , a , rise , in , interest , rates...</td>\n",
              "      <td>[O, O, C, C, C, C, O, CE, O, O, O, O, O, O, E,...</td>\n",
              "      <td>O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Further , because , of , the , concentration ...</td>\n",
              "      <td>[O, CE, O, O, C, C, C, C, C, C, C, C, C, C, C,...</td>\n",
              "      <td>O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2229</th>\n",
              "      <td>[The , increase , in , adjusted , PPNR , was ,...</td>\n",
              "      <td>[O, E, E, E, E, E, E, E, E, E, E, E, E, CE, CE...</td>\n",
              "      <td>O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2230</th>\n",
              "      <td>[Modest , noninterest , income , growth , due ...</td>\n",
              "      <td>[E, E, E, E, CE, CE, C, C, C, C, C, C, C]</td>\n",
              "      <td>E,E,E,E,CE,CE,C,C,C,C,C,C,C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231</th>\n",
              "      <td>[Wealth , management , and , trust , fees , in...</td>\n",
              "      <td>[E, E, E, E, E, E, CE, CE, CE, O, C, C, C, C, ...</td>\n",
              "      <td>E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2232</th>\n",
              "      <td>[The , 2019 , provision , increased , due , to...</td>\n",
              "      <td>[O, O, E, E, CE, CE, C, C, C, C, C, O, C, C, C...</td>\n",
              "      <td>O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2233</th>\n",
              "      <td>[The , 2019 , provision , increased , due , to...</td>\n",
              "      <td>[O, O, E, E, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2234 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88288d4f-a6b6-45f0-8dde-a1527b1f600e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88288d4f-a6b6-45f0-8dde-a1527b1f600e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88288d4f-a6b6-45f0-8dde-a1527b1f600e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-194715db-54ef-4943-b54a-8bbc40b1295c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-194715db-54ef-4943-b54a-8bbc40b1295c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-194715db-54ef-4943-b54a-8bbc40b1295c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_de036fdc-19a1-48ec-969e-aa96d5626046\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_de036fdc-19a1-48ec-969e-aa96d5626046 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "_bnujpEXVclw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyWU31kkVaHE",
        "outputId": "0a9e1712-e1c5-4fbd-b52b-67a426c3fad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 21:00:38.980077: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:00:38.980135: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:00:38.980195: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:00:41.051407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:00:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:00:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-00-45_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:00:45 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:00:45 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 17452.03it/s]\n",
            "Downloading took 0.0 min\n",
            "11/12/2023 21:00:45 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/12/2023 21:00:45 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 659.65it/s]\n",
            "Generating train split\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 696 examples [00:00, 70131.79 examples/s]\n",
            "Generating validation split\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 175 examples [00:00, 91750.40 examples/s]\n",
            "Generating test split\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 2234 examples [00:00, 112417.07 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/12/2023 21:00:45 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/12/2023 21:00:45 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:00:46,271 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:00:46,279 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:00:46,366 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:00:46,451 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:00:46,451 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:00:46,626 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:00:46,626 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:00:46,626 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:00:46,626 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:00:46,626 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:00:46,626 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:00:46,627 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:00:46,661 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:00:46,662 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:00:46,759 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:00:55,668 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:00:55,668 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/696 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2692] 2023-11-12 21:00:55,736 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:00:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Running tokenizer on train dataset: 100% 696/696 [00:00<00:00, 2442.34 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/175 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b55da7821a37263e.arrow\n",
            "11/12/2023 21:00:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b55da7821a37263e.arrow\n",
            "Running tokenizer on validation dataset: 100% 175/175 [00:00<00:00, 2778.38 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/2234 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7e965c12dcf607d9.arrow\n",
            "11/12/2023 21:00:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7e965c12dcf607d9.arrow\n",
            "Running tokenizer on prediction dataset: 100% 2234/2234 [00:01<00:00, 1870.75 examples/s]\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:01:00,928 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:01:06,000 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:01:21,680 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:01:21,680 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:01:21,680 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:01:21,680 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:01:21,680 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:01:21,680 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:01:21,680 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:01:21,682 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:01:21,683 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:01:21,683 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:01:21,683 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:01:21,683 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:01:21,713 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:38<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 21:04:00,682 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 159.0979, 'train_samples_per_second': 131.24, 'train_steps_per_second': 8.297, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:39<00:00,  8.30it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:04:00,783 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:04:00,785 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:04:07,720 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:04:07,721 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:04:07,721 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:39.09\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =     131.24\n",
            "  train_steps_per_second   =      8.297\n",
            "11/12/2023 21:04:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:04:07,736 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:04:07,738 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:04:07,738 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:04:07,738 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 14.82it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.44it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.65\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    105.574\n",
            "  eval_steps_per_second   =     13.272\n",
            "11/12/2023 21:04:09 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:04:09,398 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:04:09,400 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:04:09,400 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:04:09,401 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.34it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.85\n",
            "  predict_samples_per_second =     65.981\n",
            "  predict_steps_per_second   =       8.27\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:04:43,553 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:04:52.972463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:04:52.972520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:04:52.972560: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:04:54.859021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:04:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:04:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-04-58_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:04:59 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:04:59 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:04:59 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:04:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:04:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:04:59 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:04:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:04:59,989 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:05:00,000 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:05:00,093 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:05:00,182 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:05:00,183 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:05:00,355 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:05:00,355 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:05:00,355 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:05:00,355 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:05:00,355 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:05:00,356 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:05:00,357 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:05:00,412 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:05:00,413 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:05:00,525 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:05:07,787 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:05:07,788 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:05:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/175 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2692] 2023-11-12 21:05:07,858 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:05:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Running tokenizer on validation dataset: 100% 175/175 [00:00<00:00, 3284.66 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7e965c12dcf607d9.arrow\n",
            "11/12/2023 21:05:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7e965c12dcf607d9.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:05:12,364 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:05:17,774 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:05:29,163 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:05:29,163 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:05:29,163 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:05:29,163 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:05:29,163 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:05:29,163 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:05:29,163 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:05:29,165 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:05:29,166 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:05:29,166 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:05:29,166 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:05:29,166 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:05:29,217 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  2.00it/s][INFO|trainer.py:1956] 2023-11-12 21:08:11,977 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 162.917, 'train_samples_per_second': 128.163, 'train_steps_per_second': 8.102, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:42<00:00,  8.10it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:08:12,084 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:08:12,089 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:08:24,497 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:08:24,508 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:08:24,508 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:42.91\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.163\n",
            "  train_steps_per_second   =      8.102\n",
            "11/12/2023 21:08:24 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:08:24,537 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:08:24,540 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:08:24,541 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:08:24,541 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.20it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.94it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.71\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    102.021\n",
            "  eval_steps_per_second   =     12.825\n",
            "11/12/2023 21:08:26 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:08:26,260 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:08:26,263 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:08:26,263 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:08:26,263 >>   Batch size = 8\n",
            "100% 280/280 [00:31<00:00,  8.87it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:31.85\n",
            "  predict_samples_per_second =     70.137\n",
            "  predict_steps_per_second   =      8.791\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:08:58,371 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:09:09.764099: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:09:09.764158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:09:09.764211: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:09:12.314034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:09:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:09:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-09-15_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:09:16 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:09:16 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:09:16 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:09:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:09:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:09:16 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:09:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:09:16,959 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:09:16,967 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:09:17,055 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:09:17,144 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:09:17,145 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:09:17,333 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:09:17,333 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:09:17,333 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:09:17,333 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:09:17,333 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:09:17,333 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:09:17,334 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:09:17,368 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:09:17,369 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:09:17,464 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:09:23,877 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:09:23,877 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:09:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:09:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/2234 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2692] 2023-11-12 21:09:24,061 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:09:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "Running tokenizer on prediction dataset: 100% 2234/2234 [00:01<00:00, 1642.75 examples/s]\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:09:31,032 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:09:34,529 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:09:50,522 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:09:50,522 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:09:50,522 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:09:50,522 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:09:50,522 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:09:50,523 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:09:50,523 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:09:50,525 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:09:50,526 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:09:50,526 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:09:50,526 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:09:50,526 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:09:50,615 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:42<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 21:12:32,743 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 162.3149, 'train_samples_per_second': 128.639, 'train_steps_per_second': 8.132, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:42<00:00,  8.13it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:12:32,843 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:12:32,845 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:12:39,656 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:12:39,658 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:12:39,658 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:42.31\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    128.639\n",
            "  train_steps_per_second   =      8.132\n",
            "11/12/2023 21:12:39 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:12:39,673 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:12:39,743 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:12:39,743 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:12:39,743 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.14it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.67it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.63\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    107.025\n",
            "  eval_steps_per_second   =     13.455\n",
            "11/12/2023 21:12:41 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:12:41,381 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:12:41,383 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:12:41,383 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:12:41,383 >>   Batch size = 8\n",
            "100% 280/280 [00:31<00:00,  8.85it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:31.92\n",
            "  predict_samples_per_second =     69.974\n",
            "  predict_steps_per_second   =       8.77\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:13:13,504 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:13:25.206380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:13:25.206431: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:13:25.206468: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:13:27.146141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:13:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:13:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-13-30_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:13:30 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:13:31 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:13:31 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:13:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:13:31 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:13:31 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:13:31 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:13:31,521 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:13:31,530 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:13:31,622 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:13:31,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:13:31,714 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:13:31,888 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:13:31,888 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:13:31,888 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:13:31,888 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:13:31,889 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:13:31,889 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:13:31,890 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:13:31,926 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:13:31,927 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:13:32,024 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:13:40,344 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:13:40,345 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:13:40 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:13:46,874 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:13:50,607 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:14:05,831 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:14:05,831 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:14:05,831 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:14:05,831 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:14:05,831 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:14:05,831 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:14:05,831 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:14:05,832 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:14:05,833 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:14:05,833 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:14:05,833 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:14:05,833 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:14:05,858 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:41<00:00,  1.97it/s][INFO|trainer.py:1956] 2023-11-12 21:16:47,198 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 161.4625, 'train_samples_per_second': 129.318, 'train_steps_per_second': 8.175, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:41<00:00,  8.18it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:16:47,299 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:16:47,300 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:16:53,776 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:16:53,778 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:16:53,778 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:41.46\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    129.318\n",
            "  train_steps_per_second   =      8.175\n",
            "11/12/2023 21:16:53 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:16:53,796 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:16:53,798 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:16:53,799 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:16:53,799 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 14.98it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.53it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.64\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    106.309\n",
            "  eval_steps_per_second   =     13.365\n",
            "11/12/2023 21:16:55 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:16:55,448 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:16:55,450 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:16:55,450 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:16:55,450 >>   Batch size = 8\n",
            "100% 280/280 [00:31<00:00,  8.85it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:31.92\n",
            "  predict_samples_per_second =     69.978\n",
            "  predict_steps_per_second   =      8.771\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:17:27,558 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:17:38.782529: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:17:38.782595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:17:38.782640: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:17:40.814929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:17:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:17:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-17-44_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:17:44 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:17:44 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:17:44 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:17:44 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:17:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:17:44 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:17:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:17:44,937 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:17:44,945 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:17:45,066 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:17:45,153 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:17:45,153 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:17:45,335 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:17:45,335 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:17:45,335 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:17:45,335 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:17:45,336 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:17:45,336 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:17:45,337 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:17:45,371 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:17:45,371 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:17:45,467 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:17:53,338 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:17:53,338 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:17:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:17:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:17:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:17:57,193 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:18:02,416 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:18:18,082 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:18:18,082 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:18:18,082 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:18:18,082 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:18:18,082 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:18:18,083 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:18:18,083 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:18:18,085 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:18:18,086 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:18:18,086 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:18:18,086 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:18:18,086 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:18:18,119 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:38<00:00,  1.97it/s][INFO|trainer.py:1956] 2023-11-12 21:20:57,080 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 159.0923, 'train_samples_per_second': 131.245, 'train_steps_per_second': 8.297, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:39<00:00,  8.30it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:20:57,180 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:20:57,182 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:21:06,778 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:21:06,781 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:21:06,782 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:39.09\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    131.245\n",
            "  train_steps_per_second   =      8.297\n",
            "11/12/2023 21:21:06 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:21:06,804 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:21:06,810 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:21:06,810 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:21:06,810 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.00it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.14it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.69\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    103.342\n",
            "  eval_steps_per_second   =     12.992\n",
            "11/12/2023 21:21:08 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:21:08,506 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:21:08,508 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:21:08,509 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:21:08,509 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.41it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.58\n",
            "  predict_samples_per_second =      66.52\n",
            "  predict_steps_per_second   =      8.337\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:21:42,397 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:21:54.913486: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:21:54.913543: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:21:54.913589: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:21:56.808003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:22:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:22:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-22-00_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:22:00 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:22:00 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:22:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:22:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:22:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:22:00 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:22:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:22:01,045 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:22:01,053 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:22:01,140 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:22:01,228 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:22:01,229 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:22:01,406 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:22:01,406 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:22:01,406 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:22:01,406 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:22:01,406 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:22:01,407 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:22:01,408 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:22:01,441 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:22:01,441 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:22:01,535 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:22:09,978 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:22:09,978 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:22:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:22:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:22:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:22:14,636 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:22:18,674 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:22:35,828 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:22:35,828 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:22:35,828 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:22:35,828 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:22:35,828 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:22:35,828 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:22:35,828 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:22:35,831 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:22:35,832 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:22:35,833 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:22:35,833 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:22:35,833 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:22:35,919 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:39<00:00,  1.97it/s][INFO|trainer.py:1956] 2023-11-12 21:25:15,576 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 159.8408, 'train_samples_per_second': 130.63, 'train_steps_per_second': 8.258, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:39<00:00,  8.26it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:25:15,675 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:25:15,677 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:25:22,147 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:25:22,148 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:25:22,148 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:39.84\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =     130.63\n",
            "  train_steps_per_second   =      8.258\n",
            "11/12/2023 21:25:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:25:22,174 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:25:22,182 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:25:22,182 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:25:22,183 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 14.89it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.21it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.68\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    103.963\n",
            "  eval_steps_per_second   =      13.07\n",
            "11/12/2023 21:25:23 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:25:23,869 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:25:23,871 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:25:23,871 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:25:23,871 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.39it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.66\n",
            "  predict_samples_per_second =     66.351\n",
            "  predict_steps_per_second   =      8.316\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:25:57,873 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:26:11.582566: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:26:11.582617: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:26:11.582667: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:26:13.580542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:26:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:26:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-26-17_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:26:17 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:26:18 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:26:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:26:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:26:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:26:18 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:26:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:26:18,469 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:26:18,479 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:26:18,571 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:26:18,658 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:26:18,660 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:26:18,845 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:26:18,845 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:26:18,845 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:26:18,845 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:26:18,845 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:26:18,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:26:18,847 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:26:18,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:26:18,912 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:26:19,025 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:26:27,012 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:26:27,012 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:26:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:26:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:26:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:26:32,898 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:26:37,276 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:26:53,899 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:26:53,899 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:26:53,899 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:26:53,899 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:26:53,899 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:26:53,899 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:26:53,899 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:26:53,901 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:26:53,902 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:26:53,902 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:26:53,902 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:26:53,902 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:26:53,928 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:38<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 21:29:32,525 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 158.7207, 'train_samples_per_second': 131.552, 'train_steps_per_second': 8.316, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:38<00:00,  8.32it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:29:32,625 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:29:32,627 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:29:43,521 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:29:43,523 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:29:43,523 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:38.72\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    131.552\n",
            "  train_steps_per_second   =      8.316\n",
            "11/12/2023 21:29:43 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:29:43,553 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:29:43,556 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:29:43,556 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:29:43,556 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.11it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.63it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.75\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =      99.82\n",
            "  eval_steps_per_second   =     12.549\n",
            "11/12/2023 21:29:45 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:29:45,313 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:29:45,316 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:29:45,316 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:29:45,316 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.36it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.79\n",
            "  predict_samples_per_second =     66.097\n",
            "  predict_steps_per_second   =      8.284\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:30:19,778 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:30:30.294220: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:30:30.294287: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:30:30.294328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:30:32.671446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:30:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:30:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-30-37_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:30:37 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:30:37 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:30:37 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:30:37 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:30:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:30:37 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:30:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:30:38,106 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:30:38,114 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:30:38,328 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:30:38,415 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:30:38,416 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:30:38,597 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:30:38,597 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:30:38,597 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:30:38,597 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:30:38,597 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:30:38,598 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:30:38,601 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:30:38,652 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:30:38,653 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:30:38,759 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:30:44,629 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:30:44,630 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:30:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:30:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:30:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:30:47,804 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:30:52,938 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:31:08,625 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:31:08,625 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:31:08,625 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:31:08,625 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:31:08,625 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:31:08,625 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:31:08,625 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:31:08,627 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:31:08,628 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:31:08,628 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:31:08,628 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:31:08,628 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:31:08,695 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:39<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 21:33:48,101 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 159.5792, 'train_samples_per_second': 130.844, 'train_steps_per_second': 8.272, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:39<00:00,  8.27it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:33:48,209 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:33:48,210 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:33:54,332 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:33:54,335 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:33:54,335 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:39.57\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    130.844\n",
            "  train_steps_per_second   =      8.272\n",
            "11/12/2023 21:33:54 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:33:54,360 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:33:54,366 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:33:54,366 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:33:54,366 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.04it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.30it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.67\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    104.381\n",
            "  eval_steps_per_second   =     13.122\n",
            "11/12/2023 21:33:56 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:33:56,046 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:33:56,048 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:33:56,048 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:33:56,048 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.40it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.63\n",
            "  predict_samples_per_second =     66.413\n",
            "  predict_steps_per_second   =      8.324\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:34:29,998 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:34:42.834465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:34:42.834521: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:34:42.834568: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:34:45.552394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:34:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:34:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-34-50_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:34:50 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:34:50 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:34:50 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:34:50 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:34:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:34:51 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:34:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:34:51,271 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:34:51,281 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:34:51,372 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:34:51,458 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:34:51,460 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:34:51,635 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:34:51,635 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:34:51,635 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:34:51,635 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:34:51,635 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:34:51,635 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:34:51,636 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:34:51,698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:34:51,699 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:34:51,810 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:34:57,166 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:34:57,166 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:34:57 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:34:57 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:34:57 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:35:01,016 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:35:05,814 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:35:22,334 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:35:22,334 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:35:22,335 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:35:22,335 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:35:22,335 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:35:22,335 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:35:22,335 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:35:22,341 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:35:22,342 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:35:22,342 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:35:22,342 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:35:22,342 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:35:22,395 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:39<00:00,  1.99it/s][INFO|trainer.py:1956] 2023-11-12 21:38:01,936 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 159.7, 'train_samples_per_second': 130.745, 'train_steps_per_second': 8.265, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:39<00:00,  8.27it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:38:02,043 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:38:02,045 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:38:08,323 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:38:08,324 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:38:08,324 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:39.69\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    130.745\n",
            "  train_steps_per_second   =      8.265\n",
            "11/12/2023 21:38:08 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:38:08,349 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:38:08,352 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:38:08,352 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:38:08,352 >>   Batch size = 8\n",
            " 95% 21/22 [00:01<00:00, 15.02it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 14.28it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.67\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =    104.353\n",
            "  eval_steps_per_second   =     13.119\n",
            "11/12/2023 21:38:10 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:38:10,033 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:38:10,035 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:38:10,035 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:38:10,035 >>   Batch size = 8\n",
            "100% 280/280 [00:33<00:00,  8.38it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:33.71\n",
            "  predict_samples_per_second =     66.257\n",
            "  predict_steps_per_second   =      8.304\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:38:44,056 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n",
            "2023-11-12 21:38:55.762583: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 21:38:55.762639: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 21:38:55.762736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-12 21:38:58.645816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/12/2023 21:39:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/12/2023 21:39:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov12_21-39-03_9f7782d0b5dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/12/2023 21:39:03 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-1000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-272f65ebaff2e318\n",
            "11/12/2023 21:39:04 - INFO - datasets.builder - Using custom data configuration default-272f65ebaff2e318\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/12/2023 21:39:04 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/12/2023 21:39:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:39:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/12/2023 21:39:04 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/12/2023 21:39:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:39:04,385 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:39:04,396 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:568] 2023-11-12 21:39:04,485 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:39:04,575 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:39:04,576 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:39:04,750 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:39:04,750 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:39:04,750 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:39:04,750 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-11-12 21:39:04,750 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:39:04,751 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:39:04,751 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-12 21:39:04,800 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-12 21:39:04,801 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-12 21:39:04,913 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-12 21:39:10,820 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-12 21:39:10,820 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "11/12/2023 21:39:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e556981100138453.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "11/12/2023 21:39:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-57cc1bbc87df82cf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "11/12/2023 21:39:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-272f65ebaff2e318/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3e7eabe093a51da0.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-12 21:39:16,553 >> Loading model from /content/test-ner-2022/checkpoint-1000.\n",
            "[INFO|trainer.py:738] 2023-11-12 21:39:21,337 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-12 21:39:32,784 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-12 21:39:32,784 >>   Num examples = 696\n",
            "[INFO|trainer.py:1726] 2023-11-12 21:39:32,784 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-12 21:39:32,784 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-12 21:39:32,784 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-12 21:39:32,784 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-12 21:39:32,784 >>   Total optimization steps = 1,320\n",
            "[INFO|trainer.py:1733] 2023-11-12 21:39:32,786 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-12 21:39:32,786 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-12 21:39:32,786 >>   Continuing training from epoch 22\n",
            "[INFO|trainer.py:1755] 2023-11-12 21:39:32,786 >>   Continuing training from global step 1000\n",
            "[INFO|trainer.py:1757] 2023-11-12 21:39:32,786 >>   Will skip the first 22 epochs then the first 32 batches in the first epoch.\n",
            "  0% 0/1320 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-12 21:39:32,849 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1320/1320 [02:43<00:00,  1.98it/s][INFO|trainer.py:1956] 2023-11-12 21:42:16,012 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 163.3298, 'train_samples_per_second': 127.839, 'train_steps_per_second': 8.082, 'train_loss': 0.0006603484352429707, 'epoch': 30.0}\n",
            "100% 1320/1320 [02:43<00:00,  8.08it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-12 21:42:16,118 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-12 21:42:16,120 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-12 21:42:26,541 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-11-12 21:42:26,545 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-11-12 21:42:26,545 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0007\n",
            "  train_runtime            = 0:02:43.32\n",
            "  train_samples            =        696\n",
            "  train_samples_per_second =    127.839\n",
            "  train_steps_per_second   =      8.082\n",
            "11/12/2023 21:42:26 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:42:26,569 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:42:26,572 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:42:26,572 >>   Num examples = 175\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:42:26,573 >>   Batch size = 8\n",
            "100% 22/22 [00:01<00:00, 15.39it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 22/22 [00:01<00:00, 13.41it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9715\n",
            "  eval_f1                 =      0.943\n",
            "  eval_loss               =     0.1546\n",
            "  eval_precision          =     0.9277\n",
            "  eval_recall             =     0.9588\n",
            "  eval_runtime            = 0:00:01.77\n",
            "  eval_samples            =        175\n",
            "  eval_samples_per_second =     98.663\n",
            "  eval_steps_per_second   =     12.403\n",
            "11/12/2023 21:42:28 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-12 21:42:28,353 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-12 21:42:28,356 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-12 21:42:28,356 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3165] 2023-11-12 21:42:28,356 >>   Batch size = 8\n",
            "100% 280/280 [00:34<00:00,  8.22it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.2683\n",
            "  predict_f1                 =      0.212\n",
            "  predict_loss               =     5.4943\n",
            "  predict_precision          =     0.6826\n",
            "  predict_recall             =     0.1255\n",
            "  predict_runtime            = 0:00:34.36\n",
            "  predict_samples_per_second =      65.01\n",
            "  predict_steps_per_second   =      8.148\n",
            "[INFO|modelcard.py:452] 2023-11-12 21:43:03,074 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9276807980049875}, {'name': 'Recall', 'type': 'recall', 'value': 0.9587628865979382}, {'name': 'F1', 'type': 'f1', 'value': 0.9429657794676805}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9714560615779346}]}\n",
            "{'preds':                                                   preds\n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O\n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...\n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...\n",
            "...                                                 ...\n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O\n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O\n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O\n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0              O,O,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,CE,E,E,E,E,O...   \n",
            "2     C,C,C,C,C,C,O,CE,E,E,O,O,O,CE,O,O,O,CE,O,CE,O,...   \n",
            "3     O,O,C,C,C,C,O,CE,O,O,O,O,O,O,E,E,E,E,E,E,E,CE,...   \n",
            "4     O,CE,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O...   \n",
            "...                                                 ...   \n",
            "2229    O,E,E,E,E,E,E,E,E,E,E,E,E,CE,CE,C,C,O,C,C,C,C,C   \n",
            "2230                        E,E,E,E,CE,CE,C,C,C,C,C,C,C   \n",
            "2231                 E,E,E,E,E,E,CE,CE,CE,O,C,C,C,C,C,C   \n",
            "2232                  O,O,E,E,CE,CE,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,CE,CE,C,C,C,...   \n",
            "\n",
            "                                                  preds  \n",
            "0               O,O,O,O,O,O,E,O,E,O,O,O,O,O,O,O,O,O,O,O  \n",
            "1     O,O,O,O,O,C,C,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "2     O,O,O,O,O,O,O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O,O,...  \n",
            "3     O,O,C,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "4     O,O,O,C,C,O,O,O,O,O,O,O,O,C,C,O,O,O,O,O,O,O,O,...  \n",
            "...                                                 ...  \n",
            "2229   E,E,O,O,O,O,O,O,O,O,CE,CE,CE,O,O,C,C,O,C,C,O,O,O  \n",
            "2230                          E,E,E,E,O,O,C,C,O,O,O,O,O  \n",
            "2231                    E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    O,O,E,E,O,O,C,C,O,O,O,O,O,O,O,O  \n",
            "2233  O,O,E,E,O,O,C,C,O,O,O,O,E,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.74      0.18      0.29     28263\n",
            "          CE       0.00      0.00      0.00      4436\n",
            "           E       0.71      0.14      0.23     28549\n",
            "           O       0.18      0.89      0.29     12035\n",
            "\n",
            "    accuracy                           0.27     73283\n",
            "   macro avg       0.41      0.30      0.20     73283\n",
            "weighted avg       0.59      0.27      0.25     73283\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased   \\\n",
        "#            --train_file /content/train.json  \\\n",
        "#            --validation_file /content/dev.json  \\\n",
        "#            --test_file /content/content/data/fin_causal_2022/new_data/task2_2022_addition-full-test.json  \\\n",
        "#            --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16"
      ],
      "metadata": {
        "id": "qgtZbAM7O2Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPY95gVpUEhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ FinCausal as train and FinCausal as test#########################################"
      ],
      "metadata": {
        "id": "brY2xgAhUPEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased   \\\n",
        "#            --train_file /content/content/data/fin_causal_2022/fincausal-extra-all-train.json  \\\n",
        "#            --validation_file /content/content/data/fin_causal_2022/new_data/task2_2022_addition-dev.json  \\\n",
        "#            --test_file /content/content/data/fin_causal_2022/new_data/task2_2022_addition-dev.json  \\\n",
        "#            --output_dir /content/content/data/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16  #--classifier_dropout 0.1"
      ],
      "metadata": {
        "id": "KdwEPxZjUVW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in FinCausal.ipynb at gseetha04"
      ],
      "metadata": {
        "id": "54nE73lfV453"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJzJTLJNWb-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased   \\\n",
        "#            --train_file /content/content/data/fin_causal_2022/fincausal-extra-all-train.json  \\\n",
        "#            --validation_file /content/content/data/fin_causal_2022/new_data/task2_2022_addition-dev.json  \\\n",
        "#            --test_file /content/test.json  \\\n",
        "#            --output_dir /content/test-ner --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16  #--classifier_dropout 0.1"
      ],
      "metadata": {
        "id": "LFFOqzexWdAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "Tk0qIt6Ps-g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_train = pd.read_json('/content/fincausal-extra-all-train.json', lines=True)\n",
        "data_train.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "TgxZn36EmGsH",
        "outputId": "07b7edf3-a21b-433e-cf57-f4ddebe1f6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               words  \\\n",
              "0  [It, found, that, total, U.S., health, care, s...   \n",
              "1  [Transat, loss, more, than, doubles, as, it, w...   \n",
              "2  [MONTREAL, -, Transat, AT, Inc., 's, third-qua...   \n",
              "3  [The, Montreal-based, company, lost, $, 11.0, ...   \n",
              "\n",
              "                                                 ner          id  \\\n",
              "0  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  0001.00026   \n",
              "1            [E, E, E, E, E, O, C, C, C, C, C, C, C]  0003.00001   \n",
              "2  [O, O, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  0003.00002   \n",
              "3  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  0003.00003   \n",
              "\n",
              "                                             postfix  \n",
              "0  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ,  ,  ...  \n",
              "1               [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ]  \n",
              "2  [  ,   ,  ,  , ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,...  \n",
              "3  [ ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8636578c-4373-429e-849c-ad4a9615a17e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>id</th>\n",
              "      <th>postfix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[It, found, that, total, U.S., health, care, s...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>0001.00026</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ,  ,  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Transat, loss, more, than, doubles, as, it, w...</td>\n",
              "      <td>[E, E, E, E, E, O, C, C, C, C, C, C, C]</td>\n",
              "      <td>0003.00001</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[MONTREAL, -, Transat, AT, Inc., 's, third-qua...</td>\n",
              "      <td>[O, O, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>0003.00002</td>\n",
              "      <td>[  ,   ,  ,  , ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, Montreal-based, company, lost, $, 11.0, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>0003.00003</td>\n",
              "      <td>[ ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8636578c-4373-429e-849c-ad4a9615a17e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8636578c-4373-429e-849c-ad4a9615a17e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8636578c-4373-429e-849c-ad4a9615a17e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46d5eb28-56d3-478b-894e-40081f04b04c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46d5eb28-56d3-478b-894e-40081f04b04c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46d5eb28-56d3-478b-894e-40081f04b04c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_train = data_train[[\"words\", \"ner\"]].rename(columns={\"words\": \"A\", \"ner\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "s1, s2 = [json.dumps(tokens_test) for tokens_test in dataset_train.A.values], [json.dumps(labels_test) for labels_test in dataset_train.B.values]\n",
        "\n",
        "X_raw, y_raw = [json.loads(token_test) for token_test in s1], [json.loads(label_test) for label_test in s2]"
      ],
      "metadata": {
        "id": "WsbDU7owmGvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/Fincausal-test.json', lines=True)\n",
        "data_test.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "weohPY_QmbAs",
        "outputId": "5f0ea589-5d89-4352-c4c8-e7bc83117010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               words  \\\n",
              "0  [During, 2020, ,, the, Company, also, issued, ...   \n",
              "1  [The, significant, increases, in, earnings, as...   \n",
              "2  [The, provision, for, credit, losses, was, $, ...   \n",
              "3  [Higher, net, charge-offs, also, contributed, ...   \n",
              "\n",
              "                                                 ner  id  \\\n",
              "0  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   3   \n",
              "1  [E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...   4   \n",
              "2  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   7   \n",
              "3  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   8   \n",
              "\n",
              "                                             postfix  \n",
              "0  [ , ,  ,  ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,  , , ...  \n",
              "1  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...  \n",
              "2  [ ,  ,  ,  ,  ,  , ,  , ,  ,  ,  , ,  ,  ,  , ...  \n",
              "3  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-debb25a1-9cea-4f1c-ac51-af5cc7ffbc92\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>id</th>\n",
              "      <th>postfix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[During, 2020, ,, the, Company, also, issued, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>3</td>\n",
              "      <td>[ , ,  ,  ,  ,  ,  , ,  ,  ,  ,  ,  ,  ,  , , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, significant, increases, in, earnings, as...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...</td>\n",
              "      <td>4</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ,  ,  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, provision, for, credit, losses, was, $, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>7</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  , ,  , ,  ,  ,  , ,  ,  ,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Higher, net, charge-offs, also, contributed, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>8</td>\n",
              "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-debb25a1-9cea-4f1c-ac51-af5cc7ffbc92')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-debb25a1-9cea-4f1c-ac51-af5cc7ffbc92 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-debb25a1-9cea-4f1c-ac51-af5cc7ffbc92');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-418c086b-3ab5-45a9-a879-16eb23c14d52\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-418c086b-3ab5-45a9-a879-16eb23c14d52')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-418c086b-3ab5-45a9-a879-16eb23c14d52 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_test[[\"words\", \"ner\"]].rename(columns={\"words\": \"A\", \"ner\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "s1, s2 = [json.dumps(tokens_test) for tokens_test in dataset_test.A.values], [json.dumps(labels_test) for labels_test in dataset_test.B.values]\n",
        "\n",
        "X_raw_test, y_raw_test = [json.loads(token_test) for token_test in s1], [json.loads(label_test) for label_test in s2]"
      ],
      "metadata": {
        "id": "KGHEjg-9m4E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(label for labels in y_raw for label in labels)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "mogOidhMm_zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_raw,y_raw):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "89xIHyRymGzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(train,  '/content/train.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhcf0dATnNUe",
        "outputId": "59e9fa5c-6f22-46ed-fc1c-3ad520c45afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_raw_test,y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "EYlx3rEJnwUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/dev.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IK54lQNnwed",
        "outputId": "c54acf2e-a489-4255-b217-23296502c5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nzazMFRoAuF",
        "outputId": "438b148d-a9b7-47b9-b0be-10e77cf16fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_1bjGzNoAXN",
        "outputId": "b4d8a3cc-b1ee-4d44-bf2f-20fe50c8c8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'words': ['During',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'also',\n",
              "   'issued',\n",
              "   '$',\n",
              "   '6.5',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'senior',\n",
              "   'and',\n",
              "   'subordinated',\n",
              "   'long-term',\n",
              "   'debt',\n",
              "   '.',\n",
              "   'Total',\n",
              "   'shareholders',\n",
              "   \"'\",\n",
              "   'equity',\n",
              "   'was',\n",
              "   '$',\n",
              "   '70.9',\n",
              "   'billion',\n",
              "   'at',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'up',\n",
              "   '$',\n",
              "   '4.4',\n",
              "   'billion',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'is',\n",
              "   'due',\n",
              "   'to',\n",
              "   'net',\n",
              "   'income',\n",
              "   'in',\n",
              "   'excess',\n",
              "   'of',\n",
              "   'dividends',\n",
              "   'paid',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.8',\n",
              "   'billion',\n",
              "   'and',\n",
              "   'OCI',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.6',\n",
              "   'billion',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'significant',\n",
              "   'increases',\n",
              "   'in',\n",
              "   'earnings',\n",
              "   'assets',\n",
              "   'and',\n",
              "   'liabilities',\n",
              "   'are',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'impacts',\n",
              "   'from',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'and',\n",
              "   'the',\n",
              "   'resulting',\n",
              "   'government',\n",
              "   'stimulus',\n",
              "   'programs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'was',\n",
              "   '$',\n",
              "   '2.3',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '615',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'period',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'the',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'significant',\n",
              "   'builds',\n",
              "   'to',\n",
              "   'the',\n",
              "   'allowance',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'in',\n",
              "   'the',\n",
              "   'first',\n",
              "   'and',\n",
              "   'second',\n",
              "   'quarters',\n",
              "   'of',\n",
              "   'the',\n",
              "   'year',\n",
              "   'due',\n",
              "   'to',\n",
              "   'increased',\n",
              "   'economic',\n",
              "   'stress',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'pandemic',\n",
              "   'and',\n",
              "   'specific',\n",
              "   'consideration',\n",
              "   'of',\n",
              "   'its',\n",
              "   'impact',\n",
              "   'on',\n",
              "   'certain',\n",
              "   'industries',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'uncertainty',\n",
              "   'related',\n",
              "   'to',\n",
              "   'performance',\n",
              "   'after',\n",
              "   'the',\n",
              "   'expiration',\n",
              "   'of',\n",
              "   'relief',\n",
              "   'packages',\n",
              "   'and',\n",
              "   'COVID-19',\n",
              "   ',',\n",
              "   'increased',\n",
              "   'loan',\n",
              "   'balances',\n",
              "   'arising',\n",
              "   'from',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'the',\n",
              "   'effect',\n",
              "   'of',\n",
              "   'applying',\n",
              "   'the',\n",
              "   'CECL',\n",
              "   'methodology',\n",
              "   'in',\n",
              "   'the',\n",
              "   'current',\n",
              "   'period',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'incurred',\n",
              "   'loss',\n",
              "   'methodology',\n",
              "   'in',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'period',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Higher',\n",
              "   'net',\n",
              "   'charge-offs',\n",
              "   'also',\n",
              "   'contributed',\n",
              "   'to',\n",
              "   'the',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'the',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'and',\n",
              "   'primarily',\n",
              "   'reflect',\n",
              "   'increases',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impacts',\n",
              "   'from',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'insurance',\n",
              "   'income',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '121',\n",
              "   'million',\n",
              "   'due',\n",
              "   'to',\n",
              "   'strong',\n",
              "   'production',\n",
              "   'and',\n",
              "   'acquisitions',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impacts',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'operating',\n",
              "   'costs',\n",
              "   'were',\n",
              "   'elevated',\n",
              "   'due',\n",
              "   'to',\n",
              "   'COVID-19',\n",
              "   ',',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '250',\n",
              "   'million',\n",
              "   'of',\n",
              "   'expenses',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'period',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Segment',\n",
              "   'net',\n",
              "   'interest',\n",
              "   'income',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '4.2',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '378',\n",
              "   'million',\n",
              "   'due',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'pre-tax',\n",
              "   'income',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['Consumer',\n",
              "   'Banking',\n",
              "   'and',\n",
              "   'Wealth',\n",
              "   'average',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'leases',\n",
              "   'held',\n",
              "   'for',\n",
              "   'investment',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '67.9',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '94.9',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Consumer',\n",
              "   'Banking',\n",
              "   'and',\n",
              "   'Wealth',\n",
              "   'average',\n",
              "   'total',\n",
              "   'deposits',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '119.5',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '120.4',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'COVID-19',\n",
              "   'stimulus',\n",
              "   'impacts',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.2',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'increased',\n",
              "   'economic',\n",
              "   'stress',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '103',\n",
              "   'million',\n",
              "   'due',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'pre-tax',\n",
              "   'income',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['Average',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'leases',\n",
              "   'HFI',\n",
              "   'for',\n",
              "   'the',\n",
              "   'Corporate',\n",
              "   'and',\n",
              "   'Institutional',\n",
              "   'Group',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '47.4',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '157.0',\n",
              "   '%',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'commercial',\n",
              "   'and',\n",
              "   'industrial',\n",
              "   'loans',\n",
              "   ',',\n",
              "   'while',\n",
              "   'average',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'leases',\n",
              "   'HFI',\n",
              "   'for',\n",
              "   'Commercial',\n",
              "   'Community',\n",
              "   'Banking',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '34.2',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '62.3',\n",
              "   '%',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'and',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'commercial',\n",
              "   'and',\n",
              "   'industrial',\n",
              "   'loans',\n",
              "   ',',\n",
              "   'and',\n",
              "   'PPP',\n",
              "   'impact',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '27',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'unfunded',\n",
              "   'commitments',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'benefit',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '306',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'higher',\n",
              "   'pre-tax',\n",
              "   'loss',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   ',',\n",
              "   'average',\n",
              "   'commercial',\n",
              "   'loans',\n",
              "   'were',\n",
              "   'impacted',\n",
              "   'by',\n",
              "   'the',\n",
              "   'transfer',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.0',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'certain',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'leases',\n",
              "   'to',\n",
              "   'held',\n",
              "   'for',\n",
              "   'sale',\n",
              "   ',',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'the',\n",
              "   'average',\n",
              "   'balance',\n",
              "   'of',\n",
              "   '$',\n",
              "   '323',\n",
              "   'million',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'third',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O']},\n",
              " {'words': ['Net',\n",
              "   'charge-offs',\n",
              "   'during',\n",
              "   '2020',\n",
              "   'totaled',\n",
              "   '$',\n",
              "   '1.1',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'up',\n",
              "   '$',\n",
              "   '485',\n",
              "   'million',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'net',\n",
              "   'charge-offs',\n",
              "   'primarily',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Average',\n",
              "   'time',\n",
              "   'deposits',\n",
              "   'decreased',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'maturity',\n",
              "   'of',\n",
              "   'wholesale',\n",
              "   'negotiable',\n",
              "   'certificates',\n",
              "   'of',\n",
              "   'deposit',\n",
              "   'and',\n",
              "   'higher-cost',\n",
              "   'personal',\n",
              "   'and',\n",
              "   'business',\n",
              "   'accounts',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Liquidity',\n",
              "   'in',\n",
              "   'the',\n",
              "   'banking',\n",
              "   'industry',\n",
              "   'has',\n",
              "   'been',\n",
              "   'very',\n",
              "   'strong',\n",
              "   'during',\n",
              "   'the',\n",
              "   'current',\n",
              "   'economic',\n",
              "   'cycle',\n",
              "   '.',\n",
              "   'Much',\n",
              "   'of',\n",
              "   'this',\n",
              "   'liquidity',\n",
              "   'increase',\n",
              "   'has',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'noninterest-bearing',\n",
              "   'demand',\n",
              "   'deposits',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Service',\n",
              "   'charges',\n",
              "   'on',\n",
              "   'deposits',\n",
              "   'continued',\n",
              "   'to',\n",
              "   'rebound',\n",
              "   ',',\n",
              "   'but',\n",
              "   'remained',\n",
              "   'below',\n",
              "   '2019',\n",
              "   'combined',\n",
              "   'levels',\n",
              "   'due',\n",
              "   'to',\n",
              "   'reduced',\n",
              "   'overdraft',\n",
              "   'incident',\n",
              "   'rates',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Excluding',\n",
              "   'the',\n",
              "   'merger-related',\n",
              "   'items',\n",
              "   'mentioned',\n",
              "   'above',\n",
              "   'and',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '101',\n",
              "   'million',\n",
              "   'of',\n",
              "   'amortization',\n",
              "   'expense',\n",
              "   'for',\n",
              "   'intangibles',\n",
              "   ',',\n",
              "   'adjusted',\n",
              "   'noninterest',\n",
              "   'expense',\n",
              "   'was',\n",
              "   'up',\n",
              "   '$',\n",
              "   '994',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'reflecting',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.',\n",
              "   'The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes',\n",
              "   'was',\n",
              "   '$',\n",
              "   '311',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '153',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.',\n",
              "   'This',\n",
              "   'produced',\n",
              "   'an',\n",
              "   'effective',\n",
              "   'tax',\n",
              "   'rate',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   'of',\n",
              "   '19.0',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '17.4',\n",
              "   '%',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   'higher',\n",
              "   'effective',\n",
              "   'tax',\n",
              "   'rate',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'pre-tax',\n",
              "   'income',\n",
              "   '.'],\n",
              "  'ner': ['E', 'E', 'E', 'E', 'E', 'O', 'O', 'O', 'O', 'C', 'C', 'C', 'C']},\n",
              " {'words': ['PAMA',\n",
              "   ',',\n",
              "   'which',\n",
              "   'went',\n",
              "   'into',\n",
              "   'effect',\n",
              "   'on',\n",
              "   'January',\n",
              "   '1',\n",
              "   ',',\n",
              "   '2018',\n",
              "   ',',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'a',\n",
              "   'net',\n",
              "   'reduction',\n",
              "   'of',\n",
              "   'revenue',\n",
              "   'of',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '72.0',\n",
              "   'and',\n",
              "   '$',\n",
              "   '107.0',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'and',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   'from',\n",
              "   'all',\n",
              "   'payers',\n",
              "   'affected',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Clinical',\n",
              "   'Lab',\n",
              "   'Fee',\n",
              "   'Schedule',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Dx',\n",
              "   'revenues',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'were',\n",
              "   '$',\n",
              "   '9,253.4',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '32.2',\n",
              "   '%',\n",
              "   'over',\n",
              "   'revenues',\n",
              "   'of',\n",
              "   '$',\n",
              "   '7,000.1',\n",
              "   'in',\n",
              "   'the',\n",
              "   'corresponding',\n",
              "   'period',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'revenues',\n",
              "   'was',\n",
              "   'due',\n",
              "   'to',\n",
              "   'organic',\n",
              "   'growth',\n",
              "   'of',\n",
              "   '30.9',\n",
              "   '%',\n",
              "   'and',\n",
              "   'acquisitions',\n",
              "   'of',\n",
              "   '1.3',\n",
              "   '%',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Cost',\n",
              "   'of',\n",
              "   'revenues',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'revenues',\n",
              "   'decreased',\n",
              "   'to',\n",
              "   '64.6',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '71.9',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'This',\n",
              "   'decrease',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'COVID-19',\n",
              "   'Testing',\n",
              "   'on',\n",
              "   'revenues',\n",
              "   'and',\n",
              "   'LaunchPad',\n",
              "   'savings',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'PAMA',\n",
              "   'and',\n",
              "   'higher',\n",
              "   'personnel',\n",
              "   'costs',\n",
              "   '(',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'merit',\n",
              "   'increases',\n",
              "   'and',\n",
              "   'one',\n",
              "   'additional',\n",
              "   'payroll',\n",
              "   'day',\n",
              "   'that',\n",
              "   'predominantly',\n",
              "   'impacted',\n",
              "   'Dx',\n",
              "   ')',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Cost',\n",
              "   'of',\n",
              "   'revenues',\n",
              "   'has',\n",
              "   'increased',\n",
              "   'over',\n",
              "   'the',\n",
              "   'two-year',\n",
              "   'period',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'acquisitions',\n",
              "   ',',\n",
              "   'overall',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'volume',\n",
              "   ',',\n",
              "   'including',\n",
              "   'COVID-19',\n",
              "   'Testing',\n",
              "   ',',\n",
              "   'and',\n",
              "   'increases',\n",
              "   'in',\n",
              "   'merit-based',\n",
              "   'labor',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'with',\n",
              "   'the',\n",
              "   'corresponding',\n",
              "   'period',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impairment',\n",
              "   'of',\n",
              "   'an',\n",
              "   'equity',\n",
              "   'method',\n",
              "   'investment',\n",
              "   'and',\n",
              "   'the',\n",
              "   'decreased',\n",
              "   'profitability',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'joint',\n",
              "   'ventures',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Corporate',\n",
              "   'expenses',\n",
              "   'were',\n",
              "   '$',\n",
              "   '226.8',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '35.6',\n",
              "   '%',\n",
              "   'over',\n",
              "   'corporate',\n",
              "   'expenses',\n",
              "   'of',\n",
              "   '$',\n",
              "   '167.3',\n",
              "   'in',\n",
              "   'the',\n",
              "   'corresponding',\n",
              "   'period',\n",
              "   'of',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'corporate',\n",
              "   'expenses',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'personnel',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'executive',\n",
              "   'transition',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'COVID-19',\n",
              "   'related',\n",
              "   'expenses',\n",
              "   'and',\n",
              "   'funding',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Labcorp',\n",
              "   'Charitable',\n",
              "   'Foundation',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   '$',\n",
              "   '639.9',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'net',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'by',\n",
              "   'investing',\n",
              "   'activities',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'year',\n",
              "   'over',\n",
              "   'year',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '608.4',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'paid',\n",
              "   'for',\n",
              "   'acquisitions',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Working',\n",
              "   'capital',\n",
              "   'increased',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'accounts',\n",
              "   'receivable',\n",
              "   'and',\n",
              "   'supplies',\n",
              "   'inventory',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'revenue',\n",
              "   'growth',\n",
              "   ',',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'increases',\n",
              "   'in',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'payable',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'a',\n",
              "   'significant',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'taxable',\n",
              "   'income',\n",
              "   'during',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'and',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'accrued',\n",
              "   'payroll',\n",
              "   'tax',\n",
              "   'balances',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'deferral',\n",
              "   'of',\n",
              "   '2020',\n",
              "   'U.S.',\n",
              "   'payroll',\n",
              "   'taxes',\n",
              "   'as',\n",
              "   'part',\n",
              "   'of',\n",
              "   'the',\n",
              "   'CARES',\n",
              "   'Act',\n",
              "   'stimulus',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Capital',\n",
              "   'expenditures',\n",
              "   'were',\n",
              "   '$',\n",
              "   '381.7',\n",
              "   'and',\n",
              "   '$',\n",
              "   '400.2',\n",
              "   'for',\n",
              "   'the',\n",
              "   'years',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'and',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'Capital',\n",
              "   'expenditures',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'were',\n",
              "   '2.7',\n",
              "   '%',\n",
              "   'of',\n",
              "   'revenues',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'projects',\n",
              "   'to',\n",
              "   'support',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'core',\n",
              "   'businesses',\n",
              "   ',',\n",
              "   'projects',\n",
              "   'related',\n",
              "   'to',\n",
              "   'LaunchPad',\n",
              "   ',',\n",
              "   'and',\n",
              "   'further',\n",
              "   'DD',\n",
              "   'acquisition',\n",
              "   'integration',\n",
              "   'initiatives',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Net',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '$',\n",
              "   '517.4',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'of',\n",
              "   '$',\n",
              "   '252.7',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'This',\n",
              "   'movement',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'within',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'for',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   '$',\n",
              "   '412.2',\n",
              "   'net',\n",
              "   'financing',\n",
              "   'payments',\n",
              "   'and',\n",
              "   '$',\n",
              "   '100.0',\n",
              "   'in',\n",
              "   'share',\n",
              "   'repurchases',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '198.5',\n",
              "   'in',\n",
              "   'net',\n",
              "   'financing',\n",
              "   'receipts',\n",
              "   'more',\n",
              "   'than',\n",
              "   'offset',\n",
              "   'by',\n",
              "   '$',\n",
              "   '450.0',\n",
              "   'in',\n",
              "   'share',\n",
              "   'repurchases',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['A',\n",
              "   'one',\n",
              "   'percentage',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'or',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'the',\n",
              "   'expected',\n",
              "   'return',\n",
              "   'on',\n",
              "   'plan',\n",
              "   'assets',\n",
              "   'would',\n",
              "   'have',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'a',\n",
              "   'corresponding',\n",
              "   'change',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'pension',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '4.8',\n",
              "   'for',\n",
              "   'the',\n",
              "   'Non-U.S.',\n",
              "   'Plans',\n",
              "   '.',\n",
              "   'Net',\n",
              "   'pension',\n",
              "   'cost',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '$',\n",
              "   '11.0',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'with',\n",
              "   '$',\n",
              "   '13.8',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'pension',\n",
              "   'expense',\n",
              "   'was',\n",
              "   'due',\n",
              "   'to',\n",
              "   'market',\n",
              "   'performance',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'lower',\n",
              "   'discount',\n",
              "   'rates',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Industrial',\n",
              "   'Parts',\n",
              "   'GroupNet',\n",
              "   'sales',\n",
              "   'for',\n",
              "   'the',\n",
              "   'Industrial',\n",
              "   'Parts',\n",
              "   'Group',\n",
              "   '(',\n",
              "   '“',\n",
              "   'Industrial',\n",
              "   '”',\n",
              "   ')',\n",
              "   'were',\n",
              "   '$',\n",
              "   '5.7',\n",
              "   'billion',\n",
              "   'in',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'down',\n",
              "   '13.0',\n",
              "   '%',\n",
              "   'from',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'reflects',\n",
              "   'an',\n",
              "   '8.4',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'comparable',\n",
              "   'sales',\n",
              "   'and',\n",
              "   'an',\n",
              "   'approximate',\n",
              "   '8.5',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'EIS',\n",
              "   ',',\n",
              "   'a',\n",
              "   'non-core',\n",
              "   'component',\n",
              "   'of',\n",
              "   'the',\n",
              "   'industrial',\n",
              "   'business',\n",
              "   'due',\n",
              "   'to',\n",
              "   'its',\n",
              "   'slower-growth',\n",
              "   'and',\n",
              "   'lower-margin',\n",
              "   'profile',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Net',\n",
              "   'sales',\n",
              "   'for',\n",
              "   'the',\n",
              "   'Industrial',\n",
              "   'Parts',\n",
              "   'Group',\n",
              "   'were',\n",
              "   '$',\n",
              "   '6.5',\n",
              "   'billion',\n",
              "   'in',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'up',\n",
              "   '3.6',\n",
              "   '%',\n",
              "   'from',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'reflects',\n",
              "   'an',\n",
              "   'approximate',\n",
              "   '5.2',\n",
              "   '%',\n",
              "   'contribution',\n",
              "   'from',\n",
              "   'acquisitions',\n",
              "   'and',\n",
              "   'a',\n",
              "   '1.7',\n",
              "   '%',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'comparable',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'an',\n",
              "   'approximate',\n",
              "   '3.1',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'EIS',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['During',\n",
              "   'the',\n",
              "   'quarter',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'a',\n",
              "   '$',\n",
              "   '40',\n",
              "   'million',\n",
              "   'increase',\n",
              "   'to',\n",
              "   'cost',\n",
              "   'of',\n",
              "   'goods',\n",
              "   'sold',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'correction',\n",
              "   'of',\n",
              "   'an',\n",
              "   'immaterial',\n",
              "   'error',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'accounting',\n",
              "   'in',\n",
              "   'prior',\n",
              "   'years',\n",
              "   'for',\n",
              "   'consideration',\n",
              "   'received',\n",
              "   'from',\n",
              "   'vendors',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percent',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'reflects',\n",
              "   'loss',\n",
              "   'of',\n",
              "   'leverage',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'EIS',\n",
              "   ',',\n",
              "   'which',\n",
              "   'had',\n",
              "   'a',\n",
              "   'lower',\n",
              "   'level',\n",
              "   'of',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'relative',\n",
              "   'to',\n",
              "   'total',\n",
              "   'sales',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'accordance',\n",
              "   'with',\n",
              "   'our',\n",
              "   '$',\n",
              "   '100',\n",
              "   'million',\n",
              "   'cost',\n",
              "   'savings',\n",
              "   'plan',\n",
              "   'announced',\n",
              "   'in',\n",
              "   'late',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'we',\n",
              "   'achieved',\n",
              "   'permanent',\n",
              "   'expense',\n",
              "   'reductions',\n",
              "   'of',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '150',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'transformative',\n",
              "   'reductions',\n",
              "   'in',\n",
              "   'payroll',\n",
              "   'and',\n",
              "   'facility',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'of',\n",
              "   '$',\n",
              "   '4.6',\n",
              "   'billion',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'increased',\n",
              "   'by',\n",
              "   '$',\n",
              "   '0.3',\n",
              "   'billion',\n",
              "   'or',\n",
              "   'approximately',\n",
              "   '7.9',\n",
              "   '%',\n",
              "   'from',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'This',\n",
              "   'represents',\n",
              "   '26.1',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '25.2',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'reflects',\n",
              "   'a',\n",
              "   'combination',\n",
              "   'of',\n",
              "   'factors',\n",
              "   ',',\n",
              "   'including',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'increased',\n",
              "   'sales',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'relative',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'cost',\n",
              "   'increases',\n",
              "   'described',\n",
              "   'above',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'loss',\n",
              "   'of',\n",
              "   'leverage',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   '2.1',\n",
              "   '%',\n",
              "   'comparable',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'for',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'doubtful',\n",
              "   'accounts',\n",
              "   'was',\n",
              "   '$',\n",
              "   '23.6',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'a',\n",
              "   '$',\n",
              "   '9.7',\n",
              "   'million',\n",
              "   'increase',\n",
              "   'from',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'due',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'collectability',\n",
              "   'risks',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Non-operating',\n",
              "   'expenses',\n",
              "   'included',\n",
              "   'interest',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '93.7',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2020',\n",
              "   ',',\n",
              "   '$',\n",
              "   '95.6',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '$',\n",
              "   '101.8',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decreases',\n",
              "   'in',\n",
              "   'interest',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.9',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'and',\n",
              "   '$',\n",
              "   '6.2',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'reflect',\n",
              "   'the',\n",
              "   'combination',\n",
              "   'of',\n",
              "   'the',\n",
              "   'repayment',\n",
              "   'of',\n",
              "   'debt',\n",
              "   'and',\n",
              "   'lower',\n",
              "   'interest',\n",
              "   'rates',\n",
              "   'on',\n",
              "   'certain',\n",
              "   'variable',\n",
              "   'interest',\n",
              "   'debt',\n",
              "   'instruments',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['To',\n",
              "   'further',\n",
              "   'improve',\n",
              "   'Automotive',\n",
              "   \"'s\",\n",
              "   'segment',\n",
              "   'margin',\n",
              "   ',',\n",
              "   'this',\n",
              "   'group',\n",
              "   'will',\n",
              "   'continue',\n",
              "   'to',\n",
              "   'execute',\n",
              "   'on',\n",
              "   'its',\n",
              "   'growth',\n",
              "   'plans',\n",
              "   'and',\n",
              "   'cost',\n",
              "   'initiatives',\n",
              "   'in',\n",
              "   '2021',\n",
              "   'and',\n",
              "   'the',\n",
              "   'years',\n",
              "   'ahead',\n",
              "   '.',\n",
              "   'Automotive',\n",
              "   \"'s\",\n",
              "   'segment',\n",
              "   'profit',\n",
              "   'decreased',\n",
              "   '2.8',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'from',\n",
              "   '2018',\n",
              "   'and',\n",
              "   'segment',\n",
              "   'margin',\n",
              "   'was',\n",
              "   '7.6',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '8.1',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'segment',\n",
              "   'margin',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'loss',\n",
              "   'of',\n",
              "   'expense',\n",
              "   'leverage',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   '2.3',\n",
              "   '%',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'comparable',\n",
              "   'sales',\n",
              "   'for',\n",
              "   'Automotive',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'effective',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'rate',\n",
              "   'of',\n",
              "   '24.8',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'increased',\n",
              "   'from',\n",
              "   '24.6',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'rate',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'geographic',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'rate',\n",
              "   'mix',\n",
              "   'shifts',\n",
              "   'and',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'one-time',\n",
              "   'transaction',\n",
              "   'and',\n",
              "   'other',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'changes',\n",
              "   'in',\n",
              "   'the',\n",
              "   'realizability',\n",
              "   'of',\n",
              "   'future',\n",
              "   'tax',\n",
              "   'benefit',\n",
              "   'adjustments',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   'the',\n",
              "   'comparable',\n",
              "   'periods',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Net',\n",
              "   'income',\n",
              "   'from',\n",
              "   'continuing',\n",
              "   'operations',\n",
              "   'was',\n",
              "   '3.7',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '4.5',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'During',\n",
              "   'the',\n",
              "   'years',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '2018',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'incurred',\n",
              "   '$',\n",
              "   '634.5',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '170.1',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '34.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'of',\n",
              "   'adjustments',\n",
              "   '.',\n",
              "   'In',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'these',\n",
              "   'adjustments',\n",
              "   'include',\n",
              "   'a',\n",
              "   'goodwill',\n",
              "   'impairment',\n",
              "   'charge',\n",
              "   'related',\n",
              "   'to',\n",
              "   'our',\n",
              "   'European',\n",
              "   'reporting',\n",
              "   'unit',\n",
              "   ',',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'an',\n",
              "   'inventory',\n",
              "   'adjustment',\n",
              "   ',',\n",
              "   'realized',\n",
              "   'currency',\n",
              "   'losses',\n",
              "   ',',\n",
              "   'and',\n",
              "   'transaction',\n",
              "   'and',\n",
              "   'other',\n",
              "   'costs',\n",
              "   'and',\n",
              "   'income',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Financing',\n",
              "   'Activities',\n",
              "   'Net',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'totaled',\n",
              "   '$',\n",
              "   '1.5',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.1',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '292.2',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   'the',\n",
              "   '$',\n",
              "   '386.0',\n",
              "   'million',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'extinguishment',\n",
              "   'of',\n",
              "   'the',\n",
              "   'term',\n",
              "   'loan',\n",
              "   'A',\n",
              "   'facility',\n",
              "   'in',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Net',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'totaled',\n",
              "   '$',\n",
              "   '386.0',\n",
              "   'million',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '222.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '36.6',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   'the',\n",
              "   '$',\n",
              "   '608.8',\n",
              "   'million',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'Primarily',\n",
              "   ',',\n",
              "   'the',\n",
              "   'decrease',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'net',\n",
              "   'proceeds',\n",
              "   'from',\n",
              "   'debt',\n",
              "   'issued',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'net',\n",
              "   'payments',\n",
              "   'on',\n",
              "   'debt',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Net',\n",
              "   'periodic',\n",
              "   'benefit',\n",
              "   'income',\n",
              "   'for',\n",
              "   'our',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'pension',\n",
              "   'plans',\n",
              "   'was',\n",
              "   '$',\n",
              "   '18.0',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '16.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '15.8',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'years',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '2018',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'income',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'pension',\n",
              "   'plans',\n",
              "   'in',\n",
              "   '2020',\n",
              "   ',',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '2018',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'the',\n",
              "   'freeze',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['New',\n",
              "   'products',\n",
              "   ',',\n",
              "   'geographic',\n",
              "   'expansion',\n",
              "   'and',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'emerging',\n",
              "   'markets',\n",
              "   'were',\n",
              "   'the',\n",
              "   'primary',\n",
              "   'drivers',\n",
              "   'of',\n",
              "   'the',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   ',',\n",
              "   'with',\n",
              "   'double-',\n",
              "   'and',\n",
              "   'high-single-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'Latin',\n",
              "   'America',\n",
              "   'and',\n",
              "   'Asia',\n",
              "   'Pacific',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['•Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '9.8',\n",
              "   '%',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Baby',\n",
              "   '&',\n",
              "   'Parenting',\n",
              "   'segment',\n",
              "   ',',\n",
              "   'with',\n",
              "   'improved',\n",
              "   'retail-level',\n",
              "   'sales',\n",
              "   'in',\n",
              "   'North',\n",
              "   'America',\n",
              "   'and',\n",
              "   'sustained',\n",
              "   'momentum',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Asia',\n",
              "   'Pacific',\n",
              "   'region',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'new',\n",
              "   'product',\n",
              "   'launches',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'Home',\n",
              "   'Solutions',\n",
              "   'segment',\n",
              "   'realized',\n",
              "   'a',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'decline',\n",
              "   'of',\n",
              "   '3.6',\n",
              "   '%',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'continued',\n",
              "   'operational',\n",
              "   'challenges',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Décor',\n",
              "   'business',\n",
              "   '(',\n",
              "   'Levolor',\n",
              "   'window',\n",
              "   'treatments',\n",
              "   ')',\n",
              "   'within',\n",
              "   'the',\n",
              "   'Home',\n",
              "   'Solutions',\n",
              "   'segment',\n",
              "   'and',\n",
              "   'challenges',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Culinary',\n",
              "   'and',\n",
              "   'Décor',\n",
              "   'businesses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'a',\n",
              "   'change',\n",
              "   'in',\n",
              "   'merchandising',\n",
              "   'strategy',\n",
              "   'at',\n",
              "   'a',\n",
              "   'significant',\n",
              "   'retail',\n",
              "   'customer',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Restructuring',\n",
              "   'and',\n",
              "   'restructuring-related',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '69',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '10',\n",
              "   'million',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'have',\n",
              "   'been',\n",
              "   'incurred',\n",
              "   'through',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'majority',\n",
              "   'of',\n",
              "   'which',\n",
              "   'were',\n",
              "   'employee-related',\n",
              "   'cash',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'severance',\n",
              "   ',',\n",
              "   'retirement',\n",
              "   ',',\n",
              "   'and',\n",
              "   'other',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'primary',\n",
              "   'drivers',\n",
              "   'of',\n",
              "   'the',\n",
              "   '20',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   'increase',\n",
              "   'were',\n",
              "   'pricing',\n",
              "   'and',\n",
              "   'productivity',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   '.',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'were',\n",
              "   '25.8',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'or',\n",
              "   '$',\n",
              "   '1,521.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   'versus',\n",
              "   '25.8',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'or',\n",
              "   '$',\n",
              "   '1,515.3',\n",
              "   'million',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '32.6',\n",
              "   'million',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   '$',\n",
              "   '37.1',\n",
              "   'million',\n",
              "   'of',\n",
              "   'incremental',\n",
              "   'investments',\n",
              "   'in',\n",
              "   'brand',\n",
              "   'building',\n",
              "   'and',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'activities',\n",
              "   'to',\n",
              "   'support',\n",
              "   'new',\n",
              "   'products',\n",
              "   ',',\n",
              "   'marketing',\n",
              "   'initiatives',\n",
              "   ',',\n",
              "   'new',\n",
              "   'market',\n",
              "   'entries',\n",
              "   'and',\n",
              "   'global',\n",
              "   'expansion',\n",
              "   ',',\n",
              "   'and',\n",
              "   'a',\n",
              "   '$',\n",
              "   '7.3',\n",
              "   'million',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'structural',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'due',\n",
              "   'to',\n",
              "   'increased',\n",
              "   'annual',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'savings',\n",
              "   'from',\n",
              "   'structural',\n",
              "   'cost',\n",
              "   'savings',\n",
              "   'initiatives',\n",
              "   'and',\n",
              "   'ongoing',\n",
              "   'restructuring',\n",
              "   'projects',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'incurred',\n",
              "   '$',\n",
              "   '7.6',\n",
              "   'million',\n",
              "   'of',\n",
              "   'restructuring-related',\n",
              "   'costs',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '382.6',\n",
              "   'million',\n",
              "   'during',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'principally',\n",
              "   'relating',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impairment',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'Baby',\n",
              "   '&',\n",
              "   'Parenting',\n",
              "   'and',\n",
              "   'Hardware',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   'and',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Transformation',\n",
              "   'Plan',\n",
              "   'and',\n",
              "   'consisted',\n",
              "   'of',\n",
              "   '$',\n",
              "   '8.4',\n",
              "   'million',\n",
              "   'of',\n",
              "   'facility',\n",
              "   'and',\n",
              "   'other',\n",
              "   'exit',\n",
              "   'and',\n",
              "   'impairment',\n",
              "   'costs',\n",
              "   ',',\n",
              "   '$',\n",
              "   '33.2',\n",
              "   'million',\n",
              "   'of',\n",
              "   'employee',\n",
              "   'severance',\n",
              "   ',',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'employee',\n",
              "   'relocation',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '8.5',\n",
              "   'million',\n",
              "   'of',\n",
              "   'exited',\n",
              "   'contractual',\n",
              "   'commitments',\n",
              "   'and',\n",
              "   'other',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Interest',\n",
              "   'expense',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '76.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '10.1',\n",
              "   'million',\n",
              "   'from',\n",
              "   '$',\n",
              "   '86.2',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'average',\n",
              "   'debt',\n",
              "   'levels',\n",
              "   'in',\n",
              "   '2012',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'expense',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'pretax',\n",
              "   'income',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'and',\n",
              "   'a',\n",
              "   'change',\n",
              "   'in',\n",
              "   'the',\n",
              "   'geographical',\n",
              "   'mix',\n",
              "   'in',\n",
              "   'earnings',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   '$',\n",
              "   '23.1',\n",
              "   'million',\n",
              "   'of',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'expense',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'charges',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'incremental',\n",
              "   'tax',\n",
              "   'contingencies',\n",
              "   'and',\n",
              "   'the',\n",
              "   'expiration',\n",
              "   'of',\n",
              "   'various',\n",
              "   'statutes',\n",
              "   'of',\n",
              "   'limitation',\n",
              "   'and',\n",
              "   'audit',\n",
              "   'settlements',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['During',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'all',\n",
              "   'conditions',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'escrow',\n",
              "   'were',\n",
              "   'satisfied',\n",
              "   'and',\n",
              "   'resolved',\n",
              "   ',',\n",
              "   'and',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'had',\n",
              "   'received',\n",
              "   '$',\n",
              "   '7.8',\n",
              "   'million',\n",
              "   'from',\n",
              "   'the',\n",
              "   'escrow',\n",
              "   'and',\n",
              "   'recognized',\n",
              "   'the',\n",
              "   'proceeds',\n",
              "   'as',\n",
              "   'a',\n",
              "   'gain',\n",
              "   'from',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'the',\n",
              "   'hand',\n",
              "   'torch',\n",
              "   'and',\n",
              "   'solder',\n",
              "   'business',\n",
              "   'in',\n",
              "   'discontinued',\n",
              "   'operations',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '36.6',\n",
              "   'million',\n",
              "   'due',\n",
              "   'to',\n",
              "   '$',\n",
              "   '39.8',\n",
              "   'million',\n",
              "   'of',\n",
              "   'incremental',\n",
              "   'investments',\n",
              "   'in',\n",
              "   'brand',\n",
              "   'building',\n",
              "   'and',\n",
              "   'other',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'activities',\n",
              "   'to',\n",
              "   'support',\n",
              "   'marketing',\n",
              "   'initiatives',\n",
              "   ',',\n",
              "   'advertising',\n",
              "   'and',\n",
              "   'promotions',\n",
              "   ',',\n",
              "   'new',\n",
              "   'market',\n",
              "   'entries',\n",
              "   'and',\n",
              "   'global',\n",
              "   'expansion',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'include',\n",
              "   '$',\n",
              "   '6.3',\n",
              "   'million',\n",
              "   'of',\n",
              "   'incremental',\n",
              "   'costs',\n",
              "   'incurred',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'Chief',\n",
              "   'Executive',\n",
              "   'Officer',\n",
              "   'transition',\n",
              "   'and',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '22.2',\n",
              "   'million',\n",
              "   'in',\n",
              "   'restructuring-related',\n",
              "   'costs',\n",
              "   'for',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Transformation',\n",
              "   'Plan',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'annual',\n",
              "   'impairment',\n",
              "   'testing',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'and',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '382.6',\n",
              "   'million',\n",
              "   'during',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'principally',\n",
              "   'relating',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impairment',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'Baby',\n",
              "   '&',\n",
              "   'Parenting',\n",
              "   'and',\n",
              "   'Hardware',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'of',\n",
              "   '$',\n",
              "   '50.1',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '77.4',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'and',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'year-over-year',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'was',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'the',\n",
              "   'completion',\n",
              "   'of',\n",
              "   'Project',\n",
              "   'Acceleration',\n",
              "   'in',\n",
              "   '2010',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'primarily',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'Project',\n",
              "   'Acceleration',\n",
              "   'and',\n",
              "   'included',\n",
              "   '$',\n",
              "   '6.0',\n",
              "   'million',\n",
              "   'of',\n",
              "   'facility',\n",
              "   'and',\n",
              "   'other',\n",
              "   'exit',\n",
              "   'and',\n",
              "   'impairment',\n",
              "   'costs',\n",
              "   '$',\n",
              "   '53.5',\n",
              "   'million',\n",
              "   'of',\n",
              "   'employee',\n",
              "   'severance',\n",
              "   ',',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'employee',\n",
              "   'relocation',\n",
              "   'costs',\n",
              "   'and',\n",
              "   '$',\n",
              "   '17.9',\n",
              "   'million',\n",
              "   'of',\n",
              "   'exited',\n",
              "   'contractual',\n",
              "   'commitments',\n",
              "   'and',\n",
              "   'other',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Interest',\n",
              "   'expense',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'was',\n",
              "   '$',\n",
              "   '86.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '32.2',\n",
              "   'million',\n",
              "   'from',\n",
              "   '$',\n",
              "   '118.4',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'overall',\n",
              "   'borrowing',\n",
              "   'costs',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'the',\n",
              "   'Capital',\n",
              "   'Structure',\n",
              "   'Optimization',\n",
              "   'Plan',\n",
              "   ',',\n",
              "   'a',\n",
              "   'more',\n",
              "   'favorable',\n",
              "   'interest',\n",
              "   'rate',\n",
              "   'environment',\n",
              "   'and',\n",
              "   'a',\n",
              "   'higher',\n",
              "   'mix',\n",
              "   'of',\n",
              "   'short-term',\n",
              "   'borrowings',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'losses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'extinguishments',\n",
              "   'of',\n",
              "   'debt',\n",
              "   'of',\n",
              "   '$',\n",
              "   '218.6',\n",
              "   'million',\n",
              "   'recognized',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'the',\n",
              "   'retirement',\n",
              "   'of',\n",
              "   '$',\n",
              "   '279.3',\n",
              "   'million',\n",
              "   'of',\n",
              "   'the',\n",
              "   '$',\n",
              "   '300.0',\n",
              "   'million',\n",
              "   'aggregate',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   '10.60',\n",
              "   '%',\n",
              "   'senior',\n",
              "   'unsecured',\n",
              "   'notes',\n",
              "   'due',\n",
              "   'April',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '$',\n",
              "   '324.7',\n",
              "   'million',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   'the',\n",
              "   '$',\n",
              "   '345.0',\n",
              "   'million',\n",
              "   '5.50',\n",
              "   '%',\n",
              "   'convertible',\n",
              "   'senior',\n",
              "   'notes',\n",
              "   'due',\n",
              "   '2014',\n",
              "   'pursuant',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Capital',\n",
              "   'Structure',\n",
              "   'Optimization',\n",
              "   'Plan',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['(',\n",
              "   '2',\n",
              "   ')',\n",
              "   'Includes',\n",
              "   'restructuring-related',\n",
              "   'costs',\n",
              "   'of',\n",
              "   '$',\n",
              "   '24.3',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '37.4',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'and',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Transformation',\n",
              "   'Plan',\n",
              "   'and',\n",
              "   '$',\n",
              "   '4.1',\n",
              "   'million',\n",
              "   'of',\n",
              "   'restructuring-related',\n",
              "   'costs',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   'for',\n",
              "   '2012',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   '2011',\n",
              "   'operating',\n",
              "   'income',\n",
              "   'also',\n",
              "   'includes',\n",
              "   '$',\n",
              "   '6.3',\n",
              "   'million',\n",
              "   'of',\n",
              "   'incremental',\n",
              "   'costs',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'Chief',\n",
              "   'Executive',\n",
              "   'Officer',\n",
              "   'transition',\n",
              "   'in',\n",
              "   '2011',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'declined',\n",
              "   '3.6',\n",
              "   '%',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'continuing',\n",
              "   'challenges',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Décor',\n",
              "   'business',\n",
              "   'and',\n",
              "   'also',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'change',\n",
              "   'in',\n",
              "   'merchandising',\n",
              "   'strategy',\n",
              "   'by',\n",
              "   'a',\n",
              "   'significant',\n",
              "   'retail',\n",
              "   'customer',\n",
              "   'in',\n",
              "   'North',\n",
              "   'America',\n",
              "   ',',\n",
              "   'which',\n",
              "   'impacted',\n",
              "   'the',\n",
              "   'Décor',\n",
              "   'and',\n",
              "   'Culinary',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Foreign',\n",
              "   'currency',\n",
              "   'had',\n",
              "   'an',\n",
              "   'unfavorable',\n",
              "   'impact',\n",
              "   'of',\n",
              "   '0.3',\n",
              "   '%',\n",
              "   '.Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '217.5',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '13.2',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '11.4',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '5.0',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '228.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '13.4',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   '20',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'a',\n",
              "   'reduction',\n",
              "   'in',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   ',',\n",
              "   'as',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   'and',\n",
              "   'unfavorable',\n",
              "   'mix',\n",
              "   'were',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'pricing',\n",
              "   'and',\n",
              "   'productivity',\n",
              "   'gains',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'remained',\n",
              "   'relatively',\n",
              "   'unchanged',\n",
              "   'as',\n",
              "   'reductions',\n",
              "   'in',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'savings',\n",
              "   'realized',\n",
              "   'from',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   'were',\n",
              "   'consistent',\n",
              "   'with',\n",
              "   'the',\n",
              "   'declines',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '7.0',\n",
              "   '%',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'introduction',\n",
              "   'of',\n",
              "   'new',\n",
              "   'products',\n",
              "   'in',\n",
              "   'North',\n",
              "   'America',\n",
              "   'and',\n",
              "   'continued',\n",
              "   'investment',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'forces',\n",
              "   'in',\n",
              "   'international',\n",
              "   'markets',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '9.8',\n",
              "   '%',\n",
              "   ',',\n",
              "   'which',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'improvements',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'at',\n",
              "   'the',\n",
              "   'retail',\n",
              "   'level',\n",
              "   'in',\n",
              "   'North',\n",
              "   'America',\n",
              "   'and',\n",
              "   'sustained',\n",
              "   'growth',\n",
              "   'momentum',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Asia',\n",
              "   'Pacific',\n",
              "   'markets',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'new',\n",
              "   'products',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '72.7',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '9.9',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '21.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '40.9',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '51.6',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '7.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   '230',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'productivity',\n",
              "   ',',\n",
              "   'favorable',\n",
              "   'mix',\n",
              "   ',',\n",
              "   'and',\n",
              "   'better',\n",
              "   'leverage',\n",
              "   'of',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'on',\n",
              "   'the',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'increase',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation.SpecialtyNet',\n",
              "   'sales',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'were',\n",
              "   '$',\n",
              "   '540.6',\n",
              "   'million',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '13.0',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '2.3',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '553.6',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '0.4',\n",
              "   '%',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'declines',\n",
              "   'in',\n",
              "   'the',\n",
              "   'window',\n",
              "   'hardware',\n",
              "   'and',\n",
              "   'interactive',\n",
              "   'teaching',\n",
              "   'technologies',\n",
              "   'product',\n",
              "   'lines',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'increases',\n",
              "   'in',\n",
              "   'labeling',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '110',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '60',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'structural',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'lower',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'spending',\n",
              "   'to',\n",
              "   'support',\n",
              "   'geographic',\n",
              "   'expansion',\n",
              "   ',',\n",
              "   'new',\n",
              "   'market',\n",
              "   'entries',\n",
              "   'and',\n",
              "   'distribution',\n",
              "   'gains',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '10.3',\n",
              "   '%',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'double-',\n",
              "   'and',\n",
              "   'high-single-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'across',\n",
              "   'the',\n",
              "   'segment',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Excluding',\n",
              "   'the',\n",
              "   'impacts',\n",
              "   'of',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'sales',\n",
              "   'at',\n",
              "   'the',\n",
              "   'segment',\n",
              "   '’',\n",
              "   's',\n",
              "   'North',\n",
              "   'American',\n",
              "   'businesses',\n",
              "   'increased',\n",
              "   '9.2',\n",
              "   '%',\n",
              "   'while',\n",
              "   'sales',\n",
              "   'declined',\n",
              "   '1.2',\n",
              "   '%',\n",
              "   'at',\n",
              "   'international',\n",
              "   'businesses',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'weakness',\n",
              "   'in',\n",
              "   'the',\n",
              "   'European',\n",
              "   'markets',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Higher',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'in',\n",
              "   'the',\n",
              "   'technology',\n",
              "   'businesses',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'investments',\n",
              "   'in',\n",
              "   'strategic',\n",
              "   'initiatives',\n",
              "   'also',\n",
              "   'contributed',\n",
              "   'to',\n",
              "   'the',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   '$',\n",
              "   '57.2',\n",
              "   'million',\n",
              "   'year-over-year',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'cash',\n",
              "   'flow',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'following',\n",
              "   ':',\n",
              "   '•improved',\n",
              "   'profitability',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011•a',\n",
              "   '$',\n",
              "   '61.7',\n",
              "   'million',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   'payments',\n",
              "   'made',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011',\n",
              "   'and•a',\n",
              "   '$',\n",
              "   '9.2',\n",
              "   'million',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'customer',\n",
              "   'program',\n",
              "   'payments',\n",
              "   'during',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011Partially',\n",
              "   'offset',\n",
              "   'by•a',\n",
              "   '$',\n",
              "   '41.1',\n",
              "   'million',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'contributions',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'plans',\n",
              "   ',',\n",
              "   'including',\n",
              "   'its',\n",
              "   'primary',\n",
              "   'U.S.',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'pension',\n",
              "   'plan',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   '$',\n",
              "   '21.3',\n",
              "   'million',\n",
              "   'year-over-year',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'cash',\n",
              "   'flow',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'following',\n",
              "   'items',\n",
              "   ':',\n",
              "   '•higher',\n",
              "   'customer',\n",
              "   'program',\n",
              "   'payments',\n",
              "   'in',\n",
              "   '2011',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'including',\n",
              "   'higher',\n",
              "   'amounts',\n",
              "   'paid',\n",
              "   'in',\n",
              "   '2011',\n",
              "   'for',\n",
              "   'amounts',\n",
              "   'earned',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'customer',\n",
              "   'program',\n",
              "   'payments',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'for',\n",
              "   'amounts',\n",
              "   'earned',\n",
              "   'in',\n",
              "   '2009',\n",
              "   ',',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'an',\n",
              "   'incremental',\n",
              "   '$',\n",
              "   '114.0',\n",
              "   'million',\n",
              "   'use',\n",
              "   'of',\n",
              "   'cash',\n",
              "   'in',\n",
              "   '201133Table',\n",
              "   'of',\n",
              "   'ContentsPartially',\n",
              "   'offset',\n",
              "   'by•a',\n",
              "   '$',\n",
              "   '30.0',\n",
              "   'million',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'contributions',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'primary',\n",
              "   'U.S.',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'pension',\n",
              "   'plan',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '50.0',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'to',\n",
              "   '$',\n",
              "   '20.0',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2011',\n",
              "   'and•a',\n",
              "   '$',\n",
              "   '43.4',\n",
              "   'million',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'paid',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes.During',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'received',\n",
              "   'net',\n",
              "   'proceeds',\n",
              "   'of',\n",
              "   '$',\n",
              "   '106.0',\n",
              "   'million',\n",
              "   'from',\n",
              "   'its',\n",
              "   'short-term',\n",
              "   'borrowing',\n",
              "   'arrangements',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '34.4',\n",
              "   'million',\n",
              "   'of',\n",
              "   'net',\n",
              "   'payments',\n",
              "   'related',\n",
              "   'to',\n",
              "   'these',\n",
              "   'borrowing',\n",
              "   'arrangements',\n",
              "   'in',\n",
              "   '2011',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'short-term',\n",
              "   'borrowings',\n",
              "   ',',\n",
              "   'which',\n",
              "   'include',\n",
              "   'commercial',\n",
              "   'paper',\n",
              "   'and',\n",
              "   'the',\n",
              "   'receivables',\n",
              "   'financing',\n",
              "   'facility',\n",
              "   ',',\n",
              "   'were',\n",
              "   '$',\n",
              "   '210.7',\n",
              "   'million',\n",
              "   'at',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '103.6',\n",
              "   'million',\n",
              "   'at',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'short-term',\n",
              "   'borrowings',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'redemption',\n",
              "   'of',\n",
              "   'the',\n",
              "   '$',\n",
              "   '436.7',\n",
              "   'million',\n",
              "   'of',\n",
              "   'the',\n",
              "   '5.25',\n",
              "   '%',\n",
              "   'Junior',\n",
              "   'Convertible',\n",
              "   'Subordinated',\n",
              "   'Debentures',\n",
              "   '(',\n",
              "   'the',\n",
              "   'Debentures',\n",
              "   ')',\n",
              "   'in',\n",
              "   'July',\n",
              "   '2012',\n",
              "   'and',\n",
              "   'the',\n",
              "   'repayments',\n",
              "   'of',\n",
              "   'an',\n",
              "   'aggregate',\n",
              "   '$',\n",
              "   '750.0',\n",
              "   'million',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   'medium-term',\n",
              "   'notes',\n",
              "   'during',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'proceeds',\n",
              "   'from',\n",
              "   'long-term',\n",
              "   'debt',\n",
              "   'issuances',\n",
              "   'in',\n",
              "   'the',\n",
              "   'second',\n",
              "   'and',\n",
              "   'fourth',\n",
              "   'quarters',\n",
              "   'of',\n",
              "   '2012',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'December',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'repaid',\n",
              "   '$',\n",
              "   '500.0',\n",
              "   'million',\n",
              "   'outstanding',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   '5.50',\n",
              "   '%',\n",
              "   'notes',\n",
              "   'due',\n",
              "   'in',\n",
              "   'April',\n",
              "   '2013',\n",
              "   '(',\n",
              "   'the',\n",
              "   '“',\n",
              "   '2013',\n",
              "   'Notes',\n",
              "   '”',\n",
              "   ')',\n",
              "   'and',\n",
              "   'paid',\n",
              "   'a',\n",
              "   'premium',\n",
              "   'of',\n",
              "   '$',\n",
              "   '7.1',\n",
              "   'million',\n",
              "   'due',\n",
              "   'to',\n",
              "   'early',\n",
              "   'repayment',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['During',\n",
              "   '2012',\n",
              "   'and',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'paid',\n",
              "   '$',\n",
              "   '26.5',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '20.0',\n",
              "   'million',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'acquisitions',\n",
              "   'and',\n",
              "   'acquisition-related',\n",
              "   'activity',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'had',\n",
              "   'liabilities',\n",
              "   'of',\n",
              "   '$',\n",
              "   '727.7',\n",
              "   'million',\n",
              "   'related',\n",
              "   'to',\n",
              "   'its',\n",
              "   'unfunded',\n",
              "   'and',\n",
              "   'underfunded',\n",
              "   'pension',\n",
              "   'and',\n",
              "   'other',\n",
              "   'postretirement',\n",
              "   'benefit',\n",
              "   'plans',\n",
              "   'for',\n",
              "   'which',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'expects',\n",
              "   'to',\n",
              "   'make',\n",
              "   'contributions',\n",
              "   'of',\n",
              "   '$',\n",
              "   '144',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2013',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'had',\n",
              "   '$',\n",
              "   '45.7',\n",
              "   'million',\n",
              "   'in',\n",
              "   'standby',\n",
              "   'letters',\n",
              "   'of',\n",
              "   'credit',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'self-insurance',\n",
              "   'programs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'workers',\n",
              "   '’',\n",
              "   'compensation',\n",
              "   ',',\n",
              "   'product',\n",
              "   'liability',\n",
              "   'and',\n",
              "   'medical',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'grew',\n",
              "   '7.0',\n",
              "   '%',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Tools',\n",
              "   'segment',\n",
              "   'with',\n",
              "   'approximately',\n",
              "   'half',\n",
              "   'of',\n",
              "   'the',\n",
              "   'growth',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'the',\n",
              "   'segment',\n",
              "   '’',\n",
              "   's',\n",
              "   'international',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['•Input',\n",
              "   'and',\n",
              "   'sourced',\n",
              "   'product',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   'was',\n",
              "   'more',\n",
              "   'than',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'pricing',\n",
              "   'and',\n",
              "   'productivity',\n",
              "   ',',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'a',\n",
              "   '20',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'improvement',\n",
              "   'in',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   'increased',\n",
              "   'despite',\n",
              "   'continued',\n",
              "   'operational',\n",
              "   'challenges',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Décor',\n",
              "   'business',\n",
              "   'within',\n",
              "   'the',\n",
              "   'Home',\n",
              "   'Solutions',\n",
              "   'segment',\n",
              "   'and',\n",
              "   'pressures',\n",
              "   'due',\n",
              "   'to',\n",
              "   'uncertain',\n",
              "   'macroeconomic',\n",
              "   'conditions',\n",
              "   'in',\n",
              "   'Western',\n",
              "   'Europe.•Continued',\n",
              "   'focused',\n",
              "   'spend',\n",
              "   'for',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'activities',\n",
              "   'to',\n",
              "   'drive',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'enhance',\n",
              "   'the',\n",
              "   'new',\n",
              "   'product',\n",
              "   'pipeline',\n",
              "   ',',\n",
              "   'develop',\n",
              "   'growth',\n",
              "   'platforms',\n",
              "   'and',\n",
              "   'expand',\n",
              "   'geographically',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '2.2',\n",
              "   '%',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'double-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Latin',\n",
              "   'America',\n",
              "   'region',\n",
              "   'and',\n",
              "   'high-single-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Asia',\n",
              "   'Pacific',\n",
              "   'region',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   'and',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Transformation',\n",
              "   'Plan',\n",
              "   'and',\n",
              "   'consisted',\n",
              "   'of',\n",
              "   '$',\n",
              "   '8.4',\n",
              "   'million',\n",
              "   'of',\n",
              "   'facility',\n",
              "   'and',\n",
              "   'other',\n",
              "   'exit',\n",
              "   'and',\n",
              "   'impairment',\n",
              "   'costs',\n",
              "   ',',\n",
              "   '$',\n",
              "   '33.2',\n",
              "   'million',\n",
              "   'of',\n",
              "   'employee',\n",
              "   'severance',\n",
              "   ',',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'employee',\n",
              "   'relocation',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '8.5',\n",
              "   'million',\n",
              "   'of',\n",
              "   'exited',\n",
              "   'contractual',\n",
              "   'commitments',\n",
              "   'and',\n",
              "   'other',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Interest',\n",
              "   'expense',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '76.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '10.1',\n",
              "   'million',\n",
              "   'from',\n",
              "   '$',\n",
              "   '86.2',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'average',\n",
              "   'debt',\n",
              "   'levels',\n",
              "   'in',\n",
              "   '2012',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['During',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'has',\n",
              "   'recognized',\n",
              "   'foreign',\n",
              "   'exchange',\n",
              "   'transactional',\n",
              "   'gains',\n",
              "   'of',\n",
              "   '$',\n",
              "   '2.3',\n",
              "   'million',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'foreign',\n",
              "   'exchange',\n",
              "   'losses',\n",
              "   'of',\n",
              "   '$',\n",
              "   '14.7',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'as',\n",
              "   'currencies',\n",
              "   'generally',\n",
              "   'appreciated',\n",
              "   'against',\n",
              "   'the',\n",
              "   'U.S.',\n",
              "   'Dollar',\n",
              "   'during',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'depreciating',\n",
              "   'in',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '36.6',\n",
              "   'million',\n",
              "   'due',\n",
              "   'to',\n",
              "   '$',\n",
              "   '39.8',\n",
              "   'million',\n",
              "   'of',\n",
              "   'incremental',\n",
              "   'investments',\n",
              "   'in',\n",
              "   'brand',\n",
              "   'building',\n",
              "   'and',\n",
              "   'other',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'activities',\n",
              "   'to',\n",
              "   'support',\n",
              "   'marketing',\n",
              "   'initiatives',\n",
              "   ',',\n",
              "   'advertising',\n",
              "   'and',\n",
              "   'promotions',\n",
              "   ',',\n",
              "   'new',\n",
              "   'market',\n",
              "   'entries',\n",
              "   'and',\n",
              "   'global',\n",
              "   'expansion',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'expenses',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'include',\n",
              "   '$',\n",
              "   '6.3',\n",
              "   'million',\n",
              "   'of',\n",
              "   'incremental',\n",
              "   'costs',\n",
              "   'incurred',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'Chief',\n",
              "   'Executive',\n",
              "   'Officer',\n",
              "   'transition',\n",
              "   'and',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '22.2',\n",
              "   'million',\n",
              "   'in',\n",
              "   'restructuring-related',\n",
              "   'costs',\n",
              "   'for',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Transformation',\n",
              "   'Plan',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'aforementioned',\n",
              "   'increases',\n",
              "   'were',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   '$',\n",
              "   '31.7',\n",
              "   'million',\n",
              "   'lower',\n",
              "   'structural',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'primarily',\n",
              "   'from',\n",
              "   'lower',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   'costs',\n",
              "   'in',\n",
              "   '2011',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2010',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['As',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'annual',\n",
              "   'impairment',\n",
              "   'testing',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'and',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '382.6',\n",
              "   'million',\n",
              "   'during',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'principally',\n",
              "   'relating',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impairment',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'Baby',\n",
              "   '&',\n",
              "   'Parenting',\n",
              "   'and',\n",
              "   'Hardware',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['There',\n",
              "   'were',\n",
              "   'no',\n",
              "   'similar',\n",
              "   'charges',\n",
              "   'recorded',\n",
              "   'during',\n",
              "   '2010',\n",
              "   '.',\n",
              "   'The',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'of',\n",
              "   '$',\n",
              "   '50.1',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '77.4',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'and',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'year-over-year',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'was',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'the',\n",
              "   'completion',\n",
              "   'of',\n",
              "   'Project',\n",
              "   'Acceleration',\n",
              "   'in',\n",
              "   '2010',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   'and',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Transformation',\n",
              "   'Plan',\n",
              "   'and',\n",
              "   'consisted',\n",
              "   'of',\n",
              "   '$',\n",
              "   '8.4',\n",
              "   'million',\n",
              "   'of',\n",
              "   'facility',\n",
              "   'and',\n",
              "   'other',\n",
              "   'exit',\n",
              "   'and',\n",
              "   'impairment',\n",
              "   'costs',\n",
              "   '$',\n",
              "   '33.2',\n",
              "   'million',\n",
              "   'of',\n",
              "   'employee',\n",
              "   'severance',\n",
              "   ',',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'employee',\n",
              "   'relocation',\n",
              "   'costs',\n",
              "   'and',\n",
              "   '$',\n",
              "   '8.5',\n",
              "   'million',\n",
              "   'of',\n",
              "   'exited',\n",
              "   'contractual',\n",
              "   'commitments',\n",
              "   'and',\n",
              "   'other',\n",
              "   'restructuring',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Losses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'extinguishments',\n",
              "   'of',\n",
              "   'debt',\n",
              "   'were',\n",
              "   '$',\n",
              "   '4.8',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '218.6',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2010',\n",
              "   '.',\n",
              "   'The',\n",
              "   'losses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'extinguishments',\n",
              "   'of',\n",
              "   'debt',\n",
              "   'of',\n",
              "   '$',\n",
              "   '218.6',\n",
              "   'million',\n",
              "   'recognized',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'the',\n",
              "   'retirement',\n",
              "   'of',\n",
              "   '$',\n",
              "   '279.3',\n",
              "   'million',\n",
              "   'of',\n",
              "   'the',\n",
              "   '$',\n",
              "   '300.0',\n",
              "   'million',\n",
              "   'aggregate',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   '10.60',\n",
              "   '%',\n",
              "   'senior',\n",
              "   'unsecured',\n",
              "   'notes',\n",
              "   'due',\n",
              "   'April',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '$',\n",
              "   '324.7',\n",
              "   'million',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   'the',\n",
              "   '$',\n",
              "   '345.0',\n",
              "   'million',\n",
              "   '5.50',\n",
              "   '%',\n",
              "   'convertible',\n",
              "   'senior',\n",
              "   'notes',\n",
              "   'due',\n",
              "   '2014',\n",
              "   'pursuant',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Capital',\n",
              "   'Structure',\n",
              "   'Optimization',\n",
              "   'Plan',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'Company',\n",
              "   'recognized',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '17.9',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '5.6',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'and',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'the',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'expense',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'the',\n",
              "   '$',\n",
              "   '382.6',\n",
              "   'million',\n",
              "   'of',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'in',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'which',\n",
              "   'were',\n",
              "   'only',\n",
              "   'partially',\n",
              "   'deductible',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '218.6',\n",
              "   'million',\n",
              "   'of',\n",
              "   'losses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'extinguishments',\n",
              "   'of',\n",
              "   'debt',\n",
              "   'in',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'which',\n",
              "   'were',\n",
              "   'fully',\n",
              "   'deductible',\n",
              "   '.',\n",
              "   'The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'the',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'expense',\n",
              "   'was',\n",
              "   'also',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'the',\n",
              "   'recognition',\n",
              "   'of',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'benefits',\n",
              "   'of',\n",
              "   '$',\n",
              "   '49.0',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2011',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'reversal',\n",
              "   'of',\n",
              "   'accruals',\n",
              "   'for',\n",
              "   'certain',\n",
              "   'tax',\n",
              "   'contingencies',\n",
              "   ',',\n",
              "   'including',\n",
              "   'interest',\n",
              "   'and',\n",
              "   'penalties',\n",
              "   ',',\n",
              "   'upon',\n",
              "   'the',\n",
              "   'expiration',\n",
              "   'of',\n",
              "   'various',\n",
              "   'worldwide',\n",
              "   'statutes',\n",
              "   'of',\n",
              "   'limitation',\n",
              "   ',',\n",
              "   'and',\n",
              "   'the',\n",
              "   'recognition',\n",
              "   'of',\n",
              "   '$',\n",
              "   '63.6',\n",
              "   'million',\n",
              "   'of',\n",
              "   'previously',\n",
              "   'unrecognized',\n",
              "   'tax',\n",
              "   'benefits',\n",
              "   'in',\n",
              "   '2010',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'entering',\n",
              "   'into',\n",
              "   'a',\n",
              "   'binding',\n",
              "   'closing',\n",
              "   'agreement',\n",
              "   'related',\n",
              "   'to',\n",
              "   'its',\n",
              "   '2005',\n",
              "   'and',\n",
              "   '2006',\n",
              "   'U.S.',\n",
              "   'Federal',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'examination',\n",
              "   ',',\n",
              "   'including',\n",
              "   'all',\n",
              "   'issues',\n",
              "   'that',\n",
              "   'were',\n",
              "   'at',\n",
              "   'the',\n",
              "   'IRS',\n",
              "   'Appeals',\n",
              "   'Office',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'loss',\n",
              "   'on',\n",
              "   'disposal',\n",
              "   'of',\n",
              "   'discontinued',\n",
              "   'operations',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'was',\n",
              "   '$',\n",
              "   '15.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'after',\n",
              "   'tax',\n",
              "   ',',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'disposal',\n",
              "   'of',\n",
              "   'the',\n",
              "   'hand',\n",
              "   'torch',\n",
              "   'and',\n",
              "   'solder',\n",
              "   'business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'declined',\n",
              "   '3.6',\n",
              "   '%',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'continuing',\n",
              "   'challenges',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Décor',\n",
              "   'business',\n",
              "   'and',\n",
              "   'also',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'change',\n",
              "   'in',\n",
              "   'merchandising',\n",
              "   'strategy',\n",
              "   'by',\n",
              "   'a',\n",
              "   'significant',\n",
              "   'retail',\n",
              "   'customer',\n",
              "   'in',\n",
              "   'North',\n",
              "   'America',\n",
              "   ',',\n",
              "   'which',\n",
              "   'impacted',\n",
              "   'the',\n",
              "   'Décor',\n",
              "   'and',\n",
              "   'Culinary',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'remained',\n",
              "   'relatively',\n",
              "   'unchanged',\n",
              "   'as',\n",
              "   'reductions',\n",
              "   'in',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'savings',\n",
              "   'realized',\n",
              "   'from',\n",
              "   'Project',\n",
              "   'Renewal',\n",
              "   'were',\n",
              "   'consistent',\n",
              "   'with',\n",
              "   'the',\n",
              "   'declines',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '261.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '18.5',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '15.0',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '6.1',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '246.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '17.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   '90',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   'expansion',\n",
              "   ',',\n",
              "   'as',\n",
              "   'pricing',\n",
              "   'and',\n",
              "   'productivity',\n",
              "   'more',\n",
              "   'than',\n",
              "   'offset',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '7.0',\n",
              "   '%',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'introduction',\n",
              "   'of',\n",
              "   'new',\n",
              "   'products',\n",
              "   'in',\n",
              "   'North',\n",
              "   'America',\n",
              "   'and',\n",
              "   'continued',\n",
              "   'investment',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'forces',\n",
              "   'in',\n",
              "   'international',\n",
              "   'markets',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '109.8',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '13.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '9.3',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '7.8',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '119.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '15.3',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   '170',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'partially',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   'and',\n",
              "   'unfavorable',\n",
              "   'mix',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'pricing',\n",
              "   'and',\n",
              "   'productivity',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'was',\n",
              "   'also',\n",
              "   'the',\n",
              "   'result',\n",
              "   'of',\n",
              "   'a',\n",
              "   '100',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'brand',\n",
              "   'building',\n",
              "   'and',\n",
              "   'ongoing',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'spending',\n",
              "   ',',\n",
              "   'structural',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'to',\n",
              "   'support',\n",
              "   'geographic',\n",
              "   'expansion',\n",
              "   ',',\n",
              "   'and',\n",
              "   'sustained',\n",
              "   'investment',\n",
              "   'in',\n",
              "   'selling',\n",
              "   'and',\n",
              "   'marketing',\n",
              "   'resources',\n",
              "   'in',\n",
              "   'certain',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '92.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '12.2',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '15.4',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '14.2',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '108.3',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '14.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   '240',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'a',\n",
              "   '290',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'brand',\n",
              "   'building',\n",
              "   'and',\n",
              "   'ongoing',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'spending',\n",
              "   ',',\n",
              "   'structural',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'to',\n",
              "   'support',\n",
              "   'geographic',\n",
              "   'expansion',\n",
              "   'primarily',\n",
              "   'in',\n",
              "   'Latin',\n",
              "   'America',\n",
              "   ',',\n",
              "   'and',\n",
              "   'sustained',\n",
              "   'investments',\n",
              "   'in',\n",
              "   'selling',\n",
              "   'and',\n",
              "   'marketing',\n",
              "   'resources',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   'expansion',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2012',\n",
              "   'was',\n",
              "   '$',\n",
              "   '72.7',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '9.9',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '21.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '40.9',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '51.6',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '7.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2011',\n",
              "   '.',\n",
              "   'The',\n",
              "   '230',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'productivity',\n",
              "   ',',\n",
              "   'favorable',\n",
              "   'mix',\n",
              "   ',',\n",
              "   'and',\n",
              "   'better',\n",
              "   'leverage',\n",
              "   'of',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'on',\n",
              "   'the',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'increase',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '0.4',\n",
              "   '%',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'declines',\n",
              "   'in',\n",
              "   'the',\n",
              "   'window',\n",
              "   'hardware',\n",
              "   'and',\n",
              "   'interactive',\n",
              "   'teaching',\n",
              "   'technologies',\n",
              "   'product',\n",
              "   'lines',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'increases',\n",
              "   'in',\n",
              "   'labeling',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '1.2',\n",
              "   '%',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'high-single-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'at',\n",
              "   'Calphalon',\n",
              "   'due',\n",
              "   'to',\n",
              "   'new',\n",
              "   'product',\n",
              "   'launches',\n",
              "   'and',\n",
              "   'distribution',\n",
              "   'gains',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '110',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '0.8',\n",
              "   '%',\n",
              "   'with',\n",
              "   'low-single-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'fine',\n",
              "   'writing',\n",
              "   'products',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'a',\n",
              "   'modest',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'everyday',\n",
              "   'writing',\n",
              "   'products',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'for',\n",
              "   'everyday',\n",
              "   'writing',\n",
              "   'products',\n",
              "   'was',\n",
              "   'impacted',\n",
              "   'by',\n",
              "   'an',\n",
              "   'estimated',\n",
              "   '$',\n",
              "   '5',\n",
              "   'to',\n",
              "   '$',\n",
              "   '10',\n",
              "   'million',\n",
              "   'of',\n",
              "   'sales',\n",
              "   'shifted',\n",
              "   'from',\n",
              "   '2011',\n",
              "   'to',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2010',\n",
              "   'due',\n",
              "   'to',\n",
              "   'customer',\n",
              "   'order',\n",
              "   'acceleration',\n",
              "   'to',\n",
              "   'qualify',\n",
              "   'for',\n",
              "   'annual',\n",
              "   'volume',\n",
              "   'rebates',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'constant',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   'as',\n",
              "   'a',\n",
              "   'percentage',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '60',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'structural',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'lower',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'strategic',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'spending',\n",
              "   'to',\n",
              "   'support',\n",
              "   'geographic',\n",
              "   'expansion',\n",
              "   ',',\n",
              "   'new',\n",
              "   'market',\n",
              "   'entries',\n",
              "   'and',\n",
              "   'distribution',\n",
              "   'gains',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Excluding',\n",
              "   'the',\n",
              "   'impacts',\n",
              "   'of',\n",
              "   'currency',\n",
              "   ',',\n",
              "   'sales',\n",
              "   'at',\n",
              "   'the',\n",
              "   'segment',\n",
              "   '’',\n",
              "   's',\n",
              "   'North',\n",
              "   'American',\n",
              "   'businesses',\n",
              "   'increased',\n",
              "   '9.2',\n",
              "   '%',\n",
              "   'while',\n",
              "   'sales',\n",
              "   'declined',\n",
              "   '1.2',\n",
              "   '%',\n",
              "   'at',\n",
              "   'international',\n",
              "   'businesses',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'weakness',\n",
              "   'in',\n",
              "   'the',\n",
              "   'European',\n",
              "   'markets',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'for',\n",
              "   '2011',\n",
              "   'was',\n",
              "   '$',\n",
              "   '108.3',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '14.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'of',\n",
              "   '$',\n",
              "   '25.9',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '19.3',\n",
              "   '%',\n",
              "   ',',\n",
              "   'from',\n",
              "   '$',\n",
              "   '134.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '19.6',\n",
              "   '%',\n",
              "   'of',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'for',\n",
              "   '2010',\n",
              "   '.',\n",
              "   'The',\n",
              "   '500',\n",
              "   'basis',\n",
              "   'point',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'input',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   'and',\n",
              "   'unfavorable',\n",
              "   'product',\n",
              "   'mix',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'pricing',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'declined',\n",
              "   '5.5',\n",
              "   '%',\n",
              "   'due',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'continued',\n",
              "   'economic',\n",
              "   'pressure',\n",
              "   'and',\n",
              "   'declining',\n",
              "   'birth',\n",
              "   'rates',\n",
              "   'in',\n",
              "   'the',\n",
              "   'North',\n",
              "   'American',\n",
              "   'and',\n",
              "   'European',\n",
              "   'markets',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Core',\n",
              "   'sales',\n",
              "   'decreased',\n",
              "   '2.1',\n",
              "   '%',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'double-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'declines',\n",
              "   'at',\n",
              "   'the',\n",
              "   'hardware',\n",
              "   'businesses',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'mid-single-digit',\n",
              "   'core',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'labeling',\n",
              "   'and',\n",
              "   'technology',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   '$',\n",
              "   '57.2',\n",
              "   'million',\n",
              "   'year-over-year',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'cash',\n",
              "   'flow',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'following',\n",
              "   ':',\n",
              "   '•improved',\n",
              "   'profitability',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011•a',\n",
              "   '$',\n",
              "   '61.7',\n",
              "   'million',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'incentive',\n",
              "   'compensation',\n",
              "   'payments',\n",
              "   'made',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011',\n",
              "   'and•a',\n",
              "   '$',\n",
              "   '9.2',\n",
              "   'million',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'customer',\n",
              "   'program',\n",
              "   'payments',\n",
              "   'during',\n",
              "   '2012',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2011Partially',\n",
              "   'offset',\n",
              "   'by•a',\n",
              "   '$',\n",
              "   '41.1',\n",
              "   'million',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'contributions',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'plans',\n",
              "   ',',\n",
              "   'including',\n",
              "   'its',\n",
              "   'primary',\n",
              "   'U.S.',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'pension',\n",
              "   'plan',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['During',\n",
              "   '2012',\n",
              "   'and',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'paid',\n",
              "   '$',\n",
              "   '26.5',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '20.0',\n",
              "   'million',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'acquisitions',\n",
              "   'and',\n",
              "   'acquisition-related',\n",
              "   'activity',\n",
              "   '.',\n",
              "   'Cash',\n",
              "   'used',\n",
              "   'for',\n",
              "   'restructuring',\n",
              "   'activities',\n",
              "   'was',\n",
              "   '$',\n",
              "   '48.6',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '39.5',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '72.8',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2012',\n",
              "   ',',\n",
              "   '2011',\n",
              "   'and',\n",
              "   '2010',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'and',\n",
              "   'is',\n",
              "   'included',\n",
              "   'in',\n",
              "   'the',\n",
              "   'cash',\n",
              "   'provided',\n",
              "   'by',\n",
              "   'operating',\n",
              "   'activities',\n",
              "   '.',\n",
              "   'These',\n",
              "   'payments',\n",
              "   'relate',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'employee',\n",
              "   'severance',\n",
              "   ',',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'relocation',\n",
              "   'costs.In',\n",
              "   '2012',\n",
              "   'and',\n",
              "   '2011',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'made',\n",
              "   'contributions',\n",
              "   'of',\n",
              "   '$',\n",
              "   '100.8',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '59.7',\n",
              "   'million',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'to',\n",
              "   'its',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'plans',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['During',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'projected',\n",
              "   'benefit',\n",
              "   'obligations',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'defined',\n",
              "   'benefit',\n",
              "   'plans',\n",
              "   'increased',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '170',\n",
              "   'million',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'interest',\n",
              "   'rates',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['From',\n",
              "   'the',\n",
              "   'commencement',\n",
              "   'of',\n",
              "   'the',\n",
              "   'SRP',\n",
              "   'in',\n",
              "   'August',\n",
              "   '2011',\n",
              "   'through',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'has',\n",
              "   'repurchased',\n",
              "   'and',\n",
              "   'retired',\n",
              "   'a',\n",
              "   'total',\n",
              "   'of',\n",
              "   '8.3',\n",
              "   'million',\n",
              "   'shares',\n",
              "   'for',\n",
              "   '$',\n",
              "   '137.6',\n",
              "   'million',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'had',\n",
              "   'liabilities',\n",
              "   'of',\n",
              "   '$',\n",
              "   '727.7',\n",
              "   'million',\n",
              "   'related',\n",
              "   'to',\n",
              "   'its',\n",
              "   'unfunded',\n",
              "   'and',\n",
              "   'underfunded',\n",
              "   'pension',\n",
              "   'and',\n",
              "   'other',\n",
              "   'postretirement',\n",
              "   'benefit',\n",
              "   'plans',\n",
              "   'for',\n",
              "   'which',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'expects',\n",
              "   'to',\n",
              "   'make',\n",
              "   'contributions',\n",
              "   'of',\n",
              "   '$',\n",
              "   '144',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2013',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'had',\n",
              "   '$',\n",
              "   '45.7',\n",
              "   'million',\n",
              "   'in',\n",
              "   'standby',\n",
              "   'letters',\n",
              "   'of',\n",
              "   'credit',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'self-insurance',\n",
              "   'programs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'workers',\n",
              "   '’',\n",
              "   'compensation',\n",
              "   ',',\n",
              "   'product',\n",
              "   'liability',\n",
              "   'and',\n",
              "   'medical',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'Company',\n",
              "   'capitalized',\n",
              "   '$',\n",
              "   '43.9',\n",
              "   'million',\n",
              "   'of',\n",
              "   'software',\n",
              "   'costs',\n",
              "   'during',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'which',\n",
              "   'primarily',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'employee',\n",
              "   ',',\n",
              "   'consultant',\n",
              "   'and',\n",
              "   'related',\n",
              "   'personnel',\n",
              "   'costs',\n",
              "   'incurred',\n",
              "   'in',\n",
              "   'the',\n",
              "   'rollout',\n",
              "   'of',\n",
              "   'SAP',\n",
              "   'in',\n",
              "   'the',\n",
              "   'European',\n",
              "   'region',\n",
              "   'and',\n",
              "   'in',\n",
              "   'Brazil',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'transition',\n",
              "   'to',\n",
              "   'the',\n",
              "   'SITME',\n",
              "   'rate',\n",
              "   'did',\n",
              "   'result',\n",
              "   'in',\n",
              "   'a',\n",
              "   'one-time',\n",
              "   'foreign',\n",
              "   'exchange',\n",
              "   'gain',\n",
              "   'of',\n",
              "   '$',\n",
              "   '5.6',\n",
              "   'million',\n",
              "   ',',\n",
              "   'which',\n",
              "   'is',\n",
              "   'recognized',\n",
              "   'in',\n",
              "   'other',\n",
              "   'income',\n",
              "   'in',\n",
              "   '2010',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Despite',\n",
              "   'the',\n",
              "   'challenges',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'COVID-19',\n",
              "   ',',\n",
              "   'our',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'were',\n",
              "   '$',\n",
              "   '4.4',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'representing',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '38.3',\n",
              "   'million',\n",
              "   'over',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'of',\n",
              "   'our',\n",
              "   'TAVR',\n",
              "   'products',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'our',\n",
              "   'diluted',\n",
              "   'earnings',\n",
              "   'per',\n",
              "   'share',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'was',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'an',\n",
              "   'after-tax',\n",
              "   'charge',\n",
              "   'of',\n",
              "   '$',\n",
              "   '305.1',\n",
              "   'million',\n",
              "   'to',\n",
              "   'settle',\n",
              "   'certain',\n",
              "   'patent',\n",
              "   'litigation',\n",
              "   'related',\n",
              "   'to',\n",
              "   'transcatheter',\n",
              "   'mitral',\n",
              "   'and',\n",
              "   'tricuspid',\n",
              "   'repair',\n",
              "   'products',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'of',\n",
              "   'Critical',\n",
              "   'Care',\n",
              "   'products',\n",
              "   'was',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'of',\n",
              "   'our',\n",
              "   'enhanced',\n",
              "   'surgical',\n",
              "   'recovery',\n",
              "   'products',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'in',\n",
              "   'the',\n",
              "   'United',\n",
              "   'States',\n",
              "   ',',\n",
              "   'as',\n",
              "   'many',\n",
              "   'surgical',\n",
              "   'procedures',\n",
              "   'were',\n",
              "   'delayed',\n",
              "   'due',\n",
              "   'to',\n",
              "   'COVID-19',\n",
              "   'beginning',\n",
              "   'in',\n",
              "   'March',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'our',\n",
              "   'gross',\n",
              "   'profit',\n",
              "   'was',\n",
              "   'reduced',\n",
              "   'by',\n",
              "   '$',\n",
              "   '73.1',\n",
              "   'million',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'decision',\n",
              "   'to',\n",
              "   'discontinue',\n",
              "   'our',\n",
              "   'CENTERA',\n",
              "   'program',\n",
              "   ',',\n",
              "   'resulting',\n",
              "   'in',\n",
              "   'a',\n",
              "   '1.7',\n",
              "   'percentage',\n",
              "   'point',\n",
              "   'increase',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['These',\n",
              "   'increases',\n",
              "   'were',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'a',\n",
              "   ')',\n",
              "   'decreased',\n",
              "   'spending',\n",
              "   'on',\n",
              "   'transcatheter',\n",
              "   'aortic',\n",
              "   'valve',\n",
              "   'clinical',\n",
              "   'trials',\n",
              "   'and',\n",
              "   'b',\n",
              "   ')',\n",
              "   'decreased',\n",
              "   'performance-based',\n",
              "   'compensation',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'fair',\n",
              "   'value',\n",
              "   'of',\n",
              "   'contingent',\n",
              "   'consideration',\n",
              "   'liabilities',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '13.6',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'and',\n",
              "   'income',\n",
              "   'of',\n",
              "   '$',\n",
              "   '6.1',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'expense',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'accretion',\n",
              "   'of',\n",
              "   'interest',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'passage',\n",
              "   'of',\n",
              "   'time',\n",
              "   'and',\n",
              "   'adjustments',\n",
              "   'to',\n",
              "   'discount',\n",
              "   'rates',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'changes',\n",
              "   'in',\n",
              "   'the',\n",
              "   'projected',\n",
              "   'probability',\n",
              "   'and',\n",
              "   'timing',\n",
              "   'of',\n",
              "   'milestone',\n",
              "   'achievements',\n",
              "   ',',\n",
              "   'and',\n",
              "   'the',\n",
              "   'projected',\n",
              "   'timing',\n",
              "   'of',\n",
              "   'cash',\n",
              "   'inflows',\n",
              "   '.',\n",
              "   'The',\n",
              "   'income',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'was',\n",
              "   'due',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'longer',\n",
              "   'product',\n",
              "   'development',\n",
              "   'timelines',\n",
              "   ',',\n",
              "   'which',\n",
              "   'reduced',\n",
              "   'the',\n",
              "   'probability',\n",
              "   'of',\n",
              "   'milestone',\n",
              "   'achievements',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'accretion',\n",
              "   'of',\n",
              "   'interest',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'passage',\n",
              "   'of',\n",
              "   'time',\n",
              "   'and',\n",
              "   'discount',\n",
              "   'rate',\n",
              "   'adjustments',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Interest',\n",
              "   'expense',\n",
              "   'was',\n",
              "   '$',\n",
              "   '15.8',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '20.7',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'and',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'interest',\n",
              "   'expense',\n",
              "   'resulted',\n",
              "   'primarily',\n",
              "   'from',\n",
              "   'higher',\n",
              "   'capitalized',\n",
              "   'interest',\n",
              "   'due',\n",
              "   'to',\n",
              "   'facilities',\n",
              "   'construction',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'net',\n",
              "   'foreign',\n",
              "   'exchange',\n",
              "   'gains',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'the',\n",
              "   'foreign',\n",
              "   'currency',\n",
              "   'fluctuations',\n",
              "   'in',\n",
              "   'our',\n",
              "   'global',\n",
              "   'trade',\n",
              "   'and',\n",
              "   'intercompany',\n",
              "   'receivable',\n",
              "   'and',\n",
              "   'payable',\n",
              "   'balances',\n",
              "   ',',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'gains',\n",
              "   'and',\n",
              "   'losses',\n",
              "   'on',\n",
              "   'derivative',\n",
              "   'instruments',\n",
              "   'intended',\n",
              "   'as',\n",
              "   'an',\n",
              "   'economic',\n",
              "   'hedge',\n",
              "   'of',\n",
              "   'those',\n",
              "   'exposures',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'we',\n",
              "   'had',\n",
              "   'a',\n",
              "   'remaining',\n",
              "   'tax',\n",
              "   'obligation',\n",
              "   'of',\n",
              "   '$',\n",
              "   '238.7',\n",
              "   'million',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'deemed',\n",
              "   'repatriation',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'we',\n",
              "   'had',\n",
              "   'recorded',\n",
              "   '$',\n",
              "   '238.7',\n",
              "   'million',\n",
              "   'of',\n",
              "   'income',\n",
              "   'tax',\n",
              "   'liabilities',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'one-time',\n",
              "   'transition',\n",
              "   'tax',\n",
              "   'that',\n",
              "   'resulted',\n",
              "   'from',\n",
              "   'the',\n",
              "   'enactment',\n",
              "   'of',\n",
              "   'the',\n",
              "   '2017',\n",
              "   'Act',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'reduced',\n",
              "   'demand',\n",
              "   'and',\n",
              "   'lower',\n",
              "   'capacity',\n",
              "   ',',\n",
              "   'we',\n",
              "   'retired',\n",
              "   '227',\n",
              "   'aircraft',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'and',\n",
              "   'have',\n",
              "   'temporarily',\n",
              "   'parked',\n",
              "   'approximately',\n",
              "   '125',\n",
              "   'aircraft',\n",
              "   'as',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['In',\n",
              "   'response',\n",
              "   'to',\n",
              "   'the',\n",
              "   'reduction',\n",
              "   'in',\n",
              "   'revenue',\n",
              "   ',',\n",
              "   'we',\n",
              "   'have',\n",
              "   'implemented',\n",
              "   ',',\n",
              "   'and',\n",
              "   'will',\n",
              "   'continue',\n",
              "   'to',\n",
              "   'implement',\n",
              "   ',',\n",
              "   'cost',\n",
              "   'saving',\n",
              "   'initiatives',\n",
              "   ',',\n",
              "   'including',\n",
              "   'the',\n",
              "   'following',\n",
              "   'in',\n",
              "   '2020',\n",
              "   ':',\n",
              "   '•Reducing',\n",
              "   'capacity',\n",
              "   'as',\n",
              "   'described',\n",
              "   'above',\n",
              "   'to',\n",
              "   'align',\n",
              "   'with',\n",
              "   'expected',\n",
              "   'demand',\n",
              "   ',',\n",
              "   'which',\n",
              "   'has',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'removing',\n",
              "   'from',\n",
              "   'active',\n",
              "   'service',\n",
              "   'approximately',\n",
              "   '350',\n",
              "   'aircraft',\n",
              "   'as',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'including',\n",
              "   'certain',\n",
              "   'fleets',\n",
              "   'or',\n",
              "   'aircraft',\n",
              "   'that',\n",
              "   'we',\n",
              "   'have',\n",
              "   'decided',\n",
              "   'to',\n",
              "   'early',\n",
              "   'retire',\n",
              "   'as',\n",
              "   'described',\n",
              "   'below',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Our',\n",
              "   'cash',\n",
              "   ',',\n",
              "   'cash',\n",
              "   'equivalents',\n",
              "   ',',\n",
              "   'short-term',\n",
              "   'investments',\n",
              "   'and',\n",
              "   'aggregate',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'committed',\n",
              "   'and',\n",
              "   'available',\n",
              "   'to',\n",
              "   'be',\n",
              "   'drawn',\n",
              "   'under',\n",
              "   'our',\n",
              "   'revolving',\n",
              "   'credit',\n",
              "   'facilities',\n",
              "   '(',\n",
              "   'liquidity',\n",
              "   ')',\n",
              "   'as',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '$',\n",
              "   '16.7',\n",
              "   'billion',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'following',\n",
              "   'actions',\n",
              "   'to',\n",
              "   'increase',\n",
              "   'liquidity',\n",
              "   'and',\n",
              "   'strengthen',\n",
              "   'our',\n",
              "   'financial',\n",
              "   'position',\n",
              "   'during',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ':',\n",
              "   '•Completing',\n",
              "   'financing',\n",
              "   'transactions',\n",
              "   'for',\n",
              "   'an',\n",
              "   'aggregate',\n",
              "   'principal',\n",
              "   'amount',\n",
              "   'of',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '25.9',\n",
              "   'billion',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Finally',\n",
              "   ',',\n",
              "   'the',\n",
              "   'CARES',\n",
              "   'Act',\n",
              "   'also',\n",
              "   'provides',\n",
              "   'for',\n",
              "   'deferred',\n",
              "   'payment',\n",
              "   'of',\n",
              "   'the',\n",
              "   'employer',\n",
              "   'portion',\n",
              "   'of',\n",
              "   'social',\n",
              "   'security',\n",
              "   'taxes',\n",
              "   'through',\n",
              "   'the',\n",
              "   'end',\n",
              "   'of',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'with',\n",
              "   '50',\n",
              "   '%',\n",
              "   'of',\n",
              "   'the',\n",
              "   'deferred',\n",
              "   'amount',\n",
              "   'due',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2021',\n",
              "   'and',\n",
              "   'the',\n",
              "   'remaining',\n",
              "   '50',\n",
              "   '%',\n",
              "   'due',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2022',\n",
              "   '.',\n",
              "   'This',\n",
              "   'provided',\n",
              "   'us',\n",
              "   'with',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '200',\n",
              "   'million',\n",
              "   'of',\n",
              "   'additional',\n",
              "   'liquidity',\n",
              "   'during',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['On',\n",
              "   'December',\n",
              "   '27',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'an',\n",
              "   'additional',\n",
              "   'COVID-19',\n",
              "   'support',\n",
              "   'bill',\n",
              "   'was',\n",
              "   'enacted',\n",
              "   'into',\n",
              "   'law',\n",
              "   ',',\n",
              "   'which',\n",
              "   'extends',\n",
              "   'the',\n",
              "   'payroll',\n",
              "   'support',\n",
              "   'program',\n",
              "   'of',\n",
              "   'the',\n",
              "   'CARES',\n",
              "   'Act',\n",
              "   'and',\n",
              "   'provides',\n",
              "   'an',\n",
              "   'additional',\n",
              "   '$',\n",
              "   '15',\n",
              "   'billion',\n",
              "   'in',\n",
              "   'grants',\n",
              "   'and',\n",
              "   'loans',\n",
              "   'to',\n",
              "   'be',\n",
              "   'used',\n",
              "   'for',\n",
              "   'airline',\n",
              "   'employee',\n",
              "   'wages',\n",
              "   ',',\n",
              "   'salaries',\n",
              "   'and',\n",
              "   'benefits',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['In',\n",
              "   'January',\n",
              "   '2021',\n",
              "   ',',\n",
              "   'we',\n",
              "   'entered',\n",
              "   'into',\n",
              "   'a',\n",
              "   'payroll',\n",
              "   'support',\n",
              "   'program',\n",
              "   'extension',\n",
              "   'agreement',\n",
              "   'with',\n",
              "   'the',\n",
              "   'U.S.',\n",
              "   'Department',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Treasury',\n",
              "   '.',\n",
              "   'We',\n",
              "   'expect',\n",
              "   'to',\n",
              "   'receive',\n",
              "   '$',\n",
              "   '2.9',\n",
              "   'billion',\n",
              "   'in',\n",
              "   'payroll',\n",
              "   'support',\n",
              "   'payments',\n",
              "   ',',\n",
              "   'which',\n",
              "   'must',\n",
              "   'be',\n",
              "   'used',\n",
              "   'exclusively',\n",
              "   'for',\n",
              "   'the',\n",
              "   'payment',\n",
              "   'of',\n",
              "   'employee',\n",
              "   'wages',\n",
              "   ',',\n",
              "   'salaries',\n",
              "   'and',\n",
              "   'benefits',\n",
              "   'and',\n",
              "   'are',\n",
              "   'conditioned',\n",
              "   'on',\n",
              "   'our',\n",
              "   'agreement',\n",
              "   'to',\n",
              "   'refrain',\n",
              "   'from',\n",
              "   'conducting',\n",
              "   'involuntary',\n",
              "   'employee',\n",
              "   'layoffs',\n",
              "   'or',\n",
              "   'furloughs',\n",
              "   'from',\n",
              "   'the',\n",
              "   'date',\n",
              "   'of',\n",
              "   'the',\n",
              "   'extension',\n",
              "   'agreement',\n",
              "   'through',\n",
              "   'March',\n",
              "   '2021',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['In',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'receipt',\n",
              "   'of',\n",
              "   'these',\n",
              "   'payments',\n",
              "   ',',\n",
              "   'we',\n",
              "   'also',\n",
              "   'expect',\n",
              "   'to',\n",
              "   'issue',\n",
              "   'to',\n",
              "   'the',\n",
              "   'U.S.',\n",
              "   'Department',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Treasury',\n",
              "   'warrants',\n",
              "   'to',\n",
              "   'acquire',\n",
              "   'shares',\n",
              "   'of',\n",
              "   'Delta',\n",
              "   'common',\n",
              "   'stock',\n",
              "   ',',\n",
              "   'which',\n",
              "   'we',\n",
              "   'expect',\n",
              "   'to',\n",
              "   'be',\n",
              "   'approximately',\n",
              "   '2.1',\n",
              "   'million',\n",
              "   'shares',\n",
              "   'representing',\n",
              "   'less',\n",
              "   'than',\n",
              "   '0.5',\n",
              "   '%',\n",
              "   'of',\n",
              "   'our',\n",
              "   'outstanding',\n",
              "   'shares',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Our',\n",
              "   'pre-tax',\n",
              "   'loss',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '$',\n",
              "   '15.6',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'representing',\n",
              "   'a',\n",
              "   '$',\n",
              "   '21.8',\n",
              "   'billion',\n",
              "   'decrease',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'on',\n",
              "   'our',\n",
              "   'business',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'a',\n",
              "   '64',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'revenue',\n",
              "   'and',\n",
              "   '$',\n",
              "   '8.2',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'restructuring',\n",
              "   'charges',\n",
              "   'and',\n",
              "   '$',\n",
              "   '2.4',\n",
              "   'billion',\n",
              "   'in',\n",
              "   'investment',\n",
              "   'impairments',\n",
              "   'and',\n",
              "   'equity',\n",
              "   'method',\n",
              "   'losses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Compared',\n",
              "   'to',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'our',\n",
              "   'operating',\n",
              "   'revenue',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '29.9',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '64',\n",
              "   '%',\n",
              "   'due',\n",
              "   'to',\n",
              "   'reduced',\n",
              "   'demand',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Our',\n",
              "   'total',\n",
              "   'operating',\n",
              "   'cost',\n",
              "   'per',\n",
              "   'available',\n",
              "   'seat',\n",
              "   'mile',\n",
              "   '(',\n",
              "   'CASM',\n",
              "   ')',\n",
              "   'increased',\n",
              "   '50',\n",
              "   '%',\n",
              "   'to',\n",
              "   '22.01',\n",
              "   'cents',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   '51',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'capacity',\n",
              "   ',',\n",
              "   'which',\n",
              "   'was',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'significant',\n",
              "   'cost',\n",
              "   'reduction',\n",
              "   'measures',\n",
              "   'discussed',\n",
              "   'above',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Our',\n",
              "   'liquidity',\n",
              "   'at',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '$',\n",
              "   '16.7',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'a',\n",
              "   '$',\n",
              "   '10.8',\n",
              "   'billion',\n",
              "   'increase',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'proceeds',\n",
              "   'from',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'debt',\n",
              "   'issuances',\n",
              "   '(',\n",
              "   'including',\n",
              "   'our',\n",
              "   'SkyMiles',\n",
              "   'financing',\n",
              "   'arrangements',\n",
              "   'and',\n",
              "   'aircraft',\n",
              "   'financings',\n",
              "   ')',\n",
              "   ',',\n",
              "   'support',\n",
              "   'payments',\n",
              "   'under',\n",
              "   'the',\n",
              "   'CARES',\n",
              "   'Act',\n",
              "   'payroll',\n",
              "   'support',\n",
              "   'program',\n",
              "   'and',\n",
              "   'other',\n",
              "   'liquidity',\n",
              "   'initiatives',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Losses',\n",
              "   'during',\n",
              "   '2020',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'activities',\n",
              "   'using',\n",
              "   '$',\n",
              "   '3.8',\n",
              "   'billion',\n",
              "   '.'],\n",
              "  'ner': ['C', 'C', 'C', 'O', 'O', 'E', 'E', 'E', 'E', 'E', 'E', 'E']},\n",
              " {'words': ['During',\n",
              "   '2020',\n",
              "   'we',\n",
              "   'incurred',\n",
              "   '$',\n",
              "   '9.2',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'investing',\n",
              "   'cash',\n",
              "   'outflows',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'purchase',\n",
              "   'of',\n",
              "   'short-term',\n",
              "   'investments',\n",
              "   'and',\n",
              "   'our',\n",
              "   'tender',\n",
              "   'offer',\n",
              "   'to',\n",
              "   'acquire',\n",
              "   'shares',\n",
              "   'of',\n",
              "   'LATAM',\n",
              "   'in',\n",
              "   'January',\n",
              "   '2020',\n",
              "   '.',\n",
              "   'These',\n",
              "   'results',\n",
              "   'generated',\n",
              "   '$',\n",
              "   '4.3',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'negative',\n",
              "   'free',\n",
              "   'cash',\n",
              "   'flow',\n",
              "   '(',\n",
              "   'a',\n",
              "   'non-GAAP',\n",
              "   'financial',\n",
              "   'measure',\n",
              "   ')',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '4.2',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'free',\n",
              "   'cash',\n",
              "   'flow',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['In',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'following',\n",
              "   'the',\n",
              "   'onset',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   ',',\n",
              "   'reduced',\n",
              "   'industry',\n",
              "   'capacity',\n",
              "   'drove',\n",
              "   'a',\n",
              "   'significant',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'our',\n",
              "   'cargo',\n",
              "   'yield',\n",
              "   ',',\n",
              "   'and',\n",
              "   'we',\n",
              "   'also',\n",
              "   'generated',\n",
              "   'cargo',\n",
              "   'revenue',\n",
              "   'through',\n",
              "   'the',\n",
              "   'operation',\n",
              "   'of',\n",
              "   'cargo-only',\n",
              "   'charter',\n",
              "   'flights',\n",
              "   '(',\n",
              "   'i.e.',\n",
              "   ',',\n",
              "   'using',\n",
              "   'aircraft',\n",
              "   'in',\n",
              "   'our',\n",
              "   'fleet',\n",
              "   'not',\n",
              "   'then',\n",
              "   'being',\n",
              "   'utilized',\n",
              "   'for',\n",
              "   'passenger',\n",
              "   'travel',\n",
              "   ')',\n",
              "   '.',\n",
              "   'These',\n",
              "   'two',\n",
              "   'factors',\n",
              "   'contributed',\n",
              "   'to',\n",
              "   'the',\n",
              "   'smaller',\n",
              "   'percentage',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'cargo',\n",
              "   'revenue',\n",
              "   ',',\n",
              "   'than',\n",
              "   'in',\n",
              "   'passenger',\n",
              "   'revenue',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year.The',\n",
              "   'length',\n",
              "   'and',\n",
              "   'severity',\n",
              "   'of',\n",
              "   'the',\n",
              "   'reduction',\n",
              "   'in',\n",
              "   'travel',\n",
              "   'demand',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'are',\n",
              "   'uncertain',\n",
              "   ',',\n",
              "   'as',\n",
              "   'described',\n",
              "   'in',\n",
              "   'Item',\n",
              "   '1A',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Refinery',\n",
              "   'sales',\n",
              "   'to',\n",
              "   'third',\n",
              "   'parties',\n",
              "   ',',\n",
              "   'which',\n",
              "   'are',\n",
              "   'at',\n",
              "   'or',\n",
              "   'near',\n",
              "   'cost',\n",
              "   ',',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.1',\n",
              "   'billion',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'third-party',\n",
              "   'refinery',\n",
              "   'sales',\n",
              "   'resulted',\n",
              "   'from',\n",
              "   'the',\n",
              "   'refinery',\n",
              "   \"'s\",\n",
              "   'shift',\n",
              "   'to',\n",
              "   'producing',\n",
              "   'more',\n",
              "   'non-jet',\n",
              "   'fuel',\n",
              "   'products',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'demand',\n",
              "   'for',\n",
              "   'jet',\n",
              "   'fuel',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'volume',\n",
              "   'of',\n",
              "   'these',\n",
              "   'transactions',\n",
              "   'has',\n",
              "   'fallen',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   ',',\n",
              "   'and',\n",
              "   'our',\n",
              "   'response',\n",
              "   'to',\n",
              "   ',',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['These',\n",
              "   'include',\n",
              "   'many',\n",
              "   'of',\n",
              "   'the',\n",
              "   'cost',\n",
              "   'reduction',\n",
              "   'measures',\n",
              "   'and',\n",
              "   'programs',\n",
              "   'we',\n",
              "   'implemented',\n",
              "   'in',\n",
              "   'response',\n",
              "   'to',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Fuel',\n",
              "   'expense',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '5.3',\n",
              "   'billion',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   '51',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'capacity',\n",
              "   'and',\n",
              "   'an',\n",
              "   'approximately',\n",
              "   '25',\n",
              "   '%',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'the',\n",
              "   'market',\n",
              "   'price',\n",
              "   'per',\n",
              "   'gallon',\n",
              "   'of',\n",
              "   'jet',\n",
              "   'fuel',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Most',\n",
              "   'aircraft',\n",
              "   'operating',\n",
              "   'lease',\n",
              "   'expenses',\n",
              "   'are',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   'aircraft',\n",
              "   'rent',\n",
              "   'and',\n",
              "   'are',\n",
              "   'contractually',\n",
              "   'fixed',\n",
              "   '.',\n",
              "   'Therefore',\n",
              "   ',',\n",
              "   'aircraft',\n",
              "   'rent',\n",
              "   'did',\n",
              "   'not',\n",
              "   'decline',\n",
              "   'to',\n",
              "   'the',\n",
              "   'same',\n",
              "   'extent',\n",
              "   'as',\n",
              "   'our',\n",
              "   'other',\n",
              "   'operating',\n",
              "   'expense',\n",
              "   'line',\n",
              "   'items',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019.Restructuring',\n",
              "   'Charges',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Restructuring',\n",
              "   'charges',\n",
              "   'are',\n",
              "   'composed',\n",
              "   'of',\n",
              "   'various',\n",
              "   'expenses',\n",
              "   'that',\n",
              "   'resulted',\n",
              "   'from',\n",
              "   'our',\n",
              "   'response',\n",
              "   'to',\n",
              "   'the',\n",
              "   'unprecedented',\n",
              "   'impact',\n",
              "   'on',\n",
              "   'our',\n",
              "   'business',\n",
              "   'from',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'fleet',\n",
              "   'impairment',\n",
              "   'and',\n",
              "   'related',\n",
              "   'charges',\n",
              "   'and',\n",
              "   'voluntary',\n",
              "   'separation',\n",
              "   'program',\n",
              "   'charges',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Interest',\n",
              "   'expense',\n",
              "   'increased',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'financing',\n",
              "   'arrangements',\n",
              "   'entered',\n",
              "   'into',\n",
              "   'during',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['We',\n",
              "   'believe',\n",
              "   'that',\n",
              "   'the',\n",
              "   'jet',\n",
              "   'fuel',\n",
              "   'supply',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'the',\n",
              "   'refinery',\n",
              "   \"'s\",\n",
              "   'operation',\n",
              "   'generally',\n",
              "   'contributes',\n",
              "   'to',\n",
              "   'reducing',\n",
              "   'the',\n",
              "   'market',\n",
              "   'price',\n",
              "   'of',\n",
              "   'jet',\n",
              "   'fuel',\n",
              "   'and',\n",
              "   'lowers',\n",
              "   'our',\n",
              "   'cost',\n",
              "   'of',\n",
              "   'jet',\n",
              "   'fuel',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'what',\n",
              "   'it',\n",
              "   'otherwise',\n",
              "   'would',\n",
              "   'be',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Refinery',\n",
              "   'revenues',\n",
              "   'decreased',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'periods',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'refinery',\n",
              "   'run',\n",
              "   'rates',\n",
              "   'during',\n",
              "   'the',\n",
              "   'year',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'lower',\n",
              "   'pricing',\n",
              "   'for',\n",
              "   'refined',\n",
              "   'products',\n",
              "   '.',\n",
              "   'The',\n",
              "   'refinery',\n",
              "   'recorded',\n",
              "   'an',\n",
              "   'operating',\n",
              "   'loss',\n",
              "   'of',\n",
              "   '$',\n",
              "   '216',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'operating',\n",
              "   'income',\n",
              "   'of',\n",
              "   '$',\n",
              "   '76',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   'air',\n",
              "   'traffic',\n",
              "   'liability',\n",
              "   'has',\n",
              "   'historically',\n",
              "   'increased',\n",
              "   'during',\n",
              "   'the',\n",
              "   'winter',\n",
              "   'and',\n",
              "   'spring',\n",
              "   'as',\n",
              "   'advanced',\n",
              "   'ticket',\n",
              "   'sales',\n",
              "   'grow',\n",
              "   'prior',\n",
              "   'to',\n",
              "   'the',\n",
              "   'summer',\n",
              "   'peak',\n",
              "   'travel',\n",
              "   'season',\n",
              "   'and',\n",
              "   'decreased',\n",
              "   'during',\n",
              "   'the',\n",
              "   'summer',\n",
              "   'and',\n",
              "   'fall',\n",
              "   'months',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['We',\n",
              "   'paid',\n",
              "   '$',\n",
              "   '1.6',\n",
              "   'billion',\n",
              "   'in',\n",
              "   'profit',\n",
              "   'sharing',\n",
              "   'in',\n",
              "   'February',\n",
              "   '2020',\n",
              "   'related',\n",
              "   'to',\n",
              "   'our',\n",
              "   '2019',\n",
              "   'pre-tax',\n",
              "   'profit',\n",
              "   'in',\n",
              "   'recognition',\n",
              "   'of',\n",
              "   'our',\n",
              "   'employees',\n",
              "   \"'\",\n",
              "   'contributions',\n",
              "   'toward',\n",
              "   'meeting',\n",
              "   'our',\n",
              "   'financial',\n",
              "   'goals',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'and',\n",
              "   'our',\n",
              "   'response',\n",
              "   ',',\n",
              "   'we',\n",
              "   'have',\n",
              "   'removed',\n",
              "   'certain',\n",
              "   'aircraft',\n",
              "   'from',\n",
              "   'active',\n",
              "   'service',\n",
              "   'as',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'which',\n",
              "   'includes',\n",
              "   'owned',\n",
              "   'and',\n",
              "   'leased',\n",
              "   'aircraft',\n",
              "   'that',\n",
              "   'are',\n",
              "   'being',\n",
              "   'retired',\n",
              "   'early',\n",
              "   '.',\n",
              "   'This',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'impairment',\n",
              "   'and',\n",
              "   'other',\n",
              "   'related',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '4.4',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   'restructuring',\n",
              "   'charges',\n",
              "   'in',\n",
              "   'our',\n",
              "   'income',\n",
              "   'statement',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['At',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'our',\n",
              "   'net',\n",
              "   'deferred',\n",
              "   'tax',\n",
              "   'asset',\n",
              "   'balance',\n",
              "   'was',\n",
              "   '$',\n",
              "   '2.0',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'including',\n",
              "   'a',\n",
              "   '$',\n",
              "   '460',\n",
              "   'million',\n",
              "   'valuation',\n",
              "   'allowance',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'capital',\n",
              "   'loss',\n",
              "   'carryforwards',\n",
              "   'and',\n",
              "   'state',\n",
              "   'net',\n",
              "   'operating',\n",
              "   'losses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'gains/losses',\n",
              "   'are',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'changes',\n",
              "   'in',\n",
              "   'stock',\n",
              "   'prices',\n",
              "   ',',\n",
              "   'foreign',\n",
              "   'currency',\n",
              "   'fluctuations',\n",
              "   'and',\n",
              "   'other',\n",
              "   'valuation',\n",
              "   'techniques',\n",
              "   'for',\n",
              "   'investments',\n",
              "   'in',\n",
              "   'companies',\n",
              "   'without',\n",
              "   'publicly-traded',\n",
              "   'shares',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Selling',\n",
              "   ',',\n",
              "   'general',\n",
              "   'and',\n",
              "   'administrative',\n",
              "   'expenses',\n",
              "   'increased',\n",
              "   'by',\n",
              "   '$',\n",
              "   '499',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '16',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '3,699',\n",
              "   'million',\n",
              "   'principally',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'inclusion',\n",
              "   'of',\n",
              "   'acquired',\n",
              "   'businesses',\n",
              "   'and',\n",
              "   'overhead',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   'partly',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'our',\n",
              "   'continued',\n",
              "   'focus',\n",
              "   'on',\n",
              "   'cost',\n",
              "   'management',\n",
              "   ',',\n",
              "   'including',\n",
              "   'savings',\n",
              "   'from',\n",
              "   'restructuring',\n",
              "   'actions',\n",
              "   'initiated',\n",
              "   'in',\n",
              "   '2012',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'a',\n",
              "   'percent',\n",
              "   'of',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'selling',\n",
              "   ',',\n",
              "   'general',\n",
              "   'and',\n",
              "   'administrative',\n",
              "   'expenses',\n",
              "   'increased',\n",
              "   'slightly',\n",
              "   'to',\n",
              "   '24.5',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2013',\n",
              "   'from',\n",
              "   '23.7',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'addition',\n",
              "   'of',\n",
              "   'acquired',\n",
              "   'businesses',\n",
              "   'which',\n",
              "   'have',\n",
              "   'higher',\n",
              "   'distribution',\n",
              "   'related',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Other',\n",
              "   'charges',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '34',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '15',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '190',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'legacy',\n",
              "   'environmental',\n",
              "   'remediation',\n",
              "   'charges',\n",
              "   'in',\n",
              "   '2013',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Other',\n",
              "   'income',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '8',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '6',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '131',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'equity',\n",
              "   'earnings',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'from',\n",
              "   'our',\n",
              "   'Asian',\n",
              "   'fiber',\n",
              "   'glass',\n",
              "   'joint',\n",
              "   'ventures',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'demand',\n",
              "   'in',\n",
              "   'the',\n",
              "   'personal',\n",
              "   'computer',\n",
              "   'market',\n",
              "   'and',\n",
              "   'lower',\n",
              "   'licensing',\n",
              "   'earnings',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Glass',\n",
              "   'segment',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'diluted',\n",
              "   'earnings-per-share',\n",
              "   'resulted',\n",
              "   'from',\n",
              "   'higher',\n",
              "   'net',\n",
              "   'income',\n",
              "   'and',\n",
              "   'a',\n",
              "   'reduction',\n",
              "   'in',\n",
              "   'the',\n",
              "   'number',\n",
              "   'of',\n",
              "   'shares',\n",
              "   'outstanding',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   '10.8',\n",
              "   'million',\n",
              "   'PPG',\n",
              "   'shares',\n",
              "   'tendered',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'in',\n",
              "   'the',\n",
              "   'exchange',\n",
              "   'offer',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'the',\n",
              "   'separation',\n",
              "   'and',\n",
              "   'merger',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'former',\n",
              "   'commodity',\n",
              "   'chemicals',\n",
              "   'business',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'approximately',\n",
              "   '5.7',\n",
              "   'million',\n",
              "   'shares',\n",
              "   'repurchased',\n",
              "   'during',\n",
              "   '2013',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Offsetting',\n",
              "   'the',\n",
              "   'segment',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'gains',\n",
              "   'was',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'volume',\n",
              "   'in',\n",
              "   'the',\n",
              "   'protective',\n",
              "   'and',\n",
              "   'marine',\n",
              "   'coatings',\n",
              "   'business',\n",
              "   'due',\n",
              "   'to',\n",
              "   'further',\n",
              "   ',',\n",
              "   'notable',\n",
              "   'weakness',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Asian',\n",
              "   'marine',\n",
              "   'new-build',\n",
              "   'market',\n",
              "   'reflecting',\n",
              "   'lower',\n",
              "   'global',\n",
              "   'demand',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'lower',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'in',\n",
              "   'the',\n",
              "   'national',\n",
              "   'account',\n",
              "   'channel',\n",
              "   'was',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'previously',\n",
              "   'disclosed',\n",
              "   'change',\n",
              "   'in',\n",
              "   'products',\n",
              "   'sold',\n",
              "   'to',\n",
              "   'a',\n",
              "   'large',\n",
              "   'retail',\n",
              "   'customer',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Segment',\n",
              "   'income',\n",
              "   'was',\n",
              "   '$',\n",
              "   '858',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2013',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '114',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '15',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'income',\n",
              "   'from',\n",
              "   'acquired',\n",
              "   'businesses',\n",
              "   ',',\n",
              "   'lower',\n",
              "   'overhead',\n",
              "   'and',\n",
              "   'manufacturing',\n",
              "   'costs',\n",
              "   'stemming',\n",
              "   'from',\n",
              "   'prior',\n",
              "   'restructuring',\n",
              "   'actions',\n",
              "   'and',\n",
              "   'ongoing',\n",
              "   'cost',\n",
              "   'management',\n",
              "   ',',\n",
              "   'offset',\n",
              "   'partially',\n",
              "   'by',\n",
              "   'the',\n",
              "   'negative',\n",
              "   'impact',\n",
              "   'on',\n",
              "   'segment',\n",
              "   'income',\n",
              "   'from',\n",
              "   'the',\n",
              "   'lower',\n",
              "   'sales',\n",
              "   'volumes',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Poor',\n",
              "   'weather',\n",
              "   'conditions',\n",
              "   'in',\n",
              "   'the',\n",
              "   'first',\n",
              "   'half',\n",
              "   'of',\n",
              "   '2013',\n",
              "   'were',\n",
              "   'also',\n",
              "   'a',\n",
              "   'contributor',\n",
              "   'to',\n",
              "   'the',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'volumes',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Despite',\n",
              "   'the',\n",
              "   'year-over-year',\n",
              "   'volume',\n",
              "   'decline',\n",
              "   ',',\n",
              "   'segment',\n",
              "   'income',\n",
              "   'was',\n",
              "   '$',\n",
              "   '184',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2013',\n",
              "   ',',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '39',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '27',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'including',\n",
              "   'discretionary',\n",
              "   'cost',\n",
              "   'management',\n",
              "   'coupled',\n",
              "   'with',\n",
              "   'the',\n",
              "   'structural',\n",
              "   'cost',\n",
              "   'improvements',\n",
              "   'stemming',\n",
              "   'from',\n",
              "   'the',\n",
              "   'restructuring',\n",
              "   'actions',\n",
              "   'initiated',\n",
              "   'in',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'lower',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'on',\n",
              "   'income',\n",
              "   'from',\n",
              "   'continuing',\n",
              "   'operations',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Optical',\n",
              "   'and',\n",
              "   'Specialty',\n",
              "   'Materials',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '60',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '5',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '1,262',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'volumes',\n",
              "   '(',\n",
              "   '3',\n",
              "   '%',\n",
              "   ')',\n",
              "   'and',\n",
              "   'higher',\n",
              "   'pricing',\n",
              "   '(',\n",
              "   '2',\n",
              "   '%',\n",
              "   ')',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Sales',\n",
              "   'volumes',\n",
              "   'in',\n",
              "   'fiber',\n",
              "   'glass',\n",
              "   'increased',\n",
              "   'modestly',\n",
              "   ',',\n",
              "   'reflecting',\n",
              "   'higher',\n",
              "   'global',\n",
              "   'demand',\n",
              "   'in',\n",
              "   'various',\n",
              "   'end-use',\n",
              "   'markets',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Pension',\n",
              "   'and',\n",
              "   'postretirement',\n",
              "   'benefit',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'excluding',\n",
              "   'curtailments',\n",
              "   'and',\n",
              "   'special',\n",
              "   'termination',\n",
              "   'benefits',\n",
              "   ',',\n",
              "   'were',\n",
              "   '$',\n",
              "   '177',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2013',\n",
              "   ',',\n",
              "   'down',\n",
              "   '$',\n",
              "   '56',\n",
              "   'million',\n",
              "   'from',\n",
              "   '$',\n",
              "   '233',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2012',\n",
              "   '.',\n",
              "   'This',\n",
              "   'decline',\n",
              "   'was',\n",
              "   'due',\n",
              "   ',',\n",
              "   'in',\n",
              "   'part',\n",
              "   ',',\n",
              "   'to',\n",
              "   'a',\n",
              "   'reorganization',\n",
              "   'of',\n",
              "   'certain',\n",
              "   'Company',\n",
              "   'pension',\n",
              "   'plans',\n",
              "   ',',\n",
              "   'which',\n",
              "   'occurred',\n",
              "   'as',\n",
              "   'a',\n",
              "   'part',\n",
              "   'of',\n",
              "   'separation',\n",
              "   'activities',\n",
              "   'of',\n",
              "   'the',\n",
              "   'former',\n",
              "   'commodity',\n",
              "   'chemicals',\n",
              "   'business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   '2012',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '359',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '3',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '13,512',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'selling',\n",
              "   'prices',\n",
              "   '(',\n",
              "   '3',\n",
              "   '%',\n",
              "   ')',\n",
              "   ',',\n",
              "   'higher',\n",
              "   'sales',\n",
              "   'volumes',\n",
              "   '(',\n",
              "   '1',\n",
              "   '%',\n",
              "   ')',\n",
              "   'and',\n",
              "   'sales',\n",
              "   'from',\n",
              "   'acquired',\n",
              "   'businesses',\n",
              "   '(',\n",
              "   '2',\n",
              "   '%',\n",
              "   ')',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'unfavorable',\n",
              "   'foreign',\n",
              "   'currency',\n",
              "   'impact',\n",
              "   '(',\n",
              "   '3',\n",
              "   '%',\n",
              "   ')',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['European',\n",
              "   'volumes',\n",
              "   'declined',\n",
              "   '4',\n",
              "   '%',\n",
              "   'versus',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   'period',\n",
              "   'with',\n",
              "   'every',\n",
              "   'coatings',\n",
              "   'business',\n",
              "   'except',\n",
              "   'aerospace',\n",
              "   'experiencing',\n",
              "   'sluggish',\n",
              "   'end-use',\n",
              "   'market',\n",
              "   'conditions',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'unfavorable',\n",
              "   'currency',\n",
              "   'impact',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'U.S.',\n",
              "   'dollar',\n",
              "   'strengthening',\n",
              "   'against',\n",
              "   'the',\n",
              "   'Euro',\n",
              "   'and',\n",
              "   'Latin',\n",
              "   'American',\n",
              "   'currencies',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['Increased',\n",
              "   'demand',\n",
              "   'was',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'stronger',\n",
              "   'industrial',\n",
              "   'production',\n",
              "   'activity',\n",
              "   ',',\n",
              "   'which',\n",
              "   'aided',\n",
              "   'many',\n",
              "   'of',\n",
              "   'our',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Other',\n",
              "   'income',\n",
              "   'decreased',\n",
              "   'by',\n",
              "   '$',\n",
              "   '13',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '9',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '139',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   '$',\n",
              "   '27',\n",
              "   'million',\n",
              "   'of',\n",
              "   'lower',\n",
              "   'equity',\n",
              "   'earnings',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'from',\n",
              "   'our',\n",
              "   'Asian',\n",
              "   'fiber',\n",
              "   'glass',\n",
              "   'joint',\n",
              "   'ventures',\n",
              "   ',',\n",
              "   'reflecting',\n",
              "   'demand',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'the',\n",
              "   'consumer',\n",
              "   'electronics',\n",
              "   'market',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'Industrial',\n",
              "   'Coatings',\n",
              "   'segment',\n",
              "   '’',\n",
              "   's',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '221',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '5',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '4,379',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'pricing',\n",
              "   '(',\n",
              "   '3',\n",
              "   '%',\n",
              "   ')',\n",
              "   ',',\n",
              "   'higher',\n",
              "   'sales',\n",
              "   'volumes',\n",
              "   '(',\n",
              "   '4',\n",
              "   '%',\n",
              "   ')',\n",
              "   'and',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'from',\n",
              "   'acquired',\n",
              "   'businesses',\n",
              "   '(',\n",
              "   '1',\n",
              "   '%',\n",
              "   ')',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'unfavorable',\n",
              "   'foreign',\n",
              "   'currency',\n",
              "   'translation',\n",
              "   '(',\n",
              "   '3',\n",
              "   '%',\n",
              "   ')',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['EMEA',\n",
              "   'segment',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '43',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '2',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '2,147',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'acquisition',\n",
              "   'of',\n",
              "   'Dyrup',\n",
              "   'in',\n",
              "   'January',\n",
              "   '2012',\n",
              "   '(',\n",
              "   '8',\n",
              "   '%',\n",
              "   ')',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'unfavorable',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'foreign',\n",
              "   'currency',\n",
              "   'translation',\n",
              "   '(',\n",
              "   '7',\n",
              "   '%',\n",
              "   ')',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Segment',\n",
              "   'income',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '22',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '18',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '145',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'costs',\n",
              "   'stemming',\n",
              "   'from',\n",
              "   'aggressive',\n",
              "   'ongoing',\n",
              "   'cost',\n",
              "   'management',\n",
              "   'and',\n",
              "   'supplemented',\n",
              "   'by',\n",
              "   'the',\n",
              "   'cost',\n",
              "   'benefits',\n",
              "   'from',\n",
              "   'PPG',\n",
              "   \"'s\",\n",
              "   'restructuring',\n",
              "   'actions',\n",
              "   'and',\n",
              "   'higher',\n",
              "   'pricing',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Optical',\n",
              "   'products',\n",
              "   'achieved',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'growth',\n",
              "   'with',\n",
              "   'the',\n",
              "   'majority',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'Transitions',\n",
              "   'lens',\n",
              "   'market',\n",
              "   'penetration',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Segment',\n",
              "   'income',\n",
              "   'increased',\n",
              "   'by',\n",
              "   '$',\n",
              "   '22',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '7',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '348',\n",
              "   'million',\n",
              "   'as',\n",
              "   'income',\n",
              "   'improved',\n",
              "   'in',\n",
              "   'both',\n",
              "   'businesses',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'sales',\n",
              "   'volumes',\n",
              "   ',',\n",
              "   'overhead',\n",
              "   'and',\n",
              "   'manufacturing',\n",
              "   'cost',\n",
              "   'improvements',\n",
              "   ',',\n",
              "   'including',\n",
              "   'restructuring',\n",
              "   'cost',\n",
              "   'savings',\n",
              "   ',',\n",
              "   'and',\n",
              "   'higher',\n",
              "   'pricing',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Segment',\n",
              "   'income',\n",
              "   'declined',\n",
              "   '$',\n",
              "   '34',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '35',\n",
              "   '%',\n",
              "   'from',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'to',\n",
              "   '$',\n",
              "   '63',\n",
              "   'million',\n",
              "   '.',\n",
              "   'Lower',\n",
              "   'pricing',\n",
              "   ',',\n",
              "   'cost',\n",
              "   'inflation',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'lower',\n",
              "   'equity',\n",
              "   'earnings',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'our',\n",
              "   'fiber',\n",
              "   'glass',\n",
              "   'joint',\n",
              "   'venture',\n",
              "   'selling',\n",
              "   'to',\n",
              "   'the',\n",
              "   'consumer',\n",
              "   'electronics',\n",
              "   'industry',\n",
              "   ',',\n",
              "   'contributed',\n",
              "   'to',\n",
              "   'the',\n",
              "   'income',\n",
              "   'decline',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['A',\n",
              "   'charge',\n",
              "   'of',\n",
              "   '$',\n",
              "   '145',\n",
              "   'million',\n",
              "   'was',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   'the',\n",
              "   'first',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2012',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'updated',\n",
              "   'information',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'compiled',\n",
              "   'about',\n",
              "   'the',\n",
              "   'sites',\n",
              "   'that',\n",
              "   'was',\n",
              "   'used',\n",
              "   'to',\n",
              "   'develop',\n",
              "   'a',\n",
              "   'new',\n",
              "   'estimate',\n",
              "   'of',\n",
              "   'the',\n",
              "   'cost',\n",
              "   'to',\n",
              "   'remediate',\n",
              "   'these',\n",
              "   'sites',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Cash',\n",
              "   'from',\n",
              "   'operations',\n",
              "   'in',\n",
              "   '2013',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2012',\n",
              "   'was',\n",
              "   'aided',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'income',\n",
              "   'from',\n",
              "   'continuing',\n",
              "   'operations',\n",
              "   'which',\n",
              "   'was',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'income',\n",
              "   'from',\n",
              "   'acquired',\n",
              "   'businesses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   ',',\n",
              "   'about',\n",
              "   '10.8',\n",
              "   'million',\n",
              "   'shares',\n",
              "   'were',\n",
              "   'added',\n",
              "   'to',\n",
              "   'treasury',\n",
              "   'stock',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'January',\n",
              "   '2013',\n",
              "   'exchange',\n",
              "   'transaction',\n",
              "   'that',\n",
              "   'was',\n",
              "   'part',\n",
              "   'of',\n",
              "   'the',\n",
              "   'separation',\n",
              "   'of',\n",
              "   'the',\n",
              "   'commodity',\n",
              "   'chemicals',\n",
              "   'business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['From',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   'to',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2013',\n",
              "   ',',\n",
              "   'the',\n",
              "   'U.S.',\n",
              "   'dollar',\n",
              "   'strengthened',\n",
              "   'against',\n",
              "   'certain',\n",
              "   'currencies',\n",
              "   'in',\n",
              "   'the',\n",
              "   'countries',\n",
              "   'in',\n",
              "   'which',\n",
              "   'PPG',\n",
              "   'operates',\n",
              "   ',',\n",
              "   'while',\n",
              "   'weakening',\n",
              "   'against',\n",
              "   'others',\n",
              "   ',',\n",
              "   'the',\n",
              "   'most',\n",
              "   'notable',\n",
              "   'being',\n",
              "   'the',\n",
              "   'Euro',\n",
              "   '.',\n",
              "   'As',\n",
              "   'a',\n",
              "   'result',\n",
              "   ',',\n",
              "   'consolidated',\n",
              "   'net',\n",
              "   'assets',\n",
              "   'at',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2013',\n",
              "   'decreased',\n",
              "   'by',\n",
              "   '$',\n",
              "   '44',\n",
              "   'million',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['A',\n",
              "   '$',\n",
              "   '141',\n",
              "   'million',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'PPG',\n",
              "   'consolidated',\n",
              "   'net',\n",
              "   'assets',\n",
              "   'and',\n",
              "   'shareholders',\n",
              "   'equity',\n",
              "   'resulted',\n",
              "   'from',\n",
              "   'translating',\n",
              "   'PPG',\n",
              "   '’',\n",
              "   's',\n",
              "   'foreign',\n",
              "   'denominated',\n",
              "   'net',\n",
              "   'assets',\n",
              "   'to',\n",
              "   'U.S.',\n",
              "   'dollars',\n",
              "   'at',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2012',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2011',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Results',\n",
              "   'in',\n",
              "   '2020',\n",
              "   'and',\n",
              "   '2019',\n",
              "   'include',\n",
              "   'net',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '0.94',\n",
              "   'and',\n",
              "   '$',\n",
              "   '0.72',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   '2018',\n",
              "   'Global',\n",
              "   'Restructuring',\n",
              "   'Program',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Changes',\n",
              "   'in',\n",
              "   'net',\n",
              "   'selling',\n",
              "   'prices',\n",
              "   'increased',\n",
              "   'sales',\n",
              "   'by',\n",
              "   '4',\n",
              "   'percent',\n",
              "   ',',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'lower',\n",
              "   'promotion',\n",
              "   'spending',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Earnings',\n",
              "   'of',\n",
              "   '$',\n",
              "   '5.2',\n",
              "   'billion',\n",
              "   'were',\n",
              "   'previously',\n",
              "   'subject',\n",
              "   'to',\n",
              "   'tax',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'one-time',\n",
              "   'transition',\n",
              "   'tax',\n",
              "   'on',\n",
              "   'foreign',\n",
              "   'earnings',\n",
              "   'required',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Tax',\n",
              "   'Cuts',\n",
              "   'and',\n",
              "   'Jobs',\n",
              "   'Act',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'conjunction',\n",
              "   'with',\n",
              "   'the',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'annual',\n",
              "   'impairment',\n",
              "   'testing',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'and',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   'during',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   '(',\n",
              "   'on',\n",
              "   'December',\n",
              "   '1',\n",
              "   ')',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'recorded',\n",
              "   'a',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charge',\n",
              "   'of',\n",
              "   '$',\n",
              "   '20',\n",
              "   'million',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'a',\n",
              "   'tradename',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Learning',\n",
              "   'and',\n",
              "   'Development',\n",
              "   'segment',\n",
              "   ',',\n",
              "   'as',\n",
              "   'its',\n",
              "   'carrying',\n",
              "   'value',\n",
              "   'exceeded',\n",
              "   'its',\n",
              "   'fair',\n",
              "   'value',\n",
              "   '.',\n",
              "   'The',\n",
              "   'impairment',\n",
              "   'reflected',\n",
              "   'a',\n",
              "   'downward',\n",
              "   'revision',\n",
              "   'of',\n",
              "   'forecasted',\n",
              "   'results',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'the',\n",
              "   'delayed',\n",
              "   'and',\n",
              "   'limited',\n",
              "   're-opening',\n",
              "   'of',\n",
              "   'schools',\n",
              "   'and',\n",
              "   'offices',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'global',\n",
              "   'pandemic',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'continued',\n",
              "   'deterioration',\n",
              "   'in',\n",
              "   'sales',\n",
              "   'for',\n",
              "   'slime-related',\n",
              "   'adhesive',\n",
              "   'products.27During',\n",
              "   'the',\n",
              "   'third',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'concluded',\n",
              "   'that',\n",
              "   'a',\n",
              "   'triggering',\n",
              "   'event',\n",
              "   'had',\n",
              "   'occurred',\n",
              "   'for',\n",
              "   'an',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'asset',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Learning',\n",
              "   'and',\n",
              "   'Development',\n",
              "   'segment',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'decline',\n",
              "   'was',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Appliances',\n",
              "   'and',\n",
              "   'Cookware',\n",
              "   ',',\n",
              "   'Commercial',\n",
              "   'Solutions',\n",
              "   'and',\n",
              "   'Home',\n",
              "   'Solutions',\n",
              "   'segments',\n",
              "   'due',\n",
              "   'to',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'demand',\n",
              "   ',',\n",
              "   'notably',\n",
              "   'through',\n",
              "   'online',\n",
              "   'channels',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'gross',\n",
              "   'margin',\n",
              "   'decline',\n",
              "   'was',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'costs',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'lower',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'and',\n",
              "   'certain',\n",
              "   'temporary',\n",
              "   'manufacturing',\n",
              "   'closures',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'during',\n",
              "   'the',\n",
              "   'first',\n",
              "   'half',\n",
              "   'of',\n",
              "   'the',\n",
              "   'year',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'business',\n",
              "   'unit',\n",
              "   'mix',\n",
              "   'and',\n",
              "   'inflation',\n",
              "   'related',\n",
              "   'to',\n",
              "   'input',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'gross',\n",
              "   'profit',\n",
              "   'decline',\n",
              "   'was',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'cumulative',\n",
              "   'depreciation',\n",
              "   'expense',\n",
              "   'recorded',\n",
              "   'during',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'decision',\n",
              "   'to',\n",
              "   'retain',\n",
              "   'the',\n",
              "   'Commercial',\n",
              "   'Business',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'gross',\n",
              "   'productivity',\n",
              "   'and',\n",
              "   'lower',\n",
              "   'product',\n",
              "   'recall',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Restructuring-related',\n",
              "   'costs',\n",
              "   'reported',\n",
              "   'in',\n",
              "   'cost',\n",
              "   'of',\n",
              "   'products',\n",
              "   'sold',\n",
              "   'and',\n",
              "   'SG',\n",
              "   '&',\n",
              "   'A',\n",
              "   'for',\n",
              "   '2019',\n",
              "   'were',\n",
              "   '$',\n",
              "   '16',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '39',\n",
              "   'million',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'and',\n",
              "   'primarily',\n",
              "   'relate',\n",
              "   'to',\n",
              "   'accelerated',\n",
              "   'depreciation',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'restructuring',\n",
              "   'activities',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Interest',\n",
              "   'expense',\n",
              "   ',',\n",
              "   'net',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'decreased',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'debt',\n",
              "   'levels',\n",
              "   ',',\n",
              "   'slightly',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'higher',\n",
              "   'rate',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'previously',\n",
              "   'disclosed',\n",
              "   'debt',\n",
              "   'ratings',\n",
              "   'downgrades',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Connected',\n",
              "   'Home',\n",
              "   'and',\n",
              "   'Security',\n",
              "   'business',\n",
              "   'unit',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'ongoing',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   ',',\n",
              "   'which',\n",
              "   'shifted',\n",
              "   'consumer',\n",
              "   'purchasing',\n",
              "   'patterns',\n",
              "   'and',\n",
              "   'caused',\n",
              "   'supply',\n",
              "   'chain',\n",
              "   'constraints',\n",
              "   ',',\n",
              "   'impacting',\n",
              "   'sales',\n",
              "   'to',\n",
              "   'its',\n",
              "   'retail',\n",
              "   'and',\n",
              "   'contractor',\n",
              "   'channels',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'loss',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'decreased',\n",
              "   'to',\n",
              "   '$',\n",
              "   '85',\n",
              "   'million',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '136',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'improvement',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'results',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'cumulative',\n",
              "   'depreciation',\n",
              "   'and',\n",
              "   'amortization',\n",
              "   'expense',\n",
              "   'adjustment',\n",
              "   'in',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'decision',\n",
              "   'to',\n",
              "   'retain',\n",
              "   'the',\n",
              "   'Commercial',\n",
              "   'Business',\n",
              "   '.',\n",
              "   'The',\n",
              "   'improvement',\n",
              "   'in',\n",
              "   'the',\n",
              "   'operating',\n",
              "   'results',\n",
              "   'also',\n",
              "   'reflects',\n",
              "   'cost',\n",
              "   'containment',\n",
              "   'initiatives',\n",
              "   'to',\n",
              "   'mitigate',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'while',\n",
              "   'facilities',\n",
              "   'were',\n",
              "   'closed',\n",
              "   ',',\n",
              "   'including',\n",
              "   'employee',\n",
              "   'furloughs',\n",
              "   ',',\n",
              "   'gross',\n",
              "   'productivity',\n",
              "   'and',\n",
              "   'lower',\n",
              "   'discretionary',\n",
              "   'spending',\n",
              "   ',',\n",
              "   'including',\n",
              "   'advertising',\n",
              "   'and',\n",
              "   'promotional',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'Home',\n",
              "   'Fragrance',\n",
              "   'sales',\n",
              "   'also',\n",
              "   'reflected',\n",
              "   'the',\n",
              "   'exit',\n",
              "   'of',\n",
              "   '77',\n",
              "   'underperforming',\n",
              "   'Yankee',\n",
              "   'Candle',\n",
              "   'retail',\n",
              "   'stores',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'exiting',\n",
              "   'of',\n",
              "   'its',\n",
              "   'fundraising',\n",
              "   'business',\n",
              "   'in',\n",
              "   'third',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'loss',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'decreased',\n",
              "   'to',\n",
              "   '$',\n",
              "   '12',\n",
              "   'million',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '17',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'loss',\n",
              "   'reflects',\n",
              "   'higher',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Food',\n",
              "   'business',\n",
              "   'unit',\n",
              "   ',',\n",
              "   'cost',\n",
              "   'containment',\n",
              "   'initiatives',\n",
              "   'to',\n",
              "   'mitigate',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'while',\n",
              "   'Yankee',\n",
              "   'Candle',\n",
              "   'retail',\n",
              "   'stores',\n",
              "   'and',\n",
              "   'operating',\n",
              "   'facilities',\n",
              "   'were',\n",
              "   'temporarily',\n",
              "   'closed',\n",
              "   ',',\n",
              "   'including',\n",
              "   'employee',\n",
              "   'furloughs',\n",
              "   ',',\n",
              "   'lower',\n",
              "   'discretionary',\n",
              "   'spending',\n",
              "   ',',\n",
              "   'including',\n",
              "   'promotional',\n",
              "   'costs',\n",
              "   ',',\n",
              "   'and',\n",
              "   'gross',\n",
              "   'productivity',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'loss',\n",
              "   'was',\n",
              "   'also',\n",
              "   'unfavorably',\n",
              "   'impacted',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'costs',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'lower',\n",
              "   'sales',\n",
              "   'volume',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Home',\n",
              "   'Fragrance',\n",
              "   'business',\n",
              "   'unit',\n",
              "   'and',\n",
              "   'the',\n",
              "   'temporary',\n",
              "   'closure',\n",
              "   'of',\n",
              "   'their',\n",
              "   'key',\n",
              "   'manufacturing',\n",
              "   'facility',\n",
              "   'in',\n",
              "   'Massachusetts',\n",
              "   ',',\n",
              "   'during',\n",
              "   'the',\n",
              "   'first',\n",
              "   'half',\n",
              "   'of',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'and',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   '$',\n",
              "   '8',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'operating',\n",
              "   'leases',\n",
              "   'of',\n",
              "   'its',\n",
              "   'Yankee',\n",
              "   'Candle',\n",
              "   'retail',\n",
              "   'store',\n",
              "   'business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   ',',\n",
              "   'the',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'decline',\n",
              "   'also',\n",
              "   'reflected',\n",
              "   'softening',\n",
              "   'trends',\n",
              "   'related',\n",
              "   'to',\n",
              "   'sales',\n",
              "   'of',\n",
              "   'slime-related',\n",
              "   'adhesive',\n",
              "   'products',\n",
              "   ',',\n",
              "   'which',\n",
              "   'is',\n",
              "   'expected',\n",
              "   'to',\n",
              "   'continue',\n",
              "   'in',\n",
              "   '2021',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'exiting',\n",
              "   'of',\n",
              "   'the',\n",
              "   'North',\n",
              "   'American',\n",
              "   'distribution',\n",
              "   'of',\n",
              "   'Uniball®',\n",
              "   'products',\n",
              "   'in',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Gross',\n",
              "   'profit',\n",
              "   'for',\n",
              "   '2019',\n",
              "   'decreased',\n",
              "   '8',\n",
              "   '%',\n",
              "   'and',\n",
              "   'gross',\n",
              "   'profit',\n",
              "   'margin',\n",
              "   'declined',\n",
              "   'to',\n",
              "   '33.1',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'with',\n",
              "   '34.6',\n",
              "   '%',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'This',\n",
              "   'performance',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'net',\n",
              "   'sales',\n",
              "   ',',\n",
              "   'product',\n",
              "   'mix',\n",
              "   ',',\n",
              "   'inflation',\n",
              "   'related',\n",
              "   'to',\n",
              "   'input',\n",
              "   'costs',\n",
              "   'and',\n",
              "   'tariffs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['This',\n",
              "   'performance',\n",
              "   'also',\n",
              "   'included',\n",
              "   'overhead',\n",
              "   'reduction',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'various',\n",
              "   'initiatives',\n",
              "   ',',\n",
              "   'lower',\n",
              "   'restructuring',\n",
              "   'and',\n",
              "   'restructuring-related',\n",
              "   'charges',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'now',\n",
              "   'completed',\n",
              "   'ATP',\n",
              "   ',',\n",
              "   'and',\n",
              "   'the',\n",
              "   'lapping',\n",
              "   'of',\n",
              "   'a',\n",
              "   'bad',\n",
              "   'debt',\n",
              "   'write',\n",
              "   'off',\n",
              "   'of',\n",
              "   '$',\n",
              "   '26',\n",
              "   'million',\n",
              "   'from',\n",
              "   'a',\n",
              "   'large',\n",
              "   'customer',\n",
              "   'in',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['These',\n",
              "   'decreases',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'loss',\n",
              "   'and',\n",
              "   'operating',\n",
              "   'margin',\n",
              "   'were',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'cumulative',\n",
              "   'catch-up',\n",
              "   'adjustment',\n",
              "   'for',\n",
              "   'depreciation',\n",
              "   'and',\n",
              "   'amortization',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '57',\n",
              "   'million',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'decision',\n",
              "   'to',\n",
              "   'retain',\n",
              "   'the',\n",
              "   'Commercial',\n",
              "   'Business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'loss',\n",
              "   'on',\n",
              "   'the',\n",
              "   'extinguishment',\n",
              "   'of',\n",
              "   'debt',\n",
              "   'of',\n",
              "   '$',\n",
              "   '28',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '4',\n",
              "   'million',\n",
              "   'for',\n",
              "   '2019',\n",
              "   'and',\n",
              "   '2018',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'were',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'tender',\n",
              "   'offer',\n",
              "   'of',\n",
              "   'certain',\n",
              "   'of',\n",
              "   'its',\n",
              "   'senior',\n",
              "   'notes',\n",
              "   'and',\n",
              "   'debt',\n",
              "   'redemptions',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   ',',\n",
              "   'operating',\n",
              "   'loss',\n",
              "   'was',\n",
              "   'unfavorably',\n",
              "   'impacted',\n",
              "   'by',\n",
              "   'the',\n",
              "   'cumulative',\n",
              "   'catch-up',\n",
              "   'adjustment',\n",
              "   'for',\n",
              "   'depreciation',\n",
              "   'and',\n",
              "   'amortization',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '57',\n",
              "   'million',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Company',\n",
              "   '’',\n",
              "   's',\n",
              "   'decision',\n",
              "   'to',\n",
              "   'retain',\n",
              "   'the',\n",
              "   'Commercial',\n",
              "   'Business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Changes',\n",
              "   'in',\n",
              "   'foreign',\n",
              "   'currency',\n",
              "   'exchange',\n",
              "   'rates',\n",
              "   'unfavorably',\n",
              "   'impacted',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'by',\n",
              "   '$',\n",
              "   '23',\n",
              "   'million',\n",
              "   ',',\n",
              "   'or',\n",
              "   '2',\n",
              "   '%',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'was',\n",
              "   'also',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'softening',\n",
              "   'trends',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Writing',\n",
              "   'business',\n",
              "   'related',\n",
              "   'to',\n",
              "   'sales',\n",
              "   'of',\n",
              "   'slime-related',\n",
              "   'adhesive',\n",
              "   'products',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'international',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'growth',\n",
              "   'in',\n",
              "   'the',\n",
              "   'Writing',\n",
              "   'business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Outdoor',\n",
              "   'and',\n",
              "   'Recreation',\n",
              "   'segment',\n",
              "   'net',\n",
              "   'sales',\n",
              "   'for',\n",
              "   '2019',\n",
              "   'decreased',\n",
              "   '7',\n",
              "   '%',\n",
              "   'reflecting',\n",
              "   'lost',\n",
              "   'distribution',\n",
              "   'in',\n",
              "   'certain',\n",
              "   'product',\n",
              "   'categories',\n",
              "   'and',\n",
              "   'unfavorable',\n",
              "   'weather',\n",
              "   'conditions',\n",
              "   'affecting',\n",
              "   'the',\n",
              "   'Coleman',\n",
              "   'business',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'increased',\n",
              "   'sales',\n",
              "   'in',\n",
              "   'airbeds',\n",
              "   'and',\n",
              "   'outdoor',\n",
              "   'cooking',\n",
              "   'categories',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'income',\n",
              "   'and',\n",
              "   'operating',\n",
              "   'margins',\n",
              "   'improved',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   'certain',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   'of',\n",
              "   '$',\n",
              "   '25',\n",
              "   'million',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '351',\n",
              "   'million',\n",
              "   'of',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'and',\n",
              "   'certain',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Operating',\n",
              "   'loss',\n",
              "   'decreased',\n",
              "   'to',\n",
              "   '$',\n",
              "   '64',\n",
              "   'million',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '1.3',\n",
              "   'billion',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'Operating',\n",
              "   'loss',\n",
              "   'included',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   'goodwill',\n",
              "   'and',\n",
              "   'certain',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   'of',\n",
              "   '$',\n",
              "   '120',\n",
              "   'million',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   '2019',\n",
              "   'as',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '1.4',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'non-cash',\n",
              "   'impairment',\n",
              "   'charges',\n",
              "   'of',\n",
              "   'certain',\n",
              "   'indefinite-lived',\n",
              "   'intangible',\n",
              "   'assets',\n",
              "   'recorded',\n",
              "   'in',\n",
              "   '2018',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'operating',\n",
              "   'loss',\n",
              "   'was',\n",
              "   'also',\n",
              "   'due',\n",
              "   'to',\n",
              "   'overhead',\n",
              "   'reduction',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'various',\n",
              "   'initiatives',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'net',\n",
              "   'cash',\n",
              "   'provided',\n",
              "   'by',\n",
              "   'operating',\n",
              "   'activities',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'successful',\n",
              "   'working',\n",
              "   'capital',\n",
              "   'initiatives',\n",
              "   ',',\n",
              "   'which',\n",
              "   'included',\n",
              "   ':',\n",
              "   'the',\n",
              "   'extension',\n",
              "   'of',\n",
              "   'payment',\n",
              "   'terms',\n",
              "   'for',\n",
              "   'goods',\n",
              "   'and',\n",
              "   'services',\n",
              "   'with',\n",
              "   'vendors',\n",
              "   ',',\n",
              "   'enhanced',\n",
              "   'customer',\n",
              "   'credit',\n",
              "   'review',\n",
              "   'and',\n",
              "   'collections',\n",
              "   'processes',\n",
              "   'and',\n",
              "   'evaluating',\n",
              "   'supply',\n",
              "   'purchases',\n",
              "   'and',\n",
              "   'focused',\n",
              "   'inventory',\n",
              "   'management',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'investing',\n",
              "   'activities',\n",
              "   'for',\n",
              "   '2020',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'proceeds',\n",
              "   'from',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'divested',\n",
              "   'businesses',\n",
              "   'in',\n",
              "   'the',\n",
              "   'prior',\n",
              "   'year',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'lower',\n",
              "   'capital',\n",
              "   'expenditures',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'cash',\n",
              "   'used',\n",
              "   'in',\n",
              "   'investing',\n",
              "   'activities',\n",
              "   'for',\n",
              "   '2019',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'higher',\n",
              "   'proceeds',\n",
              "   'from',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'divested',\n",
              "   'businesses',\n",
              "   'in',\n",
              "   '2018',\n",
              "   ',',\n",
              "   'cash',\n",
              "   'received',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'in',\n",
              "   '2018',\n",
              "   'for',\n",
              "   'insurance',\n",
              "   'claims',\n",
              "   ',',\n",
              "   'which',\n",
              "   'include',\n",
              "   'a',\n",
              "   'claim',\n",
              "   'from',\n",
              "   'fire-related',\n",
              "   'losses',\n",
              "   'and',\n",
              "   'settlement',\n",
              "   'of',\n",
              "   'a',\n",
              "   'note',\n",
              "   'receivable',\n",
              "   'related',\n",
              "   'to',\n",
              "   'a',\n",
              "   'legacy',\n",
              "   'Jarden',\n",
              "   'investment',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'lower',\n",
              "   'capital',\n",
              "   'expenditures',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['The',\n",
              "   'change',\n",
              "   'in',\n",
              "   'financing',\n",
              "   'activities',\n",
              "   'for',\n",
              "   '2019',\n",
              "   'was',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'year-over-year',\n",
              "   'change',\n",
              "   'in',\n",
              "   'debt',\n",
              "   '(',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '2.2',\n",
              "   'billion',\n",
              "   ')',\n",
              "   'and',\n",
              "   'a',\n",
              "   'decrease',\n",
              "   'in',\n",
              "   'shares',\n",
              "   'repurchased',\n",
              "   '(',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '1.5',\n",
              "   'billion',\n",
              "   ')',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'payments',\n",
              "   'to',\n",
              "   'dissenting',\n",
              "   'shareholders',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '(',\n",
              "   'approximately',\n",
              "   '$',\n",
              "   '171',\n",
              "   'million',\n",
              "   ')',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'the',\n",
              "   'Jarden',\n",
              "   'acquisition',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['As',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'S',\n",
              "   '&',\n",
              "   'P',\n",
              "   'and',\n",
              "   'Moody',\n",
              "   \"'s\",\n",
              "   'downgrades',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'could',\n",
              "   'no',\n",
              "   'longer',\n",
              "   'borrow',\n",
              "   'from',\n",
              "   'the',\n",
              "   'commercial',\n",
              "   'paper',\n",
              "   'market',\n",
              "   'on',\n",
              "   'terms',\n",
              "   'it',\n",
              "   'deems',\n",
              "   'acceptable',\n",
              "   'or',\n",
              "   'favorable',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   'yield',\n",
              "   'on',\n",
              "   'the',\n",
              "   'total',\n",
              "   'loan',\n",
              "   'portfolio',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '4.33',\n",
              "   '%',\n",
              "   ',',\n",
              "   'down',\n",
              "   '66',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'period',\n",
              "   ',',\n",
              "   'reflecting',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'rate',\n",
              "   'decreases',\n",
              "   'and',\n",
              "   'deferred',\n",
              "   'interest',\n",
              "   'for',\n",
              "   'loans',\n",
              "   'granted',\n",
              "   'an',\n",
              "   'accommodation',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'COVID-19',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'purchase',\n",
              "   'accounting',\n",
              "   'accretion',\n",
              "   'from',\n",
              "   'merged',\n",
              "   'loans',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'was',\n",
              "   '$',\n",
              "   '2.3',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '615',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'period',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'the',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'reflects',\n",
              "   'the',\n",
              "   'significant',\n",
              "   'builds',\n",
              "   'to',\n",
              "   'the',\n",
              "   'allowance',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'in',\n",
              "   'the',\n",
              "   'first',\n",
              "   'and',\n",
              "   'second',\n",
              "   'quarters',\n",
              "   'of',\n",
              "   'the',\n",
              "   'year',\n",
              "   'due',\n",
              "   'to',\n",
              "   'increased',\n",
              "   'economic',\n",
              "   'stress',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'pandemic',\n",
              "   'and',\n",
              "   'specific',\n",
              "   'consideration',\n",
              "   'of',\n",
              "   'its',\n",
              "   'impact',\n",
              "   'on',\n",
              "   'certain',\n",
              "   'industries',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'uncertainty',\n",
              "   'related',\n",
              "   'to',\n",
              "   'performance',\n",
              "   'after',\n",
              "   'the',\n",
              "   'expiration',\n",
              "   'of',\n",
              "   'relief',\n",
              "   'packages',\n",
              "   'and',\n",
              "   'COVID-19',\n",
              "   ',',\n",
              "   'increased',\n",
              "   'loan',\n",
              "   'balances',\n",
              "   'arising',\n",
              "   'from',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'the',\n",
              "   'effect',\n",
              "   'of',\n",
              "   'applying',\n",
              "   'the',\n",
              "   'CECL',\n",
              "   'methodology',\n",
              "   'in',\n",
              "   'the',\n",
              "   'current',\n",
              "   'period',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'incurred',\n",
              "   'loss',\n",
              "   'methodology',\n",
              "   'in',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'period',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Residential',\n",
              "   'mortgage',\n",
              "   'banking',\n",
              "   'income',\n",
              "   'was',\n",
              "   'up',\n",
              "   'due',\n",
              "   'to',\n",
              "   'strong',\n",
              "   'production',\n",
              "   'and',\n",
              "   'refinance',\n",
              "   'activity',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'the',\n",
              "   'lower',\n",
              "   'rate',\n",
              "   'environment',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'lower',\n",
              "   'valuations',\n",
              "   'of',\n",
              "   'the',\n",
              "   'mortgage',\n",
              "   'servicing',\n",
              "   'rights',\n",
              "   'and',\n",
              "   'increased',\n",
              "   'amortization',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'prepayments',\n",
              "   'speeds',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Additionally',\n",
              "   ',',\n",
              "   'personnel',\n",
              "   'expenses',\n",
              "   'increased',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'completion',\n",
              "   'of',\n",
              "   'a',\n",
              "   'post-Merger',\n",
              "   'reevaluation',\n",
              "   'of',\n",
              "   'job',\n",
              "   'grades',\n",
              "   'that',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'additional',\n",
              "   'salaries',\n",
              "   ',',\n",
              "   'incentives',\n",
              "   'and',\n",
              "   'equity-based',\n",
              "   'compensation',\n",
              "   'expenses',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Noninterest',\n",
              "   'income',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.7',\n",
              "   'billion',\n",
              "   'due',\n",
              "   'primarily',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'higher',\n",
              "   'residential',\n",
              "   'mortgage',\n",
              "   'income',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'lower',\n",
              "   'rate',\n",
              "   'environment',\n",
              "   'driving',\n",
              "   'mortgage',\n",
              "   'production',\n",
              "   'through',\n",
              "   'refinance',\n",
              "   'activity',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'lower',\n",
              "   'residential',\n",
              "   'mortgage',\n",
              "   'servicing',\n",
              "   'income',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'higher',\n",
              "   'prepayment',\n",
              "   'and',\n",
              "   'an',\n",
              "   'MSR',\n",
              "   'fair',\n",
              "   'value',\n",
              "   'adjustment',\n",
              "   'in',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '491',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Consumer',\n",
              "   'Banking',\n",
              "   'and',\n",
              "   'Wealth',\n",
              "   'average',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'leases',\n",
              "   'held',\n",
              "   'for',\n",
              "   'investment',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '67.9',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '94.9',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Consumer',\n",
              "   'Banking',\n",
              "   'and',\n",
              "   'Wealth',\n",
              "   'average',\n",
              "   'total',\n",
              "   'deposits',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '119.5',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '120.4',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'COVID-19',\n",
              "   'stimulus',\n",
              "   'impacts',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Segment',\n",
              "   'net',\n",
              "   'interest',\n",
              "   'income',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '2.4',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Noninterest',\n",
              "   'income',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.3',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'other',\n",
              "   'increases',\n",
              "   'in',\n",
              "   'investment',\n",
              "   'banking',\n",
              "   'income',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'losses',\n",
              "   'in',\n",
              "   'trading',\n",
              "   'income',\n",
              "   'primarily',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'interest',\n",
              "   'rates',\n",
              "   'and',\n",
              "   'widening',\n",
              "   'of',\n",
              "   'credit',\n",
              "   'spreads',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.2',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'increased',\n",
              "   'economic',\n",
              "   'stress',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Noninterest',\n",
              "   'expense',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.9',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'operating',\n",
              "   'expenses',\n",
              "   'and',\n",
              "   'amortization',\n",
              "   'of',\n",
              "   'intangibles',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'in',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Corporate',\n",
              "   'and',\n",
              "   'Commercial',\n",
              "   'Banking',\n",
              "   'average',\n",
              "   'total',\n",
              "   'deposits',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '68.0',\n",
              "   'billion',\n",
              "   ',',\n",
              "   'or',\n",
              "   '102.7',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '2019',\n",
              "   'driven',\n",
              "   'primarily',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'COVID-19',\n",
              "   'stimulus',\n",
              "   'impacts',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Segment',\n",
              "   'net',\n",
              "   'interest',\n",
              "   'income',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '158',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'funding',\n",
              "   'charges',\n",
              "   'on',\n",
              "   'assets',\n",
              "   'to',\n",
              "   'other',\n",
              "   'segments',\n",
              "   'relative',\n",
              "   'to',\n",
              "   'the',\n",
              "   'funding',\n",
              "   'credit',\n",
              "   'provided',\n",
              "   'on',\n",
              "   'liabilities',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'allocated',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '27',\n",
              "   'million',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'unfunded',\n",
              "   'commitments',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Noninterest',\n",
              "   'expense',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '1.2',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'operating',\n",
              "   'expenses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'higher',\n",
              "   'merger-related',\n",
              "   'charges',\n",
              "   'and',\n",
              "   'incremental',\n",
              "   'operating',\n",
              "   'expenses',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'loss',\n",
              "   'on',\n",
              "   'early',\n",
              "   'extinguishment',\n",
              "   'of',\n",
              "   'long-term',\n",
              "   'debt',\n",
              "   ',',\n",
              "   'and',\n",
              "   'elevated',\n",
              "   'COVID-related',\n",
              "   'expenses',\n",
              "   'in',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['During',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   'sold',\n",
              "   'non-Agency',\n",
              "   'MBS',\n",
              "   ',',\n",
              "   'and',\n",
              "   'sold',\n",
              "   'and',\n",
              "   'reinvested',\n",
              "   'residential',\n",
              "   'Agency',\n",
              "   'MBS',\n",
              "   '.',\n",
              "   'These',\n",
              "   'sales',\n",
              "   'were',\n",
              "   'the',\n",
              "   'primary',\n",
              "   'drivers',\n",
              "   'for',\n",
              "   'the',\n",
              "   'gains',\n",
              "   'of',\n",
              "   '$',\n",
              "   '402',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020.As',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'approximately',\n",
              "   '1.9',\n",
              "   '%',\n",
              "   'of',\n",
              "   'the',\n",
              "   'securities',\n",
              "   'portfolio',\n",
              "   'was',\n",
              "   'variable',\n",
              "   'rate',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '3.6',\n",
              "   '%',\n",
              "   'as',\n",
              "   'of',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Consumer',\n",
              "   'loans',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '3.2',\n",
              "   'billion',\n",
              "   'during',\n",
              "   '2020',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'refinance',\n",
              "   'activity',\n",
              "   'resulting',\n",
              "   'in',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'residential',\n",
              "   'mortgages',\n",
              "   'and',\n",
              "   'residential',\n",
              "   'home',\n",
              "   'equity',\n",
              "   'and',\n",
              "   'direct',\n",
              "   'loans',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Credit',\n",
              "   'card',\n",
              "   'loans',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '780',\n",
              "   'million',\n",
              "   'during',\n",
              "   '2020',\n",
              "   'due',\n",
              "   'to',\n",
              "   'lower',\n",
              "   'business',\n",
              "   'and',\n",
              "   'consumer',\n",
              "   'spending',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'COVID-19',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['LHFS',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '2.3',\n",
              "   'billion',\n",
              "   'during',\n",
              "   '2020',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'sale',\n",
              "   'of',\n",
              "   'loans',\n",
              "   'that',\n",
              "   'had',\n",
              "   'been',\n",
              "   'placed',\n",
              "   'in',\n",
              "   'LHFS',\n",
              "   'after',\n",
              "   'the',\n",
              "   'close',\n",
              "   'of',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   'and',\n",
              "   'the',\n",
              "   'branch',\n",
              "   'divestiture',\n",
              "   'in',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'the',\n",
              "   'Merger',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'the',\n",
              "   'transfer',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.0',\n",
              "   'billion',\n",
              "   'to',\n",
              "   'LHFS',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'decision',\n",
              "   'to',\n",
              "   'exit',\n",
              "   'a',\n",
              "   'small',\n",
              "   'ticket',\n",
              "   'loan',\n",
              "   'and',\n",
              "   'lease',\n",
              "   'portfolio',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   ',',\n",
              "   'average',\n",
              "   'commercial',\n",
              "   'loans',\n",
              "   'were',\n",
              "   'impacted',\n",
              "   'by',\n",
              "   'the',\n",
              "   'transfer',\n",
              "   'of',\n",
              "   '$',\n",
              "   '1.0',\n",
              "   'billion',\n",
              "   'of',\n",
              "   'certain',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'leases',\n",
              "   'to',\n",
              "   'held',\n",
              "   'for',\n",
              "   'sale',\n",
              "   ',',\n",
              "   'which',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'the',\n",
              "   'average',\n",
              "   'balance',\n",
              "   'of',\n",
              "   '$',\n",
              "   '323',\n",
              "   'million',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'third',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O']},\n",
              " {'words': ['Average',\n",
              "   'consumer',\n",
              "   'loans',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '2.2',\n",
              "   'billion',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'seasonally',\n",
              "   'lower',\n",
              "   'loan',\n",
              "   'production',\n",
              "   'and',\n",
              "   'refinance',\n",
              "   'activity',\n",
              "   'resulting',\n",
              "   'in',\n",
              "   'a',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'residential',\n",
              "   'mortgages',\n",
              "   'and',\n",
              "   'residential',\n",
              "   'home',\n",
              "   'equity',\n",
              "   'and',\n",
              "   'direct',\n",
              "   'loans',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'remaining',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'nonperforming',\n",
              "   'loans',\n",
              "   'held',\n",
              "   'for',\n",
              "   'investment',\n",
              "   'is',\n",
              "   'primarily',\n",
              "   'in',\n",
              "   'commercial',\n",
              "   'and',\n",
              "   'industrial',\n",
              "   'loans',\n",
              "   'and',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'nonperforming',\n",
              "   'mortgage',\n",
              "   'loans',\n",
              "   'due',\n",
              "   'to',\n",
              "   'loans',\n",
              "   'exiting',\n",
              "   'certain',\n",
              "   'accommodation',\n",
              "   'programs',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'CARES',\n",
              "   'Act',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'connection',\n",
              "   'with',\n",
              "   'the',\n",
              "   'discontinuation',\n",
              "   'of',\n",
              "   'pool',\n",
              "   'level',\n",
              "   'accounting',\n",
              "   'for',\n",
              "   'PCI',\n",
              "   'loans',\n",
              "   ',',\n",
              "   'loans',\n",
              "   '90',\n",
              "   'days',\n",
              "   'or',\n",
              "   'more',\n",
              "   'past',\n",
              "   'due',\n",
              "   'and',\n",
              "   'still',\n",
              "   'accruing',\n",
              "   'decreased',\n",
              "   'as',\n",
              "   'loan-level',\n",
              "   'evaluations',\n",
              "   'resulted',\n",
              "   'in',\n",
              "   'certain',\n",
              "   'loans',\n",
              "   'being',\n",
              "   'placed',\n",
              "   'in',\n",
              "   'nonaccrual',\n",
              "   'status',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'remaining',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'the',\n",
              "   'allowance',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'primarily',\n",
              "   'reflects',\n",
              "   'deteriorated',\n",
              "   'economic',\n",
              "   'conditions',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Average',\n",
              "   'noninterest-bearing',\n",
              "   'and',\n",
              "   'interest',\n",
              "   'checking',\n",
              "   'deposit',\n",
              "   'growth',\n",
              "   'was',\n",
              "   'strong',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   'driven',\n",
              "   'by',\n",
              "   'anticipated',\n",
              "   'seasonal',\n",
              "   'inflows',\n",
              "   'in',\n",
              "   'addition',\n",
              "   'to',\n",
              "   'continued',\n",
              "   'growth',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'pandemic-related',\n",
              "   'client',\n",
              "   'behavior',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Average',\n",
              "   'time',\n",
              "   'deposits',\n",
              "   'decreased',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'maturity',\n",
              "   'of',\n",
              "   'wholesale',\n",
              "   'negotiable',\n",
              "   'certificates',\n",
              "   'of',\n",
              "   'deposit',\n",
              "   'and',\n",
              "   'higher-cost',\n",
              "   'personal',\n",
              "   'and',\n",
              "   'business',\n",
              "   'accounts',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'yield',\n",
              "   'on',\n",
              "   'the',\n",
              "   'total',\n",
              "   'loan',\n",
              "   'portfolio',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   'was',\n",
              "   '4.12',\n",
              "   '%',\n",
              "   ',',\n",
              "   'down',\n",
              "   '79',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   ',',\n",
              "   'reflecting',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'rate',\n",
              "   'decreases',\n",
              "   ',',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'purchase',\n",
              "   'accounting',\n",
              "   'accretion',\n",
              "   'from',\n",
              "   'merged',\n",
              "   'loans',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'average',\n",
              "   'rate',\n",
              "   'on',\n",
              "   'short-term',\n",
              "   'borrowings',\n",
              "   'was',\n",
              "   '0.77',\n",
              "   '%',\n",
              "   ',',\n",
              "   'down',\n",
              "   '138',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.',\n",
              "   'The',\n",
              "   'average',\n",
              "   'rate',\n",
              "   'on',\n",
              "   'long-term',\n",
              "   'debt',\n",
              "   'was',\n",
              "   '1.64',\n",
              "   '%',\n",
              "   ',',\n",
              "   'down',\n",
              "   '128',\n",
              "   'basis',\n",
              "   'points',\n",
              "   'compared',\n",
              "   'to',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.',\n",
              "   'The',\n",
              "   'lower',\n",
              "   'rates',\n",
              "   'on',\n",
              "   'interest-bearing',\n",
              "   'liabilities',\n",
              "   'reflect',\n",
              "   'the',\n",
              "   'lower',\n",
              "   'rate',\n",
              "   'environment',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes',\n",
              "   'was',\n",
              "   '$',\n",
              "   '311',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '153',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.',\n",
              "   'This',\n",
              "   'produced',\n",
              "   'an',\n",
              "   'effective',\n",
              "   'tax',\n",
              "   'rate',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   'of',\n",
              "   '19.0',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '17.4',\n",
              "   '%',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'income',\n",
              "   'taxes',\n",
              "   'was',\n",
              "   '$',\n",
              "   '311',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '$',\n",
              "   '153',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.',\n",
              "   'This',\n",
              "   'produced',\n",
              "   'an',\n",
              "   'effective',\n",
              "   'tax',\n",
              "   'rate',\n",
              "   'for',\n",
              "   'the',\n",
              "   'fourth',\n",
              "   'quarter',\n",
              "   'of',\n",
              "   '2020',\n",
              "   'of',\n",
              "   '19.0',\n",
              "   '%',\n",
              "   ',',\n",
              "   'compared',\n",
              "   'to',\n",
              "   '17.4',\n",
              "   '%',\n",
              "   'for',\n",
              "   'the',\n",
              "   'earlier',\n",
              "   'quarter',\n",
              "   '.'],\n",
              "  'ner': ['C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E']},\n",
              " {'words': ['Food',\n",
              "   'and',\n",
              "   'beverage',\n",
              "   'revenues',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '489.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'decreased',\n",
              "   'covers',\n",
              "   'at',\n",
              "   'our',\n",
              "   'restaurants',\n",
              "   'and',\n",
              "   'the',\n",
              "   'reduction',\n",
              "   'of',\n",
              "   'nightlife',\n",
              "   'offerings',\n",
              "   'at',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'adverse',\n",
              "   'effects',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Casino',\n",
              "   'expenses',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '1.03',\n",
              "   'billion',\n",
              "   ',',\n",
              "   '$',\n",
              "   '797.8',\n",
              "   'million',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '49.8',\n",
              "   'million',\n",
              "   'at',\n",
              "   'Wynn',\n",
              "   'Palace',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Macau',\n",
              "   ',',\n",
              "   'and',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'These',\n",
              "   'decreases',\n",
              "   'were',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'reductions',\n",
              "   'in',\n",
              "   'gaming',\n",
              "   'tax',\n",
              "   'expense',\n",
              "   'commensurate',\n",
              "   'with',\n",
              "   'the',\n",
              "   'declines',\n",
              "   'in',\n",
              "   'casino',\n",
              "   'revenues',\n",
              "   'at',\n",
              "   'each',\n",
              "   'property',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'the',\n",
              "   'effects',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'lower',\n",
              "   'payroll',\n",
              "   'and',\n",
              "   'other',\n",
              "   'operating',\n",
              "   'costs',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['These',\n",
              "   'decreases',\n",
              "   'were',\n",
              "   'partially',\n",
              "   'offset',\n",
              "   'by',\n",
              "   'increased',\n",
              "   'casino',\n",
              "   'expenses',\n",
              "   'of',\n",
              "   '$',\n",
              "   '22.3',\n",
              "   'million',\n",
              "   'from',\n",
              "   'Encore',\n",
              "   'Boston',\n",
              "   'Harbor',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'opening',\n",
              "   'of',\n",
              "   'the',\n",
              "   'property',\n",
              "   'in',\n",
              "   'June',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Food',\n",
              "   'and',\n",
              "   'beverage',\n",
              "   'expenses',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '214.8',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '48.8',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '22.8',\n",
              "   'million',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '11.2',\n",
              "   'million',\n",
              "   'at',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Palace',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Macau',\n",
              "   ',',\n",
              "   'and',\n",
              "   'Encore',\n",
              "   'Boston',\n",
              "   'Harbor',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'decreases',\n",
              "   'were',\n",
              "   'primarily',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'lower',\n",
              "   'operating',\n",
              "   'costs',\n",
              "   'related',\n",
              "   'to',\n",
              "   'the',\n",
              "   'declines',\n",
              "   'in',\n",
              "   'food',\n",
              "   'and',\n",
              "   'beverage',\n",
              "   'revenues',\n",
              "   'at',\n",
              "   'each',\n",
              "   'property',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'lower',\n",
              "   'nightlife',\n",
              "   'entertainment',\n",
              "   'costs',\n",
              "   'at',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   'resulting',\n",
              "   'from',\n",
              "   'the',\n",
              "   'effects',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['General',\n",
              "   'and',\n",
              "   'administrative',\n",
              "   'expenses',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '54.5',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '31.3',\n",
              "   'million',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '27.6',\n",
              "   'million',\n",
              "   'at',\n",
              "   'Wynn',\n",
              "   'Palace',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Macau',\n",
              "   ',',\n",
              "   'and',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'These',\n",
              "   'decreases',\n",
              "   'were',\n",
              "   'primarily',\n",
              "   'attributable',\n",
              "   'to',\n",
              "   'the',\n",
              "   'effects',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['In',\n",
              "   'addition',\n",
              "   ',',\n",
              "   'corporate',\n",
              "   'and',\n",
              "   'other',\n",
              "   'general',\n",
              "   'and',\n",
              "   'administrative',\n",
              "   'expenses',\n",
              "   'decreased',\n",
              "   '$',\n",
              "   '91.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'a',\n",
              "   'credit',\n",
              "   'of',\n",
              "   '$',\n",
              "   '30.2',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'net',\n",
              "   'proceeds',\n",
              "   'of',\n",
              "   'a',\n",
              "   'derivative',\n",
              "   'action',\n",
              "   'settlement',\n",
              "   'recognized',\n",
              "   'during',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'and',\n",
              "   'a',\n",
              "   'fine',\n",
              "   'of',\n",
              "   '$',\n",
              "   '35.0',\n",
              "   'million',\n",
              "   'assessed',\n",
              "   'by',\n",
              "   'the',\n",
              "   'Massachusetts',\n",
              "   'Gaming',\n",
              "   'Commission',\n",
              "   'which',\n",
              "   'was',\n",
              "   'incurred',\n",
              "   'in',\n",
              "   '2019',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['The',\n",
              "   'provision',\n",
              "   'for',\n",
              "   'credit',\n",
              "   'losses',\n",
              "   'increased',\n",
              "   '$',\n",
              "   '15.1',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '14.8',\n",
              "   'million',\n",
              "   ',',\n",
              "   '$',\n",
              "   '10.2',\n",
              "   'million',\n",
              "   ',',\n",
              "   'and',\n",
              "   '$',\n",
              "   '2.4',\n",
              "   'million',\n",
              "   'at',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Palace',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Macau',\n",
              "   ',',\n",
              "   'and',\n",
              "   'Encore',\n",
              "   'Boston',\n",
              "   'Harbor',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   '.',\n",
              "   'The',\n",
              "   'increases',\n",
              "   'were',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'the',\n",
              "   'impact',\n",
              "   'of',\n",
              "   'historical',\n",
              "   'collection',\n",
              "   'patterns',\n",
              "   'and',\n",
              "   'expectations',\n",
              "   'of',\n",
              "   'current',\n",
              "   'and',\n",
              "   'future',\n",
              "   'collection',\n",
              "   'trends',\n",
              "   'in',\n",
              "   'light',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   ',',\n",
              "   'as',\n",
              "   'well',\n",
              "   'as',\n",
              "   'the',\n",
              "   'specific',\n",
              "   'review',\n",
              "   'of',\n",
              "   'customer',\n",
              "   'accounts',\n",
              "   ',',\n",
              "   'on',\n",
              "   'our',\n",
              "   'estimated',\n",
              "   'credit',\n",
              "   'loss',\n",
              "   'for',\n",
              "   'the',\n",
              "   'respective',\n",
              "   'periods',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['Depreciation',\n",
              "   'and',\n",
              "   'amortization',\n",
              "   'increased',\n",
              "   'primarily',\n",
              "   'due',\n",
              "   'to',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'depreciation',\n",
              "   'expense',\n",
              "   'of',\n",
              "   '$',\n",
              "   '72.5',\n",
              "   'million',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'opening',\n",
              "   'of',\n",
              "   'Encore',\n",
              "   'Boston',\n",
              "   'Harbor',\n",
              "   'in',\n",
              "   'June',\n",
              "   '2019',\n",
              "   'and',\n",
              "   'an',\n",
              "   'increase',\n",
              "   'of',\n",
              "   '$',\n",
              "   '18.8',\n",
              "   'million',\n",
              "   'at',\n",
              "   'our',\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   'associated',\n",
              "   'with',\n",
              "   'the',\n",
              "   'opening',\n",
              "   'of',\n",
              "   'the',\n",
              "   'meeting',\n",
              "   'and',\n",
              "   'convention',\n",
              "   'expansion',\n",
              "   'in',\n",
              "   'February',\n",
              "   '2020',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']},\n",
              " {'words': ['We',\n",
              "   'recorded',\n",
              "   'a',\n",
              "   'loss',\n",
              "   'of',\n",
              "   '$',\n",
              "   '13.1',\n",
              "   'million',\n",
              "   'and',\n",
              "   '$',\n",
              "   '3.2',\n",
              "   'million',\n",
              "   'for',\n",
              "   'the',\n",
              "   'years',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   'and',\n",
              "   '2019',\n",
              "   ',',\n",
              "   'respectively',\n",
              "   ',',\n",
              "   'from',\n",
              "   'change',\n",
              "   'in',\n",
              "   'the',\n",
              "   'fair',\n",
              "   'value',\n",
              "   'of',\n",
              "   'an',\n",
              "   'interest',\n",
              "   'rate',\n",
              "   'collar',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'O']},\n",
              " {'words': ['During',\n",
              "   'the',\n",
              "   'year',\n",
              "   'ended',\n",
              "   'December',\n",
              "   '31',\n",
              "   ',',\n",
              "   '2020',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Palace',\n",
              "   ',',\n",
              "   'Wynn',\n",
              "   'Macau',\n",
              "   ',',\n",
              "   'the',\n",
              "   'Company',\n",
              "   \"'s\",\n",
              "   'Las',\n",
              "   'Vegas',\n",
              "   'Operations',\n",
              "   ',',\n",
              "   'and',\n",
              "   'Encore',\n",
              "   'Boston',\n",
              "   'Harbor',\n",
              "   'each',\n",
              "   'experienced',\n",
              "   'a',\n",
              "   'significant',\n",
              "   'decline',\n",
              "   'in',\n",
              "   'revenues',\n",
              "   ',',\n",
              "   'operating',\n",
              "   'income',\n",
              "   ',',\n",
              "   'and',\n",
              "   'cash',\n",
              "   'provided',\n",
              "   'by',\n",
              "   'operations',\n",
              "   'as',\n",
              "   'a',\n",
              "   'result',\n",
              "   'of',\n",
              "   'the',\n",
              "   'COVID-19',\n",
              "   'pandemic',\n",
              "   'as',\n",
              "   'noted',\n",
              "   'in',\n",
              "   'Note',\n",
              "   '1',\n",
              "   ',',\n",
              "   'Organization',\n",
              "   'and',\n",
              "   'Business',\n",
              "   '.'],\n",
              "  'ner': ['E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'E',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C',\n",
              "   'C']}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "KLGj-pXQmG52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DE4b2EMGoQbK",
        "outputId": "f96697ca-afc0-4d72-bb6e-43c5171008d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [During, 2020, ,, the, Company, also, issued, ...   \n",
              "1    [The, significant, increases, in, earnings, as...   \n",
              "2    [The, provision, for, credit, losses, was, $, ...   \n",
              "3    [Higher, net, charge-offs, also, contributed, ...   \n",
              "4    [In, addition, to, the, impacts, from, the, Me...   \n",
              "..                                                 ...   \n",
              "260  [In, addition, ,, corporate, and, other, gener...   \n",
              "261  [The, provision, for, credit, losses, increase...   \n",
              "262  [Depreciation, and, amortization, increased, p...   \n",
              "263  [We, recorded, a, loss, of, $, 13.1, million, ...   \n",
              "264  [During, the, year, ended, December, 31, ,, 20...   \n",
              "\n",
              "                                                   ner  \n",
              "0    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "1    [E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...  \n",
              "2    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "3    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "4    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "..                                                 ...  \n",
              "260  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...  \n",
              "261  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "262  [E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...  \n",
              "263  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "264  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...  \n",
              "\n",
              "[265 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e72aa163-e5a0-4766-af4b-18fff288ef2f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[During, 2020, ,, the, Company, also, issued, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, significant, increases, in, earnings, as...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, provision, for, credit, losses, was, $, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Higher, net, charge-offs, also, contributed, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[In, addition, to, the, impacts, from, the, Me...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>[In, addition, ,, corporate, and, other, gener...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>[The, provision, for, credit, losses, increase...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>[Depreciation, and, amortization, increased, p...</td>\n",
              "      <td>[E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>[We, recorded, a, loss, of, $, 13.1, million, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>[During, the, year, ended, December, 31, ,, 20...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>265 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e72aa163-e5a0-4766-af4b-18fff288ef2f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e72aa163-e5a0-4766-af4b-18fff288ef2f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e72aa163-e5a0-4766-af4b-18fff288ef2f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d9f20131-be68-48b0-a41a-1a2072e19288\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9f20131-be68-48b0-a41a-1a2072e19288')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d9f20131-be68-48b0-a41a-1a2072e19288 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_952adcc4-589c-441c-8dfe-b59f1ffa5f58\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_952adcc4-589c-441c-8dfe-b59f1ffa5f58 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "6uzssyHcoQfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "_Y103sJBqaEW",
        "outputId": "67d328cb-e1da-41fe-95ca-ccb563b6cf19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [During, 2020, ,, the, Company, also, issued, ...   \n",
              "1    [The, significant, increases, in, earnings, as...   \n",
              "2    [The, provision, for, credit, losses, was, $, ...   \n",
              "3    [Higher, net, charge-offs, also, contributed, ...   \n",
              "4    [In, addition, to, the, impacts, from, the, Me...   \n",
              "..                                                 ...   \n",
              "260  [In, addition, ,, corporate, and, other, gener...   \n",
              "261  [The, provision, for, credit, losses, increase...   \n",
              "262  [Depreciation, and, amortization, increased, p...   \n",
              "263  [We, recorded, a, loss, of, $, 13.1, million, ...   \n",
              "264  [During, the, year, ended, December, 31, ,, 20...   \n",
              "\n",
              "                                                   ner  \\\n",
              "0    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "1    [E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...   \n",
              "2    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "3    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "4    [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "..                                                 ...   \n",
              "260  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...   \n",
              "261  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "262  [E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...   \n",
              "263  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "264  [E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...   \n",
              "\n",
              "                                            liststring  \n",
              "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
              "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
              "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
              "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
              "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
              "..                                                 ...  \n",
              "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
              "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
              "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
              "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
              "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
              "\n",
              "[265 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8971dcd0-4183-4bf2-be56-198dbbfe5cb9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "      <th>liststring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[During, 2020, ,, the, Company, also, issued, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, significant, increases, in, earnings, as...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, O, O, O, O, C, C, C, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, provision, for, credit, losses, was, $, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Higher, net, charge-offs, also, contributed, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[In, addition, to, the, impacts, from, the, Me...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>[In, addition, ,, corporate, and, other, gener...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, O, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>[The, provision, for, credit, losses, increase...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>[Depreciation, and, amortization, increased, p...</td>\n",
              "      <td>[E, E, E, E, O, O, O, C, C, C, C, C, C, C, C, ...</td>\n",
              "      <td>E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>[We, recorded, a, loss, of, $, 13.1, million, ...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>[During, the, year, ended, December, 31, ,, 20...</td>\n",
              "      <td>[E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, ...</td>\n",
              "      <td>E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>265 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8971dcd0-4183-4bf2-be56-198dbbfe5cb9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8971dcd0-4183-4bf2-be56-198dbbfe5cb9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8971dcd0-4183-4bf2-be56-198dbbfe5cb9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a11a1ac8-c840-4d4d-a07e-859268af36a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a11a1ac8-c840-4d4d-a07e-859268af36a7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a11a1ac8-c840-4d4d-a07e-859268af36a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_73ab0039-38ce-4a9c-b363-7e0c8e490a6a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_73ab0039-38ce-4a9c-b363-7e0c8e490a6a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "3pMutAjh-KK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-UC7_icoQmG",
        "outputId": "6812c200-dd95-4fe6-994c-fd460e7d1957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-14 18:01:27.043520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 18:01:27.043573: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 18:01:27.043622: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 18:01:28.135033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 18:01:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 18:01:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_18-01-30_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 18:01:32 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 22712.84it/s]\n",
            "Downloading took 0.0 min\n",
            "11/14/2023 18:01:32 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/14/2023 18:01:32 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2193.29it/s]\n",
            "Generating train split\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 5662 examples [00:00, 149345.34 examples/s]\n",
            "Generating validation split\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 265 examples [00:00, 68168.69 examples/s]\n",
            "Generating test split\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 265 examples [00:00, 69355.46 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/14/2023 18:01:32 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/14/2023 18:01:32 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading (…)lve/main/config.json: 100% 414/414 [00:00<00:00, 1.92MB/s]\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 18:01:32,948 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 18:01:32,952 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 18:01:33,206 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 18:01:33,469 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 18:01:33,470 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 30.4MB/s]\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 18:01:35,035 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 18:01:35,035 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 18:01:35,036 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 18:01:35,036 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 18:01:35,036 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 18:01:35,036 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 18:01:35,037 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 18:01:35,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 18:01:35,069 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 665M/665M [00:01<00:00, 356MB/s]\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 18:01:37,865 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 18:01:41,041 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 18:01:41,041 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/5662 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 18:01:41,107 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 18:01:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Running tokenizer on train dataset: 100% 5662/5662 [00:01<00:00, 3997.95 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/265 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-723c5e0a5324e0da.arrow\n",
            "11/14/2023 18:01:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-723c5e0a5324e0da.arrow\n",
            "Running tokenizer on validation dataset: 100% 265/265 [00:00<00:00, 4971.15 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/265 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e4cfa5c5eb79fbb5.arrow\n",
            "11/14/2023 18:01:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e4cfa5c5eb79fbb5.arrow\n",
            "Running tokenizer on prediction dataset: 100% 265/265 [00:00<00:00, 5020.44 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 24.9MB/s]\n",
            "[INFO|trainer.py:738] 2023-11-14 18:01:50,658 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 18:01:50,675 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 18:01:50,675 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 18:01:50,675 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 18:01:50,675 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 18:01:50,675 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 18:01:50,675 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 18:01:50,675 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 18:01:50,676 >>   Number of trainable parameters = 332,532,739\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 18:01:50,691 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.3594, 'learning_rate': 4.764595103578154e-05, 'epoch': 1.41}\n",
            "  5% 500/10620 [02:37<52:55,  3.19it/s][INFO|trainer.py:2883] 2023-11-14 18:04:27,849 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:04:27,850 >> Configuration saved in /content/test-ner-2022/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:04:30,142 >> Model weights saved in /content/test-ner-2022/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:04:30,143 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:04:30,143 >> Special tokens file saved in /content/test-ner-2022/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.1247, 'learning_rate': 4.5291902071563094e-05, 'epoch': 2.82}\n",
            "  9% 1000/10620 [05:27<58:03,  2.76it/s][INFO|trainer.py:2883] 2023-11-14 18:07:17,769 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:07:17,770 >> Configuration saved in /content/test-ner-2022/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:07:20,115 >> Model weights saved in /content/test-ner-2022/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:07:20,116 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:07:20,116 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.0546, 'learning_rate': 4.2937853107344634e-05, 'epoch': 4.24}\n",
            " 14% 1500/10620 [08:15<46:07,  3.29it/s][INFO|trainer.py:2883] 2023-11-14 18:10:06,357 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:10:06,358 >> Configuration saved in /content/test-ner-2022/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:10:08,691 >> Model weights saved in /content/test-ner-2022/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:10:08,692 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:10:08,693 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.0342, 'learning_rate': 4.058380414312618e-05, 'epoch': 5.65}\n",
            " 19% 2000/10620 [11:02<48:48,  2.94it/s][INFO|trainer.py:2883] 2023-11-14 18:12:52,891 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-2000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:12:52,892 >> Configuration saved in /content/test-ner-2022/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:12:55,172 >> Model weights saved in /content/test-ner-2022/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:12:55,173 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:12:55,174 >> Special tokens file saved in /content/test-ner-2022/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.0301, 'learning_rate': 3.8229755178907726e-05, 'epoch': 7.06}\n",
            " 24% 2500/10620 [13:52<56:02,  2.42it/s][INFO|trainer.py:2883] 2023-11-14 18:15:43,538 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-2500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:15:43,539 >> Configuration saved in /content/test-ner-2022/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:15:45,872 >> Model weights saved in /content/test-ner-2022/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:15:45,873 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:15:45,873 >> Special tokens file saved in /content/test-ner-2022/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.0209, 'learning_rate': 3.587570621468927e-05, 'epoch': 8.47}\n",
            " 28% 3000/10620 [16:41<36:57,  3.44it/s][INFO|trainer.py:2883] 2023-11-14 18:18:32,181 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-3000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:18:32,182 >> Configuration saved in /content/test-ner-2022/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:18:34,475 >> Model weights saved in /content/test-ner-2022/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:18:34,477 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:18:34,477 >> Special tokens file saved in /content/test-ner-2022/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.0147, 'learning_rate': 3.352165725047081e-05, 'epoch': 9.89}\n",
            " 33% 3500/10620 [19:28<32:05,  3.70it/s][INFO|trainer.py:2883] 2023-11-14 18:21:19,188 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-3500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:21:19,189 >> Configuration saved in /content/test-ner-2022/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:21:21,453 >> Model weights saved in /content/test-ner-2022/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:21:21,454 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:21:21,454 >> Special tokens file saved in /content/test-ner-2022/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.0117, 'learning_rate': 3.116760828625235e-05, 'epoch': 11.3}\n",
            " 38% 4000/10620 [22:15<32:46,  3.37it/s][INFO|trainer.py:2883] 2023-11-14 18:24:06,651 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-4000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:24:06,652 >> Configuration saved in /content/test-ner-2022/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:24:08,993 >> Model weights saved in /content/test-ner-2022/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:24:08,994 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:24:08,995 >> Special tokens file saved in /content/test-ner-2022/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.0097, 'learning_rate': 2.88135593220339e-05, 'epoch': 12.71}\n",
            " 42% 4500/10620 [25:03<32:43,  3.12it/s][INFO|trainer.py:2883] 2023-11-14 18:26:54,302 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-4500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:26:54,303 >> Configuration saved in /content/test-ner-2022/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:26:56,612 >> Model weights saved in /content/test-ner-2022/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:26:56,613 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:26:56,613 >> Special tokens file saved in /content/test-ner-2022/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.0114, 'learning_rate': 2.6459510357815442e-05, 'epoch': 14.12}\n",
            " 47% 5000/10620 [27:51<28:45,  3.26it/s][INFO|trainer.py:2883] 2023-11-14 18:29:42,656 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-5000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:29:42,657 >> Configuration saved in /content/test-ner-2022/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:29:44,952 >> Model weights saved in /content/test-ner-2022/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:29:44,953 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:29:44,953 >> Special tokens file saved in /content/test-ner-2022/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 0.0079, 'learning_rate': 2.4105461393596988e-05, 'epoch': 15.54}\n",
            " 52% 5500/10620 [30:40<25:38,  3.33it/s][INFO|trainer.py:2883] 2023-11-14 18:32:30,836 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-5500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:32:30,837 >> Configuration saved in /content/test-ner-2022/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:32:32,988 >> Model weights saved in /content/test-ner-2022/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:32:32,990 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:32:32,990 >> Special tokens file saved in /content/test-ner-2022/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 0.0079, 'learning_rate': 2.175141242937853e-05, 'epoch': 16.95}\n",
            " 56% 6000/10620 [33:25<24:42,  3.12it/s][INFO|trainer.py:2883] 2023-11-14 18:35:16,352 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-6000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:35:16,353 >> Configuration saved in /content/test-ner-2022/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:35:18,639 >> Model weights saved in /content/test-ner-2022/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:35:18,640 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:35:18,640 >> Special tokens file saved in /content/test-ner-2022/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 0.0051, 'learning_rate': 1.9397363465160076e-05, 'epoch': 18.36}\n",
            " 61% 6500/10620 [36:12<20:01,  3.43it/s][INFO|trainer.py:2883] 2023-11-14 18:38:02,875 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-6500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:38:02,876 >> Configuration saved in /content/test-ner-2022/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:38:05,152 >> Model weights saved in /content/test-ner-2022/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:38:05,153 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:38:05,153 >> Special tokens file saved in /content/test-ner-2022/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 0.0013, 'learning_rate': 1.704331450094162e-05, 'epoch': 19.77}\n",
            " 66% 7000/10620 [38:59<21:00,  2.87it/s][INFO|trainer.py:2883] 2023-11-14 18:40:50,578 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-7000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:40:50,579 >> Configuration saved in /content/test-ner-2022/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:40:52,908 >> Model weights saved in /content/test-ner-2022/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:40:52,909 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:40:52,909 >> Special tokens file saved in /content/test-ner-2022/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 0.0035, 'learning_rate': 1.4689265536723165e-05, 'epoch': 21.19}\n",
            " 71% 7500/10620 [41:48<14:47,  3.52it/s][INFO|trainer.py:2883] 2023-11-14 18:43:39,605 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-7500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:43:39,606 >> Configuration saved in /content/test-ner-2022/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:43:41,860 >> Model weights saved in /content/test-ner-2022/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:43:41,861 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:43:41,861 >> Special tokens file saved in /content/test-ner-2022/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 0.0022, 'learning_rate': 1.2335216572504708e-05, 'epoch': 22.6}\n",
            " 75% 8000/10620 [44:37<15:29,  2.82it/s][INFO|trainer.py:2883] 2023-11-14 18:46:27,943 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-8000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:46:27,944 >> Configuration saved in /content/test-ner-2022/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:46:30,219 >> Model weights saved in /content/test-ner-2022/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:46:30,220 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:46:30,221 >> Special tokens file saved in /content/test-ner-2022/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 0.0022, 'learning_rate': 9.981167608286254e-06, 'epoch': 24.01}\n",
            " 80% 8500/10620 [47:25<10:31,  3.35it/s][INFO|trainer.py:2883] 2023-11-14 18:49:16,401 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-8500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:49:16,402 >> Configuration saved in /content/test-ner-2022/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:49:18,692 >> Model weights saved in /content/test-ner-2022/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:49:18,693 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:49:18,693 >> Special tokens file saved in /content/test-ner-2022/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 0.0016, 'learning_rate': 7.627118644067798e-06, 'epoch': 25.42}\n",
            " 85% 9000/10620 [50:15<11:02,  2.45it/s][INFO|trainer.py:2883] 2023-11-14 18:52:06,216 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-9000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:52:06,217 >> Configuration saved in /content/test-ner-2022/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:52:08,461 >> Model weights saved in /content/test-ner-2022/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:52:08,462 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:52:08,462 >> Special tokens file saved in /content/test-ner-2022/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 0.0008, 'learning_rate': 5.273069679849341e-06, 'epoch': 26.84}\n",
            " 89% 9500/10620 [53:03<06:37,  2.82it/s][INFO|trainer.py:2883] 2023-11-14 18:54:53,830 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-9500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:54:53,831 >> Configuration saved in /content/test-ner-2022/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:54:56,005 >> Model weights saved in /content/test-ner-2022/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:54:56,006 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:54:56,006 >> Special tokens file saved in /content/test-ner-2022/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 0.0011, 'learning_rate': 2.919020715630885e-06, 'epoch': 28.25}\n",
            " 94% 10000/10620 [55:49<02:56,  3.51it/s][INFO|trainer.py:2883] 2023-11-14 18:57:40,596 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-10000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 18:57:40,597 >> Configuration saved in /content/test-ner-2022/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 18:57:42,904 >> Model weights saved in /content/test-ner-2022/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 18:57:42,905 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 18:57:42,906 >> Special tokens file saved in /content/test-ner-2022/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 0.0008, 'learning_rate': 5.649717514124295e-07, 'epoch': 29.66}\n",
            " 99% 10500/10620 [58:41<00:34,  3.47it/s][INFO|trainer.py:2883] 2023-11-14 19:00:32,666 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-10500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:00:32,667 >> Configuration saved in /content/test-ner-2022/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:00:34,859 >> Model weights saved in /content/test-ner-2022/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:00:34,860 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:00:34,860 >> Special tokens file saved in /content/test-ner-2022/checkpoint-10500/special_tokens_map.json\n",
            "100% 10620/10620 [59:34<00:00,  3.02it/s][INFO|trainer.py:1956] 2023-11-14 19:01:25,082 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3574.4176, 'train_samples_per_second': 47.521, 'train_steps_per_second': 2.971, 'train_loss': 0.03323743462618686, 'epoch': 30.0}\n",
            "100% 10620/10620 [59:34<00:00,  2.97it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:01:25,096 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:01:25,097 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:01:27,265 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:01:27,266 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:01:27,266 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0332\n",
            "  train_runtime            = 0:59:34.41\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =     47.521\n",
            "  train_steps_per_second   =      2.971\n",
            "11/14/2023 19:01:27 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:01:27,280 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:01:27,307 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:01:27,307 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:01:27,307 >>   Batch size = 8\n",
            " 94% 32/34 [00:01<00:00, 24.55it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 17.96it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9583\n",
            "  eval_f1                 =     0.9572\n",
            "  eval_loss               =     0.4066\n",
            "  eval_precision          =     0.9589\n",
            "  eval_recall             =     0.9555\n",
            "  eval_runtime            = 0:00:02.09\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    126.242\n",
            "  eval_steps_per_second   =     16.197\n",
            "11/14/2023 19:01:29 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:01:29,408 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:01:29,410 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:01:29,410 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:01:29,410 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.62it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9583\n",
            "  predict_f1                 =     0.9572\n",
            "  predict_loss               =     0.4066\n",
            "  predict_precision          =     0.9589\n",
            "  predict_recall             =     0.9555\n",
            "  predict_runtime            = 0:00:01.88\n",
            "  predict_samples_per_second =    140.652\n",
            "  predict_steps_per_second   =     18.046\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:01:31,646 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9589014028848054}, {'name': 'Recall', 'type': 'recall', 'value': 0.9555030517818468}, {'name': 'F1', 'type': 'f1', 'value': 0.957199211045365}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9583231781623203}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:01:39.054047: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:01:39.054117: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:01:39.054178: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:01:40.380048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:01:43 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:01:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-01-43_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:01:43 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:01:44 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:01:44 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:01:44 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:01:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:01:44 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:01:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:01:45,257 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:01:45,266 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:01:45,534 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:01:45,786 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:01:45,787 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:01:46,297 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:01:46,297 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:01:46,297 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:01:46,297 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:01:46,297 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:01:46,297 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:01:46,298 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:01:46,329 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:01:46,330 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:01:46,431 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:01:52,029 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:01:52,029 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:01:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/265 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 19:01:52,102 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:01:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Running tokenizer on validation dataset: 100% 265/265 [00:00<00:00, 3031.38 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e4cfa5c5eb79fbb5.arrow\n",
            "11/14/2023 19:01:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e4cfa5c5eb79fbb5.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:01:59,404 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:02:00,023 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:02:02,226 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:02:02,226 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:02:02,226 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:02:02,226 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:02:02,226 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:02:02,226 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:02:02,226 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:02:02,228 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:02:02,228 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:02:02,228 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:02:02,228 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:02:02,228 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:02:02,266 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 366.08it/s][INFO|trainer.py:1956] 2023-11-14 19:02:40,329 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.1126, 'train_samples_per_second': 4456.793, 'train_steps_per_second': 278.648, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 278.66it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:02:40,343 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:02:40,344 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:02:42,859 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:02:42,860 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:02:42,860 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.11\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4456.793\n",
            "  train_steps_per_second   =    278.648\n",
            "11/14/2023 19:02:42 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:02:42,874 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:02:42,876 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:02:42,876 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:02:42,876 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 25.24it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.49it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.90\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    139.174\n",
            "  eval_steps_per_second   =     17.856\n",
            "11/14/2023 19:02:44 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:02:44,783 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:02:44,785 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:02:44,785 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:02:44,785 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.54it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.88\n",
            "  predict_samples_per_second =    140.354\n",
            "  predict_steps_per_second   =     18.008\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:02:46,946 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:02:51.538720: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:02:51.538771: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:02:51.538816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:02:52.662410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:02:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:02:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-02-54_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:02:55 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:02:56 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:02:56 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:02:56 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:02:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:02:56 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:02:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:02:56,827 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:02:56,831 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:02:57,091 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:02:57,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:02:57,362 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:02:57,876 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:02:57,876 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:02:57,876 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:02:57,876 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:02:57,876 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:02:57,876 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:02:57,877 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:02:57,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:02:57,908 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:02:57,941 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:03:01,060 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:03:01,061 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:03:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:03:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/265 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 19:03:01,141 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:03:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "Running tokenizer on prediction dataset: 100% 265/265 [00:00<00:00, 4319.32 examples/s]\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:03:04,817 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:03:05,496 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:03:07,809 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:03:07,809 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:03:07,809 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:03:07,809 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:03:07,809 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:03:07,809 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:03:07,809 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:03:07,811 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:03:07,811 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:03:07,811 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:03:07,811 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:03:07,811 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:03:07,850 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 10607/10620 [00:34<00:00, 135.22it/s][INFO|trainer.py:1956] 2023-11-14 19:03:46,111 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.3117, 'train_samples_per_second': 4433.627, 'train_steps_per_second': 277.2, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.22it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:03:46,125 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:03:46,126 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:03:48,764 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:03:48,765 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:03:48,765 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.31\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4433.627\n",
            "  train_steps_per_second   =      277.2\n",
            "11/14/2023 19:03:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:03:48,779 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:03:48,781 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:03:48,781 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:03:48,781 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 25.27it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.29it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.92\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    137.658\n",
            "  eval_steps_per_second   =     17.662\n",
            "11/14/2023 19:03:50 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:03:50,709 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:03:50,711 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:03:50,711 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:03:50,711 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.57it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.88\n",
            "  predict_samples_per_second =    140.555\n",
            "  predict_steps_per_second   =     18.033\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:03:52,872 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:03:57.437932: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:03:57.437995: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:03:57.438039: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:03:58.561034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:04:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:04:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-04-00_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:04:01 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:04:02 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:04:02 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:04:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:04:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:04:02 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:04:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:04:02,660 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:04:02,664 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:04:02,918 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:04:03,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:04:03,181 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:04:03,704 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:04:03,704 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:04:03,704 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:04:03,704 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:04:03,705 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:04:03,705 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:04:03,706 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:04:03,736 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:04:03,737 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:04:03,770 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:04:06,868 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:04:06,869 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:04:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:04:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:04:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:04:10,569 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:04:11,205 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:04:13,467 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:04:13,467 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:04:13,467 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:04:13,468 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:04:13,468 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:04:13,468 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:04:13,468 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:04:13,469 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:04:13,469 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:04:13,469 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:04:13,469 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:04:13,469 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:04:13,508 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 364.82it/s][INFO|trainer.py:1956] 2023-11-14 19:04:51,699 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.2411, 'train_samples_per_second': 4441.823, 'train_steps_per_second': 277.712, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.74it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:04:51,712 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:04:51,713 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:04:54,220 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:04:54,222 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:04:54,222 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.24\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4441.823\n",
            "  train_steps_per_second   =    277.712\n",
            "11/14/2023 19:04:54 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:04:54,242 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:04:54,244 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:04:54,244 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:04:54,244 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 25.20it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.28it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.92\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    137.615\n",
            "  eval_steps_per_second   =     17.656\n",
            "11/14/2023 19:04:56 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:04:56,173 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:04:56,174 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:04:56,174 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:04:56,175 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.23it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.91\n",
            "  predict_samples_per_second =    138.081\n",
            "  predict_steps_per_second   =     17.716\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:04:58,484 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:05:03.151794: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:05:03.151845: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:05:03.151896: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:05:04.274525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:05:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:05:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-05-06_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:05:06 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:05:07 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:05:07 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:05:07 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:05:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:05:07 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:05:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:05:08,388 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:05:08,392 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:05:08,655 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:05:08,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:05:08,906 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:05:09,418 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:05:09,418 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:05:09,418 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:05:09,418 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:05:09,418 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:05:09,418 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:05:09,419 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:05:09,450 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:05:09,450 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:05:09,483 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:05:12,613 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:05:12,613 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:05:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:05:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:05:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:05:16,355 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:05:16,990 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:05:19,077 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:05:19,078 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:05:19,078 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:05:19,078 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:05:19,078 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:05:19,078 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:05:19,078 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:05:19,079 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:05:19,079 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:05:19,080 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:05:19,080 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:05:19,080 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:05:19,213 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.95it/s][INFO|trainer.py:1956] 2023-11-14 19:05:57,459 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.391, 'train_samples_per_second': 4424.475, 'train_steps_per_second': 276.627, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.65it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:05:57,473 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:05:57,474 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:05:59,988 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:05:59,989 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:05:59,990 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.39\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4424.475\n",
            "  train_steps_per_second   =    276.627\n",
            "11/14/2023 19:06:00 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:06:00,004 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:06:00,006 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:06:00,006 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:06:00,006 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 24.97it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 17.91it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.96\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     134.88\n",
            "  eval_steps_per_second   =     17.305\n",
            "11/14/2023 19:06:01 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:06:01,973 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:06:01,976 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:06:01,976 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:06:01,976 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.52it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.89\n",
            "  predict_samples_per_second =     140.08\n",
            "  predict_steps_per_second   =     17.973\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:06:04,145 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:06:08.696217: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:06:08.696270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:06:08.696313: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:06:09.800316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:06:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:06:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-06-11_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:06:12 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:06:13 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:06:13 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:06:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:06:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:06:13 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:06:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:06:13,945 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:06:13,949 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:06:14,210 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:06:14,461 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:06:14,462 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:06:14,987 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:06:14,987 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:06:14,987 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:06:14,987 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:06:14,987 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:06:14,987 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:06:14,988 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:06:15,019 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:06:15,020 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:06:15,055 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:06:18,216 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:06:18,216 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:06:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:06:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:06:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:06:21,918 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:06:22,541 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:06:24,727 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:06:24,727 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:06:24,727 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:06:24,727 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:06:24,727 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:06:24,727 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:06:24,727 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:06:24,729 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:06:24,729 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:06:24,729 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:06:24,729 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:06:24,729 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:06:24,767 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 364.10it/s][INFO|trainer.py:1956] 2023-11-14 19:07:03,002 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.2839, 'train_samples_per_second': 4436.853, 'train_steps_per_second': 277.401, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.43it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:07:03,015 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:07:03,016 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:07:05,560 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:07:05,561 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:07:05,561 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.28\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4436.853\n",
            "  train_steps_per_second   =    277.401\n",
            "11/14/2023 19:07:05 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:07:05,575 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:07:05,577 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:07:05,577 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:07:05,577 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 25.29it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.28it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.92\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    137.602\n",
            "  eval_steps_per_second   =     17.655\n",
            "11/14/2023 19:07:07 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:07:07,506 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:07:07,507 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:07:07,508 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:07:07,508 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.68it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.87\n",
            "  predict_samples_per_second =    141.429\n",
            "  predict_steps_per_second   =     18.146\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:07:09,653 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:07:14.153663: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:07:14.153717: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:07:14.153762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:07:15.269283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:07:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:07:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-07-17_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:07:17 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:07:18 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:07:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:07:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:07:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:07:18 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:07:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:07:19,497 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:07:19,501 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:07:19,763 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:07:20,024 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:07:20,025 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:07:20,551 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:07:20,551 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:07:20,551 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:07:20,551 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:07:20,551 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:07:20,551 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:07:20,552 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:07:20,584 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:07:20,584 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:07:20,617 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:07:23,723 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:07:23,723 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:07:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:07:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:07:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:07:27,443 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:07:28,061 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:07:30,258 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:07:30,258 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:07:30,258 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:07:30,258 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:07:30,258 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:07:30,258 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:07:30,258 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:07:30,260 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:07:30,260 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:07:30,260 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:07:30,260 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:07:30,260 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:07:30,299 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 363.59it/s][INFO|trainer.py:1956] 2023-11-14 19:08:08,564 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.3151, 'train_samples_per_second': 4433.244, 'train_steps_per_second': 277.176, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.20it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:08:08,577 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:08:08,578 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:08:12,904 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:08:12,906 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:08:12,906 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.31\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4433.244\n",
            "  train_steps_per_second   =    277.176\n",
            "11/14/2023 19:08:12 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:08:12,920 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:08:12,922 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:08:12,922 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:08:12,922 >>   Batch size = 8\n",
            " 94% 32/34 [00:01<00:00, 24.54it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.28it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.92\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    137.596\n",
            "  eval_steps_per_second   =     17.654\n",
            "11/14/2023 19:08:14 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:08:14,850 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:08:14,852 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:08:14,852 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:08:14,852 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.62it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.87\n",
            "  predict_samples_per_second =    140.979\n",
            "  predict_steps_per_second   =     18.088\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:08:17,009 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:08:21.576109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:08:21.576160: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:08:21.576207: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:08:22.701820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:08:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:08:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-08-24_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:08:25 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:08:26 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:08:26 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:08:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:08:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:08:26 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:08:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:08:26,884 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:08:26,888 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:08:27,140 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:08:27,401 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:08:27,402 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:08:27,915 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:08:27,915 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:08:27,915 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:08:27,915 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:08:27,915 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:08:27,915 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:08:27,916 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:08:27,947 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:08:27,947 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:08:27,980 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:08:31,079 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:08:31,080 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:08:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:08:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:08:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:08:34,747 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:08:35,373 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:08:37,572 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:08:37,573 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:08:37,573 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:08:37,573 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:08:37,573 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:08:37,573 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:08:37,573 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:08:37,574 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:08:37,575 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:08:37,575 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:08:37,575 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:08:37,575 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:08:37,613 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 363.66it/s][INFO|trainer.py:1956] 2023-11-14 19:09:15,872 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.3088, 'train_samples_per_second': 4433.968, 'train_steps_per_second': 277.221, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.25it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:09:15,886 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:09:15,887 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:09:19,112 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:09:19,113 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:09:19,113 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.30\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4433.968\n",
            "  train_steps_per_second   =    277.221\n",
            "11/14/2023 19:09:19 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:09:19,127 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:09:19,129 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:09:19,129 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:09:19,129 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 25.26it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.24it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.93\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    137.299\n",
            "  eval_steps_per_second   =     17.616\n",
            "11/14/2023 19:09:21 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:09:21,062 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:09:21,064 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:09:21,064 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:09:21,064 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.40it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.90\n",
            "  predict_samples_per_second =    139.299\n",
            "  predict_steps_per_second   =     17.872\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:09:23,249 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:09:27.884683: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:09:27.884733: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:09:27.884777: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:09:28.992072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:09:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:09:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-09-31_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:09:31 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:09:32 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:09:32 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:09:32 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:09:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:09:32 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:09:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:09:33,121 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:09:33,125 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:09:33,389 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:09:33,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:09:33,649 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:09:34,171 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:09:34,171 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:09:34,171 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:09:34,171 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:09:34,171 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:09:34,171 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:09:34,172 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:09:34,203 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:09:34,204 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:09:34,236 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:09:37,342 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:09:37,342 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:09:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:09:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:09:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:09:41,106 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:09:41,737 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:09:43,944 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:09:43,944 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:09:43,944 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:09:43,944 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:09:43,944 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:09:43,944 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:09:43,944 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:09:43,945 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:09:43,946 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:09:43,946 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:09:43,946 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:09:43,946 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:09:43,984 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 364.13it/s][INFO|trainer.py:1956] 2023-11-14 19:10:22,219 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.285, 'train_samples_per_second': 4436.723, 'train_steps_per_second': 277.393, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.42it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:10:22,233 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:10:22,234 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:10:24,750 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:10:24,751 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:10:24,751 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.28\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4436.723\n",
            "  train_steps_per_second   =    277.393\n",
            "11/14/2023 19:10:24 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:10:24,765 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:10:24,767 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:10:24,767 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:10:24,767 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 24.59it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.02it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.95\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =    135.738\n",
            "  eval_steps_per_second   =     17.415\n",
            "11/14/2023 19:10:26 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:10:26,722 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:10:26,724 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:10:26,724 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:10:26,724 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.54it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.88\n",
            "  predict_samples_per_second =    140.359\n",
            "  predict_steps_per_second   =     18.008\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:10:28,906 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n",
            "2023-11-14 19:10:33.435362: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:10:33.435414: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:10:33.435459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:10:34.540720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:10:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:10:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-10-36_c0e78a6d9811,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 19:10:37 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-432370ffbd9cae8a\n",
            "11/14/2023 19:10:38 - INFO - datasets.builder - Using custom data configuration default-432370ffbd9cae8a\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:10:38 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 19:10:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:10:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:10:38 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 19:10:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:10:38,686 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:10:38,690 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:10:38,979 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:10:39,264 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:10:39,265 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:10:39,778 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:10:39,778 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:10:39,778 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:10:39,778 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:10:39,778 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:10:39,778 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:10:39,779 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:10:39,810 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:10:39,811 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:10:39,843 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:10:42,974 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:10:42,974 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "11/14/2023 19:10:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a7c65c6f393c843f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "11/14/2023 19:10:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a8045d0d44aa9c66.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "11/14/2023 19:10:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432370ffbd9cae8a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1a95ec64f42e7fb6.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 19:10:46,693 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 19:10:47,315 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:10:49,496 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:10:49,496 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:10:49,496 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:10:49,496 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:10:49,496 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:10:49,496 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:10:49,496 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:10:49,498 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 19:10:49,498 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 19:10:49,498 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 19:10:49,498 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 19:10:49,498 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:10:49,537 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 363.94it/s][INFO|trainer.py:1956] 2023-11-14 19:11:27,804 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.3165, 'train_samples_per_second': 4433.074, 'train_steps_per_second': 277.165, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.19it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 19:11:27,817 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:11:27,818 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:11:30,330 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:11:30,331 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:11:30,331 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.31\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4433.074\n",
            "  train_steps_per_second   =    277.165\n",
            "11/14/2023 19:11:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:11:30,346 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:11:30,348 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:11:30,348 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:11:30,348 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 24.88it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:01<00:00, 18.00it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =      0.958\n",
            "  eval_f1                 =     0.9566\n",
            "  eval_loss               =     0.4081\n",
            "  eval_precision          =     0.9587\n",
            "  eval_recall             =     0.9545\n",
            "  eval_runtime            = 0:00:01.95\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     135.58\n",
            "  eval_steps_per_second   =     17.395\n",
            "11/14/2023 19:11:32 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 19:11:32,305 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 19:11:32,307 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 19:11:32,307 >>   Num examples = 265\n",
            "[INFO|trainer.py:3165] 2023-11-14 19:11:32,307 >>   Batch size = 8\n",
            "100% 34/34 [00:01<00:00, 18.49it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =      0.958\n",
            "  predict_f1                 =     0.9566\n",
            "  predict_loss               =     0.4081\n",
            "  predict_precision          =     0.9587\n",
            "  predict_recall             =     0.9545\n",
            "  predict_runtime            = 0:00:01.89\n",
            "  predict_samples_per_second =    139.981\n",
            "  predict_steps_per_second   =      17.96\n",
            "[INFO|modelcard.py:452] 2023-11-14 19:11:34,513 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9586711489025114}, {'name': 'Recall', 'type': 'recall', 'value': 0.954518606024808}, {'name': 'F1', 'type': 'f1', 'value': 0.9565903709550119}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9579982126899017}]}\n",
            "{'preds':                                                  preds\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...\n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C\n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...  \n",
            "3    C,C,C,O,O,O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C  \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.97      0.96      0.97      6590\n",
            "           E       0.96      0.96      0.96      5079\n",
            "           O       0.87      0.92      0.89       640\n",
            "\n",
            "    accuracy                           0.96     12309\n",
            "   macro avg       0.93      0.95      0.94     12309\n",
            "weighted avg       0.96      0.96      0.96     12309\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83TWYdZzoQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########Fincausal as train and SCITE as test#################################"
      ],
      "metadata": {
        "id": "j24yS0pMq04Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uWAyRPBkq1Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCITE, SCITE - 0.94\n",
        "#SCITE, Fincausal - 0.25\n",
        "#SCITE, Organizational - 0.25\n",
        "#Fincausal, SCITE -\n",
        "#Fincausal, Organizational -\n",
        "#Fincausal, Fincausal - 0.96\n",
        "#Organizational, SCITE - 0.37\n",
        "#Organizational, Fincausal - 0.70\n",
        "#Organizational, Organizational - 0.87"
      ],
      "metadata": {
        "id": "iRpf-zHTW3dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_excel('/content/test_data.xlsx',sheet_name = \"Sheet3\")\n",
        "data_test.head(4)"
      ],
      "metadata": {
        "id": "GRfO9Wdho_VX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "089dc973-a609-42c6-894c-b80106493414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentences  \\\n",
              "0  [\"Various\",\"hormonal\",\",\",\"bacterial\",\"and\",\"i...   \n",
              "1  [\"A\",\"stereo\",\"buss\",\"outputs\",\"the\",\"stereo\",...   \n",
              "2  [\"The\",\"incoming\",\"water\",\"caused\",\"a\",\"stain\"...   \n",
              "3  [\"The\",\"genreal\",\"anesthetic\",\"cause\",\"unconsc...   \n",
              "\n",
              "                                           Bio_label  \n",
              "0              [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\"]  \n",
              "1  [\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"E\",\"E\",\"O\",\"C\",\"C\",\"...  \n",
              "2  [\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\",\"...  \n",
              "3  [\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5da852c9-cee2-4707-8f89-586f33e3e5fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>Bio_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"Various\",\"hormonal\",\",\",\"bacterial\",\"and\",\"i...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"A\",\"stereo\",\"buss\",\"outputs\",\"the\",\"stereo\",...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"E\",\"E\",\"O\",\"C\",\"C\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"The\",\"incoming\",\"water\",\"caused\",\"a\",\"stain\"...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"The\",\"genreal\",\"anesthetic\",\"cause\",\"unconsc...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5da852c9-cee2-4707-8f89-586f33e3e5fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5da852c9-cee2-4707-8f89-586f33e3e5fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5da852c9-cee2-4707-8f89-586f33e3e5fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f115e5e1-8a0e-42a5-83fc-e0ea95d5f6b3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f115e5e1-8a0e-42a5-83fc-e0ea95d5f6b3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f115e5e1-8a0e-42a5-83fc-e0ea95d5f6b3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_test[[\"sentences\", \"Bio_label\"]].rename(columns={\"sentences\": \"A\", \"Bio_label\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw_test, y_raw_test = [json.loads(tokens_test) for tokens_test in dataset_test.A.values], [json.loads(labels_test) for labels_test in dataset_test.B.values]"
      ],
      "metadata": {
        "id": "YvWAOMlRrp6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(y_raw_test):\n",
        "    for j, a in enumerate(x):\n",
        "        if 'CE' in a:\n",
        "            y_raw_test[i][j] = a.replace('CE', 'O')"
      ],
      "metadata": {
        "id": "sNVW8Y2dvZxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_raw_test,y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "vde5sv6xrixB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/dev.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRVCe2LCW4dU",
        "outputId": "ded140b8-fb7d-4d7c-8d25-0ad57d04cb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alW6d7BntBPk",
        "outputId": "e5ca575e-394d-435f-906e-3ee68f70fe43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "sEfLvc2LtS9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2LcLlycotUiB",
        "outputId": "0cc89a09-3515-48da-d69c-8f1e4b8daf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 words  \\\n",
              "0    [Various, hormonal, ,, bacterial, and, inflamm...   \n",
              "1    [A, stereo, buss, outputs, the, stereo, buss, ...   \n",
              "2    [The, incoming, water, caused, a, stain, on, t...   \n",
              "3    [The, genreal, anesthetic, cause, unconsciousn...   \n",
              "4    [The, alleged, abuse, resulted, in, bruises, a...   \n",
              "..                                                 ...   \n",
              "186  [Thus, ,, evaluating, capital, punishment, as,...   \n",
              "187  [About, 30, ducks, were, found, dead, in, Klam...   \n",
              "188  [He, created, and, advocated, -, flower, power...   \n",
              "189  [Method, according, to, claim, 1, ,, character...   \n",
              "190  [After, the, war, ,, as, the, Midway, was, pre...   \n",
              "\n",
              "                                                   ner  \n",
              "0                          [C, C, C, C, C, C, C, O, E]  \n",
              "1    [C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...  \n",
              "2    [C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...  \n",
              "3     [C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]  \n",
              "4                    [C, C, C, O, O, E, O, E, O, O, O]  \n",
              "..                                                 ...  \n",
              "186  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "187  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "188  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "189  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "190  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "\n",
              "[191 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef221fd0-e96a-40ca-b4f4-88dc6b657c79\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Various, hormonal, ,, bacterial, and, inflamm...</td>\n",
              "      <td>[C, C, C, C, C, C, C, O, E]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, stereo, buss, outputs, the, stereo, buss, ...</td>\n",
              "      <td>[C, C, C, O, E, E, E, E, O, C, C, C, O, E, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, incoming, water, caused, a, stain, on, t...</td>\n",
              "      <td>[C, C, C, O, E, E, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, genreal, anesthetic, cause, unconsciousn...</td>\n",
              "      <td>[C, C, C, O, E, O, E, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, alleged, abuse, resulted, in, bruises, a...</td>\n",
              "      <td>[C, C, C, O, O, E, O, E, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>[Thus, ,, evaluating, capital, punishment, as,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>[About, 30, ducks, were, found, dead, in, Klam...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>[He, created, and, advocated, -, flower, power...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>[Method, according, to, claim, 1, ,, character...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>[After, the, war, ,, as, the, Midway, was, pre...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef221fd0-e96a-40ca-b4f4-88dc6b657c79')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ef221fd0-e96a-40ca-b4f4-88dc6b657c79 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ef221fd0-e96a-40ca-b4f4-88dc6b657c79');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-136671a4-cd31-4b8f-ad9b-d0c8071ff83e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-136671a4-cd31-4b8f-ad9b-d0c8071ff83e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-136671a4-cd31-4b8f-ad9b-d0c8071ff83e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ae55ccd3-3ee3-488c-a773-4fd5339a0174\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ae55ccd3-3ee3-488c-a773-4fd5339a0174 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "c9VzveoNtGqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abT-5kcLri1F",
        "outputId": "a34a849d-568b-42b0-b5bc-98d052bb0325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-14 19:43:32.867107: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 19:43:32.867162: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 19:43:32.867207: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 19:43:34.072319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 19:43:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 19:43:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_19-43-36_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 19:43:38 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 20295.02it/s]\n",
            "Downloading took 0.0 min\n",
            "11/14/2023 19:43:38 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/14/2023 19:43:38 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2169.84it/s]\n",
            "Generating train split\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 5662 examples [00:00, 157641.04 examples/s]\n",
            "Generating validation split\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 5662 examples [00:00, 283309.66 examples/s]\n",
            "Generating test split\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 191 examples [00:00, 82990.99 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/14/2023 19:43:38 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/14/2023 19:43:38 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading (…)lve/main/config.json: 100% 414/414 [00:00<00:00, 1.82MB/s]\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:43:39,708 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:43:39,713 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 19:43:40,035 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:43:40,291 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:43:40,292 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 25.7MB/s]\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:43:42,041 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:43:42,041 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:43:42,041 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:43:42,041 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 19:43:42,041 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:43:42,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:43:42,042 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 19:43:42,075 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 19:43:42,076 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 665M/665M [00:01<00:00, 489MB/s]\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 19:43:44,347 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 19:43:47,542 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 19:43:47,542 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/5662 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 19:43:47,615 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 19:43:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Running tokenizer on train dataset: 100% 5662/5662 [00:01<00:00, 4754.64 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/5662 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c4e48e0c0f153b1f.arrow\n",
            "11/14/2023 19:43:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c4e48e0c0f153b1f.arrow\n",
            "Running tokenizer on validation dataset: 100% 5662/5662 [00:01<00:00, 4930.79 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/191 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9fba284355e6f13a.arrow\n",
            "11/14/2023 19:43:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9fba284355e6f13a.arrow\n",
            "Running tokenizer on prediction dataset: 100% 191/191 [00:00<00:00, 8132.54 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 22.3MB/s]\n",
            "[INFO|trainer.py:738] 2023-11-14 19:43:58,523 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 19:43:58,539 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 19:43:58,540 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 19:43:58,540 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 19:43:58,540 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 19:43:58,540 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 19:43:58,540 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 19:43:58,540 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 19:43:58,541 >>   Number of trainable parameters = 332,532,739\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 19:43:58,556 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.3594, 'learning_rate': 4.764595103578154e-05, 'epoch': 1.41}\n",
            "  5% 500/10620 [02:38<53:23,  3.16it/s][INFO|trainer.py:2883] 2023-11-14 19:46:36,856 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:46:36,857 >> Configuration saved in /content/test-ner-2022/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:46:39,214 >> Model weights saved in /content/test-ner-2022/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:46:39,215 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:46:39,215 >> Special tokens file saved in /content/test-ner-2022/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.1247, 'learning_rate': 4.5291902071563094e-05, 'epoch': 2.82}\n",
            "  9% 1000/10620 [05:27<58:22,  2.75it/s][INFO|trainer.py:2883] 2023-11-14 19:49:26,352 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:49:26,353 >> Configuration saved in /content/test-ner-2022/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:49:28,689 >> Model weights saved in /content/test-ner-2022/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:49:28,690 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:49:28,691 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.0546, 'learning_rate': 4.2937853107344634e-05, 'epoch': 4.24}\n",
            " 14% 1500/10620 [08:17<46:32,  3.27it/s][INFO|trainer.py:2883] 2023-11-14 19:52:16,541 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:52:16,542 >> Configuration saved in /content/test-ner-2022/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:52:18,948 >> Model weights saved in /content/test-ner-2022/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:52:18,949 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:52:18,949 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.0342, 'learning_rate': 4.058380414312618e-05, 'epoch': 5.65}\n",
            " 19% 2000/10620 [11:05<49:06,  2.93it/s][INFO|trainer.py:2883] 2023-11-14 19:55:04,172 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-2000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:55:04,173 >> Configuration saved in /content/test-ner-2022/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:55:06,519 >> Model weights saved in /content/test-ner-2022/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:55:06,521 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:55:06,521 >> Special tokens file saved in /content/test-ner-2022/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.0301, 'learning_rate': 3.8229755178907726e-05, 'epoch': 7.06}\n",
            " 24% 2500/10620 [13:57<56:14,  2.41it/s][INFO|trainer.py:2883] 2023-11-14 19:57:56,031 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-2500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 19:57:56,032 >> Configuration saved in /content/test-ner-2022/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 19:57:58,387 >> Model weights saved in /content/test-ner-2022/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 19:57:58,388 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 19:57:58,388 >> Special tokens file saved in /content/test-ner-2022/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.0209, 'learning_rate': 3.587570621468927e-05, 'epoch': 8.47}\n",
            " 28% 3000/10620 [16:47<37:09,  3.42it/s][INFO|trainer.py:2883] 2023-11-14 20:00:45,745 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-3000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:00:45,746 >> Configuration saved in /content/test-ner-2022/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:00:48,046 >> Model weights saved in /content/test-ner-2022/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:00:48,048 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:00:48,048 >> Special tokens file saved in /content/test-ner-2022/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.0147, 'learning_rate': 3.352165725047081e-05, 'epoch': 9.89}\n",
            " 33% 3500/10620 [19:32<32:25,  3.66it/s][INFO|trainer.py:2883] 2023-11-14 20:03:31,463 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-3500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:03:31,464 >> Configuration saved in /content/test-ner-2022/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:03:33,800 >> Model weights saved in /content/test-ner-2022/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:03:33,801 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:03:33,801 >> Special tokens file saved in /content/test-ner-2022/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.0117, 'learning_rate': 3.116760828625235e-05, 'epoch': 11.3}\n",
            " 38% 4000/10620 [22:21<32:57,  3.35it/s][INFO|trainer.py:2883] 2023-11-14 20:06:20,209 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-4000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:06:20,210 >> Configuration saved in /content/test-ner-2022/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:06:22,553 >> Model weights saved in /content/test-ner-2022/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:06:22,554 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:06:22,554 >> Special tokens file saved in /content/test-ner-2022/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.0097, 'learning_rate': 2.88135593220339e-05, 'epoch': 12.71}\n",
            " 42% 4500/10620 [25:10<32:59,  3.09it/s][INFO|trainer.py:2883] 2023-11-14 20:09:09,451 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-4500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:09:09,452 >> Configuration saved in /content/test-ner-2022/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:09:11,750 >> Model weights saved in /content/test-ner-2022/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:09:11,751 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:09:11,752 >> Special tokens file saved in /content/test-ner-2022/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.0114, 'learning_rate': 2.6459510357815442e-05, 'epoch': 14.12}\n",
            " 47% 5000/10620 [28:00<28:59,  3.23it/s][INFO|trainer.py:2883] 2023-11-14 20:11:59,542 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-5000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:11:59,544 >> Configuration saved in /content/test-ner-2022/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:12:01,882 >> Model weights saved in /content/test-ner-2022/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:12:01,883 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:12:01,884 >> Special tokens file saved in /content/test-ner-2022/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 0.0079, 'learning_rate': 2.4105461393596988e-05, 'epoch': 15.54}\n",
            " 52% 5500/10620 [30:50<25:50,  3.30it/s][INFO|trainer.py:2883] 2023-11-14 20:14:49,029 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-5500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:14:49,030 >> Configuration saved in /content/test-ner-2022/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:14:51,448 >> Model weights saved in /content/test-ner-2022/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:14:51,449 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:14:51,449 >> Special tokens file saved in /content/test-ner-2022/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 0.0079, 'learning_rate': 2.175141242937853e-05, 'epoch': 16.95}\n",
            " 56% 6000/10620 [33:37<24:55,  3.09it/s][INFO|trainer.py:2883] 2023-11-14 20:17:36,088 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-6000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:17:36,089 >> Configuration saved in /content/test-ner-2022/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:17:38,487 >> Model weights saved in /content/test-ner-2022/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:17:38,489 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:17:38,489 >> Special tokens file saved in /content/test-ner-2022/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 0.0051, 'learning_rate': 1.9397363465160076e-05, 'epoch': 18.36}\n",
            " 61% 6500/10620 [36:27<20:11,  3.40it/s][INFO|trainer.py:2883] 2023-11-14 20:20:26,286 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-6500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:20:26,287 >> Configuration saved in /content/test-ner-2022/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:20:28,674 >> Model weights saved in /content/test-ner-2022/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:20:28,675 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:20:28,675 >> Special tokens file saved in /content/test-ner-2022/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 0.0013, 'learning_rate': 1.704331450094162e-05, 'epoch': 19.77}\n",
            " 66% 7000/10620 [39:16<21:07,  2.86it/s][INFO|trainer.py:2883] 2023-11-14 20:23:14,881 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-7000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:23:14,882 >> Configuration saved in /content/test-ner-2022/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:23:17,290 >> Model weights saved in /content/test-ner-2022/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:23:17,291 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:23:17,291 >> Special tokens file saved in /content/test-ner-2022/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 0.0035, 'learning_rate': 1.4689265536723165e-05, 'epoch': 21.19}\n",
            " 71% 7500/10620 [42:04<14:56,  3.48it/s][INFO|trainer.py:2883] 2023-11-14 20:26:03,302 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-7500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:26:03,303 >> Configuration saved in /content/test-ner-2022/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:26:05,666 >> Model weights saved in /content/test-ner-2022/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:26:05,668 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:26:05,668 >> Special tokens file saved in /content/test-ner-2022/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 0.0022, 'learning_rate': 1.2335216572504708e-05, 'epoch': 22.6}\n",
            " 75% 8000/10620 [44:58<15:36,  2.80it/s][INFO|trainer.py:2883] 2023-11-14 20:28:56,673 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-8000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:28:56,674 >> Configuration saved in /content/test-ner-2022/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:28:58,979 >> Model weights saved in /content/test-ner-2022/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:28:58,980 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:28:58,980 >> Special tokens file saved in /content/test-ner-2022/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 0.0022, 'learning_rate': 9.981167608286254e-06, 'epoch': 24.01}\n",
            " 80% 8500/10620 [47:47<10:35,  3.33it/s][INFO|trainer.py:2883] 2023-11-14 20:31:45,796 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-8500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:31:45,798 >> Configuration saved in /content/test-ner-2022/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:31:48,183 >> Model weights saved in /content/test-ner-2022/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:31:48,184 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:31:48,185 >> Special tokens file saved in /content/test-ner-2022/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 0.0016, 'learning_rate': 7.627118644067798e-06, 'epoch': 25.42}\n",
            " 85% 9000/10620 [50:38<11:04,  2.44it/s][INFO|trainer.py:2883] 2023-11-14 20:34:36,899 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-9000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:34:36,901 >> Configuration saved in /content/test-ner-2022/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:34:39,171 >> Model weights saved in /content/test-ner-2022/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:34:39,172 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:34:39,173 >> Special tokens file saved in /content/test-ner-2022/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 0.0008, 'learning_rate': 5.273069679849341e-06, 'epoch': 26.84}\n",
            " 89% 9500/10620 [53:25<06:41,  2.79it/s][INFO|trainer.py:2883] 2023-11-14 20:37:24,007 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-9500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:37:24,008 >> Configuration saved in /content/test-ner-2022/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:37:26,395 >> Model weights saved in /content/test-ner-2022/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:37:26,396 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:37:26,396 >> Special tokens file saved in /content/test-ner-2022/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 0.0011, 'learning_rate': 2.919020715630885e-06, 'epoch': 28.25}\n",
            " 94% 10000/10620 [56:13<02:57,  3.49it/s][INFO|trainer.py:2883] 2023-11-14 20:40:12,102 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-10000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:40:12,103 >> Configuration saved in /content/test-ner-2022/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:40:14,486 >> Model weights saved in /content/test-ner-2022/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:40:14,487 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:40:14,488 >> Special tokens file saved in /content/test-ner-2022/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 0.0008, 'learning_rate': 5.649717514124295e-07, 'epoch': 29.66}\n",
            " 99% 10500/10620 [59:03<00:34,  3.44it/s][INFO|trainer.py:2883] 2023-11-14 20:43:02,102 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-10500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:43:02,104 >> Configuration saved in /content/test-ner-2022/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:43:04,471 >> Model weights saved in /content/test-ner-2022/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:43:04,473 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:43:04,473 >> Special tokens file saved in /content/test-ner-2022/checkpoint-10500/special_tokens_map.json\n",
            "100% 10620/10620 [59:52<00:00,  2.99it/s][INFO|trainer.py:1956] 2023-11-14 20:43:51,141 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3592.6092, 'train_samples_per_second': 47.28, 'train_steps_per_second': 2.956, 'train_loss': 0.03323743462618686, 'epoch': 30.0}\n",
            "100% 10620/10620 [59:52<00:00,  2.96it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:43:51,153 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:43:51,154 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:43:53,537 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:43:53,538 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:43:53,539 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0332\n",
            "  train_runtime            = 0:59:52.60\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =      47.28\n",
            "  train_steps_per_second   =      2.956\n",
            "11/14/2023 20:43:53 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:43:53,556 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:43:53,584 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:43:53,584 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:43:53,584 >>   Batch size = 8\n",
            "100% 706/708 [00:33<00:00, 24.81it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:42<00:00, 16.73it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9998\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9999\n",
            "  eval_runtime            = 0:00:42.57\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    133.001\n",
            "  eval_steps_per_second   =     16.631\n",
            "11/14/2023 20:44:36 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:44:36,159 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:44:36,161 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:44:36,161 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:44:36,162 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 33.01it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3054\n",
            "  predict_f1                 =     0.3062\n",
            "  predict_loss               =     5.9504\n",
            "  predict_precision          =     0.1936\n",
            "  predict_recall             =     0.7329\n",
            "  predict_runtime            = 0:00:00.75\n",
            "  predict_samples_per_second =     251.41\n",
            "  predict_steps_per_second   =     31.591\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:44:37,903 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996383550320314}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998794226065387}, {'name': 'F1', 'type': 'f1', 'value': 0.9997588742873875}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.86      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:44:45.350057: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:44:45.350120: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:44:45.350169: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:44:46.923933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:44:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:44:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-44-49_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:44:50 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:44:51 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:44:51 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:44:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:44:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:44:51 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:44:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:44:52,329 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:44:52,337 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:44:52,761 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:44:53,181 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:44:53,182 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:44:54,022 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:44:54,023 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:44:54,023 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:44:54,023 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:44:54,023 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:44:54,023 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:44:54,024 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:44:54,056 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:44:54,057 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:44:54,151 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:44:59,738 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:44:59,738 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:44:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/5662 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 20:44:59,850 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:44:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Running tokenizer on validation dataset: 100% 5662/5662 [00:01<00:00, 4638.61 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9fba284355e6f13a.arrow\n",
            "11/14/2023 20:45:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9fba284355e6f13a.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:45:07,962 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:45:08,616 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:45:10,789 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:45:10,790 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:45:10,790 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:45:10,790 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:45:10,790 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:45:10,790 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:45:10,790 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:45:10,791 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:45:10,791 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:45:10,792 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:45:10,792 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:45:10,792 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:45:10,933 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10558/10620 [00:19<00:00, 385.39it/s][INFO|trainer.py:1956] 2023-11-14 20:45:49,549 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.7686, 'train_samples_per_second': 4381.385, 'train_steps_per_second': 273.933, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 273.96it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:45:49,562 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:45:49,563 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:45:52,445 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:45:52,446 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:45:52,447 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.76\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4381.385\n",
            "  train_steps_per_second   =    273.933\n",
            "11/14/2023 20:45:52 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:45:52,462 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:45:52,464 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:45:52,464 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:45:52,464 >>   Batch size = 8\n",
            "100% 706/708 [00:33<00:00, 24.86it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:42<00:00, 16.75it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:42.32\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    133.761\n",
            "  eval_steps_per_second   =     16.726\n",
            "11/14/2023 20:46:34 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:46:34,797 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:46:34,799 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:46:34,799 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:46:34,800 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 33.11it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.75\n",
            "  predict_samples_per_second =    253.277\n",
            "  predict_steps_per_second   =     31.825\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:46:35,977 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:46:41.029413: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:46:41.029477: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:46:41.029520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:46:42.251570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:46:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:46:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-46-44_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:46:45 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:46:45 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:46:45 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:46:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:46:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:46:45 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:46:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:46:46,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:46:46,595 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:46:46,844 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:46:47,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:46:47,108 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:46:47,628 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:46:47,628 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:46:47,628 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:46:47,628 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:46:47,628 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:46:47,628 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:46:47,629 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:46:47,660 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:46:47,660 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:46:47,694 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:46:50,852 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:46:50,852 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:46:50 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:46:50 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/191 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 20:46:50,931 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:46:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "Running tokenizer on prediction dataset: 100% 191/191 [00:00<00:00, 5764.35 examples/s]\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:46:55,840 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:46:56,536 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:46:58,856 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:46:58,856 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:46:58,856 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:46:58,856 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:46:58,856 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:46:58,856 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:46:58,856 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:46:58,858 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:46:58,858 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:46:58,858 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:46:58,858 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:46:58,858 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:46:58,898 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 10602/10620 [00:32<00:00, 140.63it/s][INFO|trainer.py:1956] 2023-11-14 20:47:37,441 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5932, 'train_samples_per_second': 4401.289, 'train_steps_per_second': 275.178, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.20it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:47:37,454 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:47:37,455 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:47:40,119 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:47:40,120 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:47:40,121 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.59\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4401.289\n",
            "  train_steps_per_second   =    275.178\n",
            "11/14/2023 20:47:40 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:47:40,135 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:47:40,137 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:47:40,137 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:47:40,138 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.37it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.05it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.59\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    136.116\n",
            "  eval_steps_per_second   =     17.021\n",
            "11/14/2023 20:48:21 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:48:21,740 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:48:21,742 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:48:21,742 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:48:21,742 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 34.39it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.72\n",
            "  predict_samples_per_second =    262.534\n",
            "  predict_steps_per_second   =     32.989\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:48:22,727 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:48:27.171024: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:48:27.171072: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:48:27.171122: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:48:28.295616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:48:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:48:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-48-30_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:48:30 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:48:31 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:48:31 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:48:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:48:31 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:48:31 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:48:31 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:48:32,521 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:48:32,525 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:48:32,804 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:48:33,065 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:48:33,066 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:48:33,576 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:48:33,576 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:48:33,576 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:48:33,576 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:48:33,576 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:48:33,576 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:48:33,577 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:48:33,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:48:33,608 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:48:33,641 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:48:36,740 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:48:36,740 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:48:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:48:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:48:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:48:41,110 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:48:41,745 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:48:43,957 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:48:43,957 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:48:43,957 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:48:43,957 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:48:43,957 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:48:43,958 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:48:43,958 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:48:43,959 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:48:43,959 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:48:43,959 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:48:43,959 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:48:43,959 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:48:43,998 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.93it/s][INFO|trainer.py:1956] 2023-11-14 20:49:22,362 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.4139, 'train_samples_per_second': 4421.835, 'train_steps_per_second': 276.462, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.49it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:49:22,375 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:49:22,376 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:49:24,843 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:49:24,844 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:49:24,844 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.41\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4421.835\n",
            "  train_steps_per_second   =    276.462\n",
            "11/14/2023 20:49:24 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:49:24,858 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:49:24,860 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:49:24,861 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:49:24,861 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.32it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.62\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    136.015\n",
            "  eval_steps_per_second   =     17.008\n",
            "11/14/2023 20:50:06 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:50:06,493 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:50:06,495 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:50:06,495 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:50:06,495 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 34.38it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.72\n",
            "  predict_samples_per_second =    262.685\n",
            "  predict_steps_per_second   =     33.008\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:50:07,499 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:50:12.001180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:50:12.001236: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:50:12.001288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:50:13.120946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:50:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:50:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-50-15_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:50:16 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:50:17 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:50:17 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:50:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:50:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:50:17 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:50:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:50:18,505 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:50:18,509 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:50:18,984 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:50:19,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:50:19,441 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:50:20,373 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:50:20,373 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:50:20,373 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:50:20,373 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:50:20,373 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:50:20,373 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:50:20,374 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:50:20,405 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:50:20,406 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:50:20,439 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:50:23,531 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:50:23,531 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:50:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:50:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:50:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:50:28,441 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:50:29,067 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:50:31,262 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:50:31,262 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:50:31,262 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:50:31,263 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:50:31,263 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:50:31,263 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:50:31,263 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:50:31,264 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:50:31,264 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:50:31,264 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:50:31,264 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:50:31,264 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:50:31,303 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.74it/s][INFO|trainer.py:1956] 2023-11-14 20:51:09,679 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.4257, 'train_samples_per_second': 4420.478, 'train_steps_per_second': 276.377, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.40it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:51:09,692 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:51:09,693 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:51:12,186 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:51:12,187 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:51:12,187 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.42\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4420.478\n",
            "  train_steps_per_second   =    276.377\n",
            "11/14/2023 20:51:12 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:51:12,201 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:51:12,203 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:51:12,203 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:51:12,203 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.37it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.62\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    136.033\n",
            "  eval_steps_per_second   =      17.01\n",
            "11/14/2023 20:51:53 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:51:53,830 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:51:53,832 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:51:53,832 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:51:53,832 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 33.73it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.74\n",
            "  predict_samples_per_second =    257.735\n",
            "  predict_steps_per_second   =     32.386\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:51:55,038 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:51:59.570959: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:51:59.571031: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:51:59.571074: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:52:00.701792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:52:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:52:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-52-02_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:52:03 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:52:04 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:52:04 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:52:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:52:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:52:04 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:52:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:52:05,904 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:52:05,908 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:52:06,340 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:52:06,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:52:06,897 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:52:07,749 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:52:07,749 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:52:07,749 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:52:07,749 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:52:07,749 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:52:07,749 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:52:07,750 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:52:07,781 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:52:07,782 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:52:07,815 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:52:10,955 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:52:10,955 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:52:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:52:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:52:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:52:15,766 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:52:16,400 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:52:18,612 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:52:18,612 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:52:18,612 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:52:18,612 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:52:18,612 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:52:18,612 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:52:18,612 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:52:18,613 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:52:18,614 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:52:18,614 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:52:18,614 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:52:18,614 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:52:18,653 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.50it/s][INFO|trainer.py:1956] 2023-11-14 20:52:57,076 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.4726, 'train_samples_per_second': 4415.091, 'train_steps_per_second': 276.041, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.06it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:52:57,089 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:52:57,090 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:52:59,654 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:52:59,655 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:52:59,655 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.47\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4415.091\n",
            "  train_steps_per_second   =    276.041\n",
            "11/14/2023 20:52:59 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:52:59,670 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:52:59,672 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:52:59,672 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:52:59,672 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.39it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.62\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    136.009\n",
            "  eval_steps_per_second   =     17.007\n",
            "11/14/2023 20:53:41 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:53:41,306 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:53:41,308 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:53:41,308 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:53:41,308 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 34.43it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.72\n",
            "  predict_samples_per_second =    262.412\n",
            "  predict_steps_per_second   =     32.973\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:53:42,472 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:53:47.036301: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:53:47.036352: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:53:47.036397: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:53:48.148701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:53:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:53:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-53-50_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:53:50 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:53:51 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:53:51 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:53:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:53:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:53:51 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:53:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:53:52,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:53:52,922 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:53:53,396 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:53:54,321 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:53:54,322 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:53:55,690 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:53:55,690 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:53:55,690 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:53:55,690 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:53:55,690 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:53:55,691 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:53:55,691 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:53:55,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:53:55,723 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:53:55,756 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:53:58,886 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:53:58,886 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:53:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:53:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:53:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:54:02,585 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:54:03,209 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:54:05,385 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:54:05,386 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:54:05,386 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:54:05,386 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:54:05,386 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:54:05,386 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:54:05,386 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:54:05,387 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:54:05,387 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:54:05,387 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:54:05,387 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:54:05,388 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:54:05,426 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 365.02it/s][INFO|trainer.py:1956] 2023-11-14 20:54:43,624 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.2477, 'train_samples_per_second': 4441.049, 'train_steps_per_second': 277.664, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 277.69it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:54:43,637 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:54:43,638 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:54:45,956 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:54:45,958 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:54:45,958 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.24\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4441.049\n",
            "  train_steps_per_second   =    277.664\n",
            "11/14/2023 20:54:45 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:54:45,971 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:54:45,974 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:54:45,974 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:54:45,974 >>   Batch size = 8\n",
            "100% 706/708 [00:33<00:00, 25.41it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.06it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.57\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    136.195\n",
            "  eval_steps_per_second   =      17.03\n",
            "11/14/2023 20:55:27 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:55:27,549 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:55:27,551 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:55:27,551 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:55:27,551 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 34.85it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.71\n",
            "  predict_samples_per_second =    266.254\n",
            "  predict_steps_per_second   =     33.456\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:55:28,777 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:55:33.257416: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:55:33.257465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:55:33.257510: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:55:34.377859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:55:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:55:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-55-36_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:55:36 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:55:37 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:55:37 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:55:37 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:55:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:55:37 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:55:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:55:38,544 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:55:38,548 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:55:38,807 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:55:39,066 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:55:39,067 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:55:39,585 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:55:39,585 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:55:39,585 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:55:39,585 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:55:39,585 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:55:39,586 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:55:39,587 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:55:39,618 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:55:39,619 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:55:39,652 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:55:42,742 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:55:42,742 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:55:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:55:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:55:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:55:47,734 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:55:48,368 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:55:50,567 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:55:50,567 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:55:50,567 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:55:50,567 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:55:50,567 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:55:50,567 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:55:50,567 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:55:50,569 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:55:50,569 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:55:50,569 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:55:50,569 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:55:50,569 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:55:50,608 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 362.99it/s][INFO|trainer.py:1956] 2023-11-14 20:56:28,943 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.3855, 'train_samples_per_second': 4425.114, 'train_steps_per_second': 276.667, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.69it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:56:28,957 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:56:28,958 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:56:31,437 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:56:31,438 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:56:31,438 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.38\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4425.114\n",
            "  train_steps_per_second   =    276.667\n",
            "11/14/2023 20:56:31 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:56:31,452 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:56:31,454 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:56:31,454 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:56:31,454 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.43it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.64\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    135.963\n",
            "  eval_steps_per_second   =     17.001\n",
            "11/14/2023 20:57:13 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:57:13,103 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:57:13,105 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:57:13,105 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:57:13,105 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 34.35it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.72\n",
            "  predict_samples_per_second =     261.86\n",
            "  predict_steps_per_second   =     32.904\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:57:14,092 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:57:18.647276: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:57:18.647331: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:57:18.647385: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:57:19.762948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:57:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:57:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-57-21_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:57:22 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:57:23 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:57:23 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:57:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:57:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:57:23 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:57:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:57:25,030 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:57:25,034 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:57:25,471 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:57:25,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:57:25,908 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:57:26,782 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:57:26,782 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:57:26,783 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:57:26,783 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:57:26,783 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:57:26,783 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:57:26,784 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:57:26,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:57:26,816 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:57:26,848 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:57:29,955 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:57:29,955 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:57:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:57:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:57:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:57:34,215 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:57:34,859 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:57:37,071 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:57:37,071 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:57:37,071 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:57:37,071 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:57:37,071 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:57:37,071 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:57:37,072 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:57:37,073 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:57:37,073 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:57:37,073 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:57:37,073 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:57:37,073 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:57:37,112 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.07it/s][INFO|trainer.py:1956] 2023-11-14 20:58:15,544 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.482, 'train_samples_per_second': 4414.012, 'train_steps_per_second': 275.973, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.00it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 20:58:15,558 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 20:58:15,559 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 20:58:18,062 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 20:58:18,064 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 20:58:18,064 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.48\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4414.012\n",
            "  train_steps_per_second   =    275.973\n",
            "11/14/2023 20:58:18 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:58:18,077 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:58:18,079 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:58:18,080 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:58:18,080 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.46it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 16.99it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.73\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =     135.65\n",
            "  eval_steps_per_second   =     16.962\n",
            "11/14/2023 20:58:59 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 20:58:59,824 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 20:58:59,826 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 20:58:59,826 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 20:58:59,826 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 33.23it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.75\n",
            "  predict_samples_per_second =    254.026\n",
            "  predict_steps_per_second   =      31.92\n",
            "[INFO|modelcard.py:452] 2023-11-14 20:59:01,015 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n",
            "2023-11-14 20:59:05.559576: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 20:59:05.559632: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 20:59:05.559686: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 20:59:06.670924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 20:59:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 20:59:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_20-59-08_6820adff26c2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 20:59:09 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-dd4beac0b63cabfb\n",
            "11/14/2023 20:59:10 - INFO - datasets.builder - Using custom data configuration default-dd4beac0b63cabfb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 20:59:10 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 20:59:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:59:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 20:59:10 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 20:59:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:59:11,784 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:59:11,788 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 20:59:12,209 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:59:12,631 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:59:12,632 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:59:13,478 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:59:13,478 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:59:13,478 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:59:13,478 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 20:59:13,478 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:59:13,478 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:59:13,479 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 20:59:13,510 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 20:59:13,511 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 20:59:13,544 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 20:59:16,724 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 20:59:16,724 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "11/14/2023 20:59:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d77dbef6d759154c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "11/14/2023 20:59:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e8771ccc75c120e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "11/14/2023 20:59:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-dd4beac0b63cabfb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-267f8e6f4aaa7bb7.arrow\n",
            "[INFO|trainer.py:2067] 2023-11-14 20:59:21,044 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 20:59:21,663 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 20:59:23,780 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 20:59:23,780 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 20:59:23,780 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 20:59:23,780 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 20:59:23,780 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 20:59:23,780 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 20:59:23,780 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 20:59:23,782 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 20:59:23,782 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 20:59:23,782 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 20:59:23,782 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 20:59:23,782 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 20:59:23,918 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 367.64it/s][INFO|trainer.py:1956] 2023-11-14 21:00:02,305 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5347, 'train_samples_per_second': 4407.973, 'train_steps_per_second': 275.596, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.62it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 21:00:02,319 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 21:00:02,320 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 21:00:04,800 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 21:00:04,801 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 21:00:04,801 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.53\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4407.973\n",
            "  train_steps_per_second   =    275.596\n",
            "11/14/2023 21:00:04 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 21:00:04,816 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 21:00:04,818 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 21:00:04,818 >>   Num examples = 5662\n",
            "[INFO|trainer.py:3165] 2023-11-14 21:00:04,818 >>   Batch size = 8\n",
            "100% 707/708 [00:33<00:00, 24.38it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 708/708 [00:41<00:00, 17.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9998\n",
            "  eval_f1                 =     0.9997\n",
            "  eval_loss               =     0.0007\n",
            "  eval_precision          =     0.9996\n",
            "  eval_recall             =     0.9998\n",
            "  eval_runtime            = 0:00:41.61\n",
            "  eval_samples            =       5662\n",
            "  eval_samples_per_second =    136.047\n",
            "  eval_steps_per_second   =     17.012\n",
            "11/14/2023 21:00:46 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 21:00:46,441 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 21:00:46,443 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 21:00:46,443 >>   Num examples = 191\n",
            "[INFO|trainer.py:3165] 2023-11-14 21:00:46,443 >>   Batch size = 8\n",
            "100% 24/24 [00:00<00:00, 32.78it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3057\n",
            "  predict_f1                 =     0.3056\n",
            "  predict_loss               =     5.9457\n",
            "  predict_precision          =     0.1932\n",
            "  predict_recall             =     0.7308\n",
            "  predict_runtime            = 0:00:00.76\n",
            "  predict_samples_per_second =    250.133\n",
            "  predict_steps_per_second   =      31.43\n",
            "[INFO|modelcard.py:452] 2023-11-14 21:00:47,637 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.9996469473865496}, {'name': 'Recall', 'type': 'recall', 'value': 0.9998363592517312}, {'name': 'F1', 'type': 'f1', 'value': 0.999741644347609}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9997688241907972}]}\n",
            "{'preds':                                                  preds\n",
            "0                                    C,C,C,C,C,C,C,O,E\n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C\n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                                C,C,C,O,O,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...\n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...\n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                    C,C,C,C,C,C,C,O,E  \n",
            "1                    C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,C,C  \n",
            "2    C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3                      C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                                C,C,C,O,O,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "187  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,...  \n",
            "188  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,...  \n",
            "189  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "190  C,C,C,C,C,C,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.27      0.87      0.41       465\n",
            "           E       0.19      0.73      0.31       468\n",
            "           O       0.98      0.13      0.24      2688\n",
            "\n",
            "    accuracy                           0.31      3621\n",
            "   macro avg       0.48      0.58      0.32      3621\n",
            "weighted avg       0.79      0.31      0.27      3621\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############FinCausal as train and Organizational as test##############################"
      ],
      "metadata": {
        "id": "ofAcPa7J8YaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# data location\n",
        "#parent_dir = \"/content/drive/MyDrive\"\n",
        "parent_dir = \"/content/drive/MyDrive/\"\n",
        "mk_data_path = lambda filename: os.path.join(parent_dir, filename)\n",
        "#data_path = f\"{parent_dir}/BERT_data_final.xlsx\"\n",
        "data_path = mk_data_path(\"BERT_data_final1.xlsx\")\n",
        "\n",
        "# once finished, flush changes to files to make them visible (assuming we write to Google Drive)\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB5Xhuhz8hIK",
        "outputId": "db79c3a1-a0fa-4c8c-a970-54c4e08445a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(data_path)\n",
        "data.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "m2hJuR6s8yRI",
        "outputId": "59b805ed-faa7-499c-c1c6-a543e45ca517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                Causal-relation text  \\\n",
              "0  [\"When \",\"a \",\"policyholder \",\"or \",\"insured \"...   \n",
              "1  [\"During \",\"2020 \",\"in \",\"response \",\"to \",\"th...   \n",
              "2  [\"Prolonged \",\"periods \",\"of \",\"low \",\"interes...   \n",
              "3  [\"Conversely \",\"a \",\"rise \",\"in \",\"interest \",...   \n",
              "\n",
              "                                            BIO Code  \\\n",
              "0  [\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"...   \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"...   \n",
              "2  [\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT...   \n",
              "3  [\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT\",\"O...   \n",
              "\n",
              "       {\"words\":Causal-relation text,\"ner\":BIO Code}  \\\n",
              "0  {\"words\":[\"When \",\"a \",\"policyholder \",\"or \",\"...   \n",
              "1  {\"words\":[\"During \",\"2020 \",\"in \",\"response \",...   \n",
              "2  {\"words\":[\"Prolonged \",\"periods \",\"of \",\"low \"...   \n",
              "3  {\"words\":[\"Conversely \",\"a \",\"rise \",\"in \",\"in...   \n",
              "\n",
              "                                    BIO_Code_changed  \\\n",
              "0  [\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"...   \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"...   \n",
              "2  [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"E\",\"E\",\"O\",...   \n",
              "3  [\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"O\",\"O\",\"O\",...   \n",
              "\n",
              "                                BIO Code-changed-NER  \n",
              "0  [\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...  \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-...  \n",
              "2  [\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"...  \n",
              "3  [\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-098faa7c-9bb6-4b82-a57e-2df5cbfd93a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Causal-relation text</th>\n",
              "      <th>BIO Code</th>\n",
              "      <th>{\"words\":Causal-relation text,\"ner\":BIO Code}</th>\n",
              "      <th>BIO_Code_changed</th>\n",
              "      <th>BIO Code-changed-NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"When \",\"a \",\"policyholder \",\"or \",\"insured \"...</td>\n",
              "      <td>[\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"...</td>\n",
              "      <td>{\"words\":[\"When \",\"a \",\"policyholder \",\"or \",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"During \",\"2020 \",\"in \",\"response \",\"to \",\"th...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"...</td>\n",
              "      <td>{\"words\":[\"During \",\"2020 \",\"in \",\"response \",...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"Prolonged \",\"periods \",\"of \",\"low \",\"interes...</td>\n",
              "      <td>[\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT...</td>\n",
              "      <td>{\"words\":[\"Prolonged \",\"periods \",\"of \",\"low \"...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"E\",\"E\",\"O\",...</td>\n",
              "      <td>[\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"Conversely \",\"a \",\"rise \",\"in \",\"interest \",...</td>\n",
              "      <td>[\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT\",\"O...</td>\n",
              "      <td>{\"words\":[\"Conversely \",\"a \",\"rise \",\"in \",\"in...</td>\n",
              "      <td>[\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"O\",\"O\",\"O\",...</td>\n",
              "      <td>[\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-098faa7c-9bb6-4b82-a57e-2df5cbfd93a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-098faa7c-9bb6-4b82-a57e-2df5cbfd93a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-098faa7c-9bb6-4b82-a57e-2df5cbfd93a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-89bf62c9-bdea-49e9-befe-7d4ee5328099\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89bf62c9-bdea-49e9-befe-7d4ee5328099')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-89bf62c9-bdea-49e9-befe-7d4ee5328099 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset = data[[\"Causal-relation text\", \"BIO_Code_changed\"]].rename(columns={\"Causal-relation text\": \"X\", \"BIO_Code_changed\": \"y\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw_test, y_raw_test = [json.loads(tokens) for tokens in dataset.X.values], [json.loads(labels) for labels in dataset.y.values]"
      ],
      "metadata": {
        "id": "e4VDtei68zPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Our data is split into sentences\n",
        "\n",
        "\n",
        "# # FIXME: we also need test data!\n",
        "# X_train_raw, X_dev_raw, y_train_raw, y_dev_raw = train_test_split(X_raw, y_raw, test_size=.2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "cWzfSlAn83Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(y_raw_test):\n",
        "    for j, a in enumerate(x):\n",
        "        if 'CT' in a:\n",
        "            y_raw_test[i][j] = a.replace('CT', 'O')"
      ],
      "metadata": {
        "id": "3xFB57hK9JTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners2 = []\n",
        "for i,j in zip(X_raw_test, y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners2.append(_new_ner)"
      ],
      "metadata": {
        "id": "zw05lT7Q_EAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners2[i] for i in range(len(new_ners2))]\n",
        "dump_json_lines(test,  '/content/dev.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeKpZS2q-4Dc",
        "outputId": "7c4befe5-c8d3-4d1b-fb58-c611983601a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners2[i] for i in range(len(new_ners2))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo0Xj6tT_mMR",
        "outputId": "0551522f-d6c1-4ff5-9c01-83893fafe87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)\n",
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "9aTQUUqg_pDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrqWzHcX_sl_",
        "outputId": "53761422-145c-45bc-b32b-c7d7d0ad56b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-14 23:40:12.413688: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:40:12.413746: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:40:12.413776: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:40:13.866586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:40:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:40:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-40-16_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:40:16 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:40:17 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:40:17 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:40:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:40:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:40:17 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:40:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:40:18,579 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:40:18,588 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:40:18,852 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:40:19,105 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:40:19,106 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:40:19,609 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:40:19,609 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:40:19,609 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:40:19,609 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:40:19,609 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:40:19,609 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:40:19,610 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:40:19,642 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:40:19,643 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:40:19,739 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:40:25,119 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:40:25,119 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:40:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/2234 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 23:40:25,210 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:40:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Running tokenizer on validation dataset: 100% 2234/2234 [00:00<00:00, 6153.63 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-50fd9fcaf954adee.arrow\n",
            "11/14/2023 23:40:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-50fd9fcaf954adee.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:40:33,097 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:40:33,735 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:40:35,985 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:40:35,985 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:40:35,985 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:40:35,985 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:40:35,985 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:40:35,985 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:40:35,985 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:40:35,986 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:40:35,987 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:40:35,987 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:40:35,987 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:40:35,987 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:40:36,026 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10559/10620 [00:19<00:00, 384.08it/s][INFO|trainer.py:1967] 2023-11-14 23:41:14,532 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5562, 'train_samples_per_second': 4405.514, 'train_steps_per_second': 275.442, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.46it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:41:14,545 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:41:14,546 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:41:17,297 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:41:17,298 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:41:17,299 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.55\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4405.514\n",
            "  train_steps_per_second   =    275.442\n",
            "11/14/2023 23:41:17 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:41:17,312 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:41:17,314 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:41:17,315 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:41:17,315 >>   Batch size = 8\n",
            " 99% 278/280 [00:08<00:00, 32.65it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.26it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.75\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =     207.66\n",
            "  eval_steps_per_second   =     26.027\n",
            "11/14/2023 23:41:28 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:41:28,075 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:41:28,077 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:41:28,077 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:41:28,077 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.37it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.71\n",
            "  predict_samples_per_second =    208.514\n",
            "  predict_steps_per_second   =     26.134\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:41:39,139 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:41:44.285203: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:41:44.285251: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:41:44.285281: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:41:45.428170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:41:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:41:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-41-47_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:41:48 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:41:49 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:41:49 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:41:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:41:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:41:49 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:41:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:41:49,609 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:41:49,613 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:41:49,872 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:41:50,134 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:41:50,135 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:41:50,665 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:41:50,665 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:41:50,666 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:41:50,666 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:41:50,666 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:41:50,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:41:50,667 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:41:50,698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:41:50,699 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:41:50,732 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:41:53,860 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:41:53,860 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:41:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:41:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/2234 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-14 23:41:53,959 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:41:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "Running tokenizer on prediction dataset: 100% 2234/2234 [00:00<00:00, 6945.08 examples/s]\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:41:57,871 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:41:58,552 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:42:00,893 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:42:00,893 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:42:00,893 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:42:00,893 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:42:00,893 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:42:00,893 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:42:00,893 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:42:00,895 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:42:00,895 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:42:00,895 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:42:00,895 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:42:00,895 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:42:00,934 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 10605/10620 [00:33<00:00, 138.95it/s][INFO|trainer.py:1967] 2023-11-14 23:42:39,465 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5804, 'train_samples_per_second': 4402.752, 'train_steps_per_second': 275.269, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.29it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:42:39,478 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:42:39,479 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:42:42,060 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:42:42,061 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:42:42,061 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.58\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4402.752\n",
            "  train_steps_per_second   =    275.269\n",
            "11/14/2023 23:42:42 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:42:42,075 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:42:42,077 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:42:42,077 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:42:42,077 >>   Batch size = 8\n",
            " 99% 278/280 [00:08<00:00, 32.09it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.12it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.81\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    206.596\n",
            "  eval_steps_per_second   =     25.894\n",
            "11/14/2023 23:42:52 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:42:52,893 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:42:52,895 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:42:52,895 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:42:52,895 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.22it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.77\n",
            "  predict_samples_per_second =    207.324\n",
            "  predict_steps_per_second   =     25.985\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:43:04,043 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:43:09.149083: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:43:09.149139: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:43:09.149170: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:43:10.287646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:43:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:43:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-43-12_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:43:12 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:43:13 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:43:13 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:43:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:43:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:43:13 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:43:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:43:14,504 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:43:14,508 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:43:14,762 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:43:15,079 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:43:15,080 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:43:15,581 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:43:15,581 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:43:15,581 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:43:15,581 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:43:15,581 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:43:15,581 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:43:15,582 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:43:15,613 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:43:15,614 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:43:15,647 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:43:18,764 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:43:18,764 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:43:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:43:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:43:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:43:22,581 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:43:23,213 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:43:25,347 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:43:25,347 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:43:25,348 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:43:25,348 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:43:25,348 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:43:25,348 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:43:25,348 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:43:25,349 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:43:25,349 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:43:25,349 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:43:25,349 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:43:25,349 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:43:25,490 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 366.38it/s][INFO|trainer.py:1967] 2023-11-14 23:44:03,993 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.6549, 'train_samples_per_second': 4394.268, 'train_steps_per_second': 274.739, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 274.76it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:44:04,007 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:44:04,008 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:44:06,540 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:44:06,542 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:44:06,542 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.65\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4394.268\n",
            "  train_steps_per_second   =    274.739\n",
            "11/14/2023 23:44:06 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:44:06,557 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:44:06,559 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:44:06,559 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:44:06,559 >>   Batch size = 8\n",
            "100% 280/280 [00:08<00:00, 33.17it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.27it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.75\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    207.739\n",
            "  eval_steps_per_second   =     26.037\n",
            "11/14/2023 23:44:17 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:44:17,316 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:44:17,317 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:44:17,318 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:44:17,318 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.20it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.78\n",
            "  predict_samples_per_second =    207.207\n",
            "  predict_steps_per_second   =      25.97\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:44:28,466 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:44:33.656245: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:44:33.656298: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:44:33.656340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:44:34.809248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:44:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:44:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-44-37_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:44:37 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:44:38 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:44:38 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:44:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:44:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:44:38 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:44:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:44:39,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:44:39,072 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:44:39,334 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:44:39,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:44:39,596 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:44:40,117 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:44:40,117 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:44:40,117 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:44:40,117 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:44:40,117 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:44:40,117 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:44:40,118 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:44:40,150 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:44:40,150 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:44:40,183 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:44:43,295 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:44:43,295 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:44:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:44:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:44:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:44:47,073 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:44:47,717 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:44:49,950 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:44:49,951 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:44:49,951 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:44:49,951 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:44:49,951 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:44:49,951 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:44:49,951 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:44:49,952 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:44:49,953 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:44:49,953 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:44:49,953 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:44:49,953 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:44:49,992 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 367.52it/s][INFO|trainer.py:1967] 2023-11-14 23:45:28,502 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5609, 'train_samples_per_second': 4404.981, 'train_steps_per_second': 275.409, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.44it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:45:28,516 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:45:28,517 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:45:31,063 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:45:31,064 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:45:31,065 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.56\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4404.981\n",
            "  train_steps_per_second   =    275.409\n",
            "11/14/2023 23:45:31 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:45:31,079 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:45:31,082 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:45:31,082 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:45:31,082 >>   Batch size = 8\n",
            " 99% 276/280 [00:08<00:00, 31.39it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.30it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.74\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =     207.94\n",
            "  eval_steps_per_second   =     26.062\n",
            "11/14/2023 23:45:41 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:45:41,828 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:45:41,830 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:45:41,830 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:45:41,830 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.34it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.72\n",
            "  predict_samples_per_second =    208.264\n",
            "  predict_steps_per_second   =     26.103\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:45:52,927 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:45:58.093198: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:45:58.093250: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:45:58.093285: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:45:59.232315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:46:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:46:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-46-01_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:46:01 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:46:02 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:46:02 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:46:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:46:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:46:02 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:46:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:46:03,483 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:46:03,487 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:46:03,756 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:46:04,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:46:04,483 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:46:04,991 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:46:04,992 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:46:04,992 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:46:04,992 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:46:04,992 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:46:04,992 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:46:04,993 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:46:05,024 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:46:05,025 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:46:05,058 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:46:08,176 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:46:08,176 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:46:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:46:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:46:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:46:11,865 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:46:12,487 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:46:14,686 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:46:14,686 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:46:14,686 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:46:14,686 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:46:14,686 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:46:14,686 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:46:14,686 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:46:14,688 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:46:14,688 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:46:14,688 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:46:14,688 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:46:14,688 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:46:14,727 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.20it/s][INFO|trainer.py:1967] 2023-11-14 23:46:53,193 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5161, 'train_samples_per_second': 4410.099, 'train_steps_per_second': 275.729, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.75it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:46:53,206 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:46:53,207 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:46:55,732 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:46:55,733 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:46:55,733 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.51\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4410.099\n",
            "  train_steps_per_second   =    275.729\n",
            "11/14/2023 23:46:55 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:46:55,748 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:46:55,750 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:46:55,750 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:46:55,750 >>   Batch size = 8\n",
            " 99% 278/280 [00:08<00:00, 32.47it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.13it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.81\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    206.577\n",
            "  eval_steps_per_second   =     25.892\n",
            "11/14/2023 23:47:06 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:47:06,567 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:47:06,569 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:47:06,569 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:47:06,569 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.14it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.80\n",
            "  predict_samples_per_second =    206.691\n",
            "  predict_steps_per_second   =     25.906\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:47:17,742 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:47:22.907959: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:47:22.908024: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:47:22.908053: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:47:24.062301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:47:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:47:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-47-26_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:47:26 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:47:27 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:47:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:47:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:47:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:47:27 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:47:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:47:28,288 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:47:28,292 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:47:28,554 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:47:28,817 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:47:28,818 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:47:29,346 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:47:29,346 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:47:29,346 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:47:29,346 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:47:29,346 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:47:29,347 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:47:29,347 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:47:29,379 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:47:29,380 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:47:29,414 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:47:32,544 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:47:32,544 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:47:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:47:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:47:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:47:36,251 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:47:36,880 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:47:39,079 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:47:39,079 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:47:39,079 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:47:39,079 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:47:39,079 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:47:39,079 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:47:39,079 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:47:39,080 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:47:39,081 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:47:39,081 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:47:39,081 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:47:39,081 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:47:39,119 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 368.12it/s][INFO|trainer.py:1967] 2023-11-14 23:48:17,593 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5238, 'train_samples_per_second': 4409.223, 'train_steps_per_second': 275.674, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.70it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:48:17,607 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:48:17,608 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:48:20,122 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:48:20,123 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:48:20,124 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.52\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4409.223\n",
            "  train_steps_per_second   =    275.674\n",
            "11/14/2023 23:48:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:48:20,138 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:48:20,140 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:48:20,140 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:48:20,140 >>   Batch size = 8\n",
            " 99% 276/280 [00:08<00:00, 31.26it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.30it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.74\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    207.982\n",
            "  eval_steps_per_second   =     26.068\n",
            "11/14/2023 23:48:30 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:48:30,884 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:48:30,886 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:48:30,886 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:48:30,886 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.42it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.69\n",
            "  predict_samples_per_second =    208.902\n",
            "  predict_steps_per_second   =     26.183\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:48:41,941 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:48:47.060975: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:48:47.061043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:48:47.061073: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:48:48.194064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:48:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:48:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-48-50_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:48:50 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:48:51 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:48:51 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:48:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:48:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:48:51 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:48:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:48:52,335 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:48:52,339 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:48:52,600 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:48:52,862 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:48:52,863 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:48:53,389 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:48:53,389 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:48:53,389 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:48:53,389 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:48:53,389 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:48:53,389 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:48:53,390 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:48:53,425 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:48:53,425 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:48:53,464 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:48:56,624 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:48:56,624 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:48:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:48:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:48:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:49:00,357 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:49:00,990 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:49:03,183 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:49:03,184 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:49:03,184 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:49:03,184 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:49:03,184 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:49:03,184 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:49:03,184 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:49:03,185 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:49:03,185 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:49:03,185 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:49:03,185 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:49:03,186 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:49:03,224 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 367.68it/s][INFO|trainer.py:1967] 2023-11-14 23:49:41,720 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5459, 'train_samples_per_second': 4406.693, 'train_steps_per_second': 275.516, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.54it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:49:41,733 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:49:41,734 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:49:47,731 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:49:47,733 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:49:47,733 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.54\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4406.693\n",
            "  train_steps_per_second   =    275.516\n",
            "11/14/2023 23:49:47 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:49:47,747 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:49:47,749 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:49:47,749 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:49:47,749 >>   Batch size = 8\n",
            "100% 280/280 [00:08<00:00, 33.54it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.30it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.74\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    207.963\n",
            "  eval_steps_per_second   =     26.065\n",
            "11/14/2023 23:49:58 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:49:58,494 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:49:58,496 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:49:58,496 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:49:58,496 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.12it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.81\n",
            "  predict_samples_per_second =    206.557\n",
            "  predict_steps_per_second   =     25.889\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:50:09,671 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:50:14.773674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:50:14.773732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:50:14.773762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:50:15.902978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:50:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:50:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-50-18_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:50:18 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:50:19 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:50:19 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:50:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:50:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:50:19 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:50:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:50:20,098 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:50:20,102 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:50:20,362 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:50:20,622 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:50:20,623 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:50:21,147 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:50:21,148 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:50:21,148 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:50:21,148 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:50:21,148 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:50:21,148 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:50:21,149 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:50:21,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:50:21,180 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:50:21,214 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:50:24,329 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:50:24,329 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:50:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:50:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:50:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:50:28,027 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:50:28,658 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:50:30,871 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:50:30,871 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:50:30,871 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:50:30,871 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:50:30,871 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:50:30,871 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:50:30,871 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:50:30,873 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:50:30,873 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:50:30,873 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:50:30,873 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:50:30,873 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:50:30,912 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 367.63it/s][INFO|trainer.py:1967] 2023-11-14 23:51:09,419 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.556, 'train_samples_per_second': 4405.542, 'train_steps_per_second': 275.444, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.47it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:51:09,431 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:51:09,432 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:51:11,951 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:51:11,952 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:51:11,952 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.55\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4405.542\n",
            "  train_steps_per_second   =    275.444\n",
            "11/14/2023 23:51:11 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:51:11,966 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:51:11,969 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:51:11,969 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:51:11,969 >>   Batch size = 8\n",
            " 99% 278/280 [00:08<00:00, 32.38it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 26.18it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.79\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    207.011\n",
            "  eval_steps_per_second   =     25.946\n",
            "11/14/2023 23:51:22 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:51:22,763 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:51:22,765 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:51:22,765 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:51:22,765 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.28it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.75\n",
            "  predict_samples_per_second =    207.773\n",
            "  predict_steps_per_second   =     26.041\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:51:33,893 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:51:39.093792: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:51:39.093845: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:51:39.093893: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:51:40.219594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:51:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:51:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-51-42_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:51:42 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:51:43 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:51:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:51:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:51:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:51:43 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:51:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:51:44,417 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:51:44,421 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:51:44,674 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:51:44,935 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:51:44,936 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:51:45,448 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:51:45,448 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:51:45,448 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:51:45,448 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:51:45,448 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:51:45,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:51:45,449 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:51:45,480 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:51:45,481 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:51:45,515 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:51:48,627 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:51:48,627 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:51:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:51:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:51:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:51:52,370 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:51:53,004 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:51:55,162 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:51:55,162 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:51:55,162 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:51:55,163 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:51:55,163 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:51:55,163 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:51:55,163 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:51:55,164 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:51:55,164 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:51:55,164 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:51:55,164 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:51:55,164 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:51:55,203 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10563/10620 [00:20<00:00, 364.27it/s][INFO|trainer.py:1967] 2023-11-14 23:52:33,502 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.3488, 'train_samples_per_second': 4429.347, 'train_steps_per_second': 276.932, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 276.96it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:52:33,515 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:52:33,516 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:52:35,863 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:52:35,864 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:52:35,865 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.34\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4429.347\n",
            "  train_steps_per_second   =    276.932\n",
            "11/14/2023 23:52:35 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:52:35,878 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:52:35,880 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:52:35,880 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:52:35,880 >>   Batch size = 8\n",
            "100% 279/280 [00:08<00:00, 32.48it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 25.88it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.91\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    204.704\n",
            "  eval_steps_per_second   =     25.657\n",
            "11/14/2023 23:52:46 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:52:46,796 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:52:46,798 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:52:46,798 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:52:46,798 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.22it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.77\n",
            "  predict_samples_per_second =    207.333\n",
            "  predict_steps_per_second   =     25.986\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:52:57,930 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n",
            "2023-11-14 23:53:03.073175: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 23:53:03.073230: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 23:53:03.073266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 23:53:04.212204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 23:53:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 23:53:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov14_23-53-06_60d73189cc46,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/14/2023 23:53:06 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-10500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b9853b025d7a96b2\n",
            "11/14/2023 23:53:07 - INFO - datasets.builder - Using custom data configuration default-b9853b025d7a96b2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 23:53:07 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 23:53:07 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:53:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 23:53:07 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 23:53:07 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:53:08,386 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:53:08,390 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 23:53:08,665 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:53:08,930 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:53:08,931 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:53:09,449 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:53:09,449 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:53:09,449 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:53:09,449 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-14 23:53:09,449 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:53:09,449 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:53:09,450 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-14 23:53:09,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 23:53:09,483 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-14 23:53:09,516 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-14 23:53:12,662 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 23:53:12,662 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "11/14/2023 23:53:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3ced57f699494133.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "11/14/2023 23:53:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6d4ebb38ef00a744.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "11/14/2023 23:53:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b9853b025d7a96b2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee24d527eb5ca333.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-14 23:53:16,614 >> Loading model from /content/test-ner-2022/checkpoint-10500.\n",
            "[INFO|trainer.py:738] 2023-11-14 23:53:17,247 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 23:53:19,448 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 23:53:19,448 >>   Num examples = 5,662\n",
            "[INFO|trainer.py:1726] 2023-11-14 23:53:19,448 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-14 23:53:19,448 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-14 23:53:19,448 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-14 23:53:19,448 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 23:53:19,448 >>   Total optimization steps = 10,620\n",
            "[INFO|trainer.py:1733] 2023-11-14 23:53:19,450 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-14 23:53:19,450 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-14 23:53:19,450 >>   Continuing training from epoch 29\n",
            "[INFO|trainer.py:1755] 2023-11-14 23:53:19,450 >>   Continuing training from global step 10500\n",
            "[INFO|trainer.py:1757] 2023-11-14 23:53:19,450 >>   Will skip the first 29 epochs then the first 234 batches in the first epoch.\n",
            "  0% 0/10620 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-14 23:53:19,489 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 99% 10562/10620 [00:20<00:00, 367.66it/s][INFO|trainer.py:1967] 2023-11-14 23:53:57,994 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 38.5551, 'train_samples_per_second': 4405.641, 'train_steps_per_second': 275.45, 'train_loss': 5.251050290078093e-06, 'epoch': 30.0}\n",
            "100% 10620/10620 [00:38<00:00, 275.48it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-14 23:53:58,007 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 23:53:58,008 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 23:54:00,504 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 23:54:00,505 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 23:54:00,505 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:00:38.55\n",
            "  train_samples            =       5662\n",
            "  train_samples_per_second =   4405.641\n",
            "  train_steps_per_second   =     275.45\n",
            "11/14/2023 23:54:00 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:54:00,519 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:54:00,521 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:54:00,522 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:54:00,522 >>   Batch size = 8\n",
            " 99% 278/280 [00:08<00:00, 32.32it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 280/280 [00:10<00:00, 25.95it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.7096\n",
            "  eval_f1                 =     0.7663\n",
            "  eval_loss               =      2.689\n",
            "  eval_precision          =     0.6926\n",
            "  eval_recall             =     0.8575\n",
            "  eval_runtime            = 0:00:10.88\n",
            "  eval_samples            =       2234\n",
            "  eval_samples_per_second =    205.218\n",
            "  eval_steps_per_second   =     25.721\n",
            "11/14/2023 23:54:11 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-14 23:54:11,410 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-14 23:54:11,412 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-14 23:54:11,412 >>   Num examples = 2234\n",
            "[INFO|trainer.py:3178] 2023-11-14 23:54:11,412 >>   Batch size = 8\n",
            "100% 280/280 [00:10<00:00, 26.26it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7096\n",
            "  predict_f1                 =     0.7663\n",
            "  predict_loss               =      2.689\n",
            "  predict_precision          =     0.6926\n",
            "  predict_recall             =     0.8575\n",
            "  predict_runtime            = 0:00:10.75\n",
            "  predict_samples_per_second =     207.64\n",
            "  predict_steps_per_second   =     26.025\n",
            "[INFO|modelcard.py:452] 2023-11-14 23:54:22,529 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6926384880891756}, {'name': 'Recall', 'type': 'recall', 'value': 0.8575431713895408}, {'name': 'F1', 'type': 'f1', 'value': 0.7663197433288991}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.709564291854864}]}\n",
            "{'preds':                                                   preds\n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E\n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...\n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...\n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...\n",
            "...                                                 ...\n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C\n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C\n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C\n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C\n",
            "\n",
            "[2234 rows x 1 columns]}\n",
            "                                             liststring  \\\n",
            "0               O,O,C,C,C,C,C,C,C,O,O,O,E,E,E,E,E,E,E,E   \n",
            "1     O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,O,O,O,E,E,E,E,O,...   \n",
            "2     C,C,C,C,C,C,O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3     O,O,C,C,C,C,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,O,O,...   \n",
            "4     O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,O,O,...   \n",
            "...                                                 ...   \n",
            "2229      O,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,O,C,C,C,C,C   \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C   \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C   \n",
            "2232                    O,O,E,E,O,O,C,C,C,C,C,O,C,C,C,C   \n",
            "2233  O,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C   \n",
            "\n",
            "                                                  preds  \n",
            "0               C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E  \n",
            "1     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,...  \n",
            "2     E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,C,C,C,C,C,...  \n",
            "3     C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "4     C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,E,E,...  \n",
            "...                                                 ...  \n",
            "2229      E,E,E,E,E,O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "2230                          E,E,E,E,O,O,C,C,C,C,C,C,C  \n",
            "2231                    E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C  \n",
            "2232                    E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C  \n",
            "2233  E,E,E,E,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,C,C,C,C,C,C  \n",
            "\n",
            "[2234 rows x 2 columns]\n",
            "(73283,)\n",
            "(73283,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.70      0.82      0.75     28263\n",
            "           E       0.70      0.86      0.77     28549\n",
            "           O       0.91      0.26      0.41     16471\n",
            "\n",
            "    accuracy                           0.71     73283\n",
            "   macro avg       0.77      0.65      0.64     73283\n",
            "weighted avg       0.74      0.71      0.68     73283\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######Organizational data as train and test##############################"
      ],
      "metadata": {
        "id": "3CfcTd_QHRb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# data location\n",
        "#parent_dir = \"/content/drive/MyDrive\"\n",
        "parent_dir = \"/content/drive/MyDrive/\"\n",
        "mk_data_path = lambda filename: os.path.join(parent_dir, filename)\n",
        "#data_path = f\"{parent_dir}/BERT_data_final.xlsx\"\n",
        "data_path = mk_data_path(\"BERT_data_final1.xlsx\")\n",
        "\n",
        "# once finished, flush changes to files to make them visible (assuming we write to Google Drive)\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzLk6rDsHWZP",
        "outputId": "5208bc53-cd4d-4c5b-d784-47168a479c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel(data_path)\n",
        "data.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "WUMXeDL_IJC-",
        "outputId": "c6d35c25-e276-4c77-c915-007a9078ea3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                Causal-relation text  \\\n",
              "0  [\"When \",\"a \",\"policyholder \",\"or \",\"insured \"...   \n",
              "1  [\"During \",\"2020 \",\"in \",\"response \",\"to \",\"th...   \n",
              "2  [\"Prolonged \",\"periods \",\"of \",\"low \",\"interes...   \n",
              "3  [\"Conversely \",\"a \",\"rise \",\"in \",\"interest \",...   \n",
              "\n",
              "                                            BIO Code  \\\n",
              "0  [\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"...   \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"...   \n",
              "2  [\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT...   \n",
              "3  [\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT\",\"O...   \n",
              "\n",
              "       {\"words\":Causal-relation text,\"ner\":BIO Code}  \\\n",
              "0  {\"words\":[\"When \",\"a \",\"policyholder \",\"or \",\"...   \n",
              "1  {\"words\":[\"During \",\"2020 \",\"in \",\"response \",...   \n",
              "2  {\"words\":[\"Prolonged \",\"periods \",\"of \",\"low \"...   \n",
              "3  {\"words\":[\"Conversely \",\"a \",\"rise \",\"in \",\"in...   \n",
              "\n",
              "                                    BIO_Code_changed  \\\n",
              "0  [\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"...   \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"...   \n",
              "2  [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"E\",\"E\",\"O\",...   \n",
              "3  [\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"O\",\"O\",\"O\",...   \n",
              "\n",
              "                                BIO Code-changed-NER  \n",
              "0  [\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...  \n",
              "1  [\"O\",\"O\",\"O\",\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-...  \n",
              "2  [\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"...  \n",
              "3  [\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80e86c1e-ca9c-459f-a82f-d6d8928c9928\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Causal-relation text</th>\n",
              "      <th>BIO Code</th>\n",
              "      <th>{\"words\":Causal-relation text,\"ner\":BIO Code}</th>\n",
              "      <th>BIO_Code_changed</th>\n",
              "      <th>BIO Code-changed-NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"When \",\"a \",\"policyholder \",\"or \",\"insured \"...</td>\n",
              "      <td>[\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"...</td>\n",
              "      <td>{\"words\":[\"When \",\"a \",\"policyholder \",\"or \",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"O\",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"During \",\"2020 \",\"in \",\"response \",\"to \",\"th...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"...</td>\n",
              "      <td>{\"words\":[\"During \",\"2020 \",\"in \",\"response \",...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"...</td>\n",
              "      <td>[\"O\",\"O\",\"O\",\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"Prolonged \",\"periods \",\"of \",\"low \",\"interes...</td>\n",
              "      <td>[\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT...</td>\n",
              "      <td>{\"words\":[\"Prolonged \",\"periods \",\"of \",\"low \"...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"E\",\"E\",\"O\",...</td>\n",
              "      <td>[\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"Conversely \",\"a \",\"rise \",\"in \",\"interest \",...</td>\n",
              "      <td>[\"O\",\"O\",\"B-C\",\"I-C\",\"I-C\",\"I-C\",\"O\",\"B-CT\",\"O...</td>\n",
              "      <td>{\"words\":[\"Conversely \",\"a \",\"rise \",\"in \",\"in...</td>\n",
              "      <td>[\"O\",\"O\",\"C\",\"C\",\"C\",\"C\",\"O\",\"CT\",\"O\",\"O\",\"O\",...</td>\n",
              "      <td>[\"O\",\"O\",\"B-ENTITY\",\"B-ENTITY\",\"B-ENTITY\",\"B-E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80e86c1e-ca9c-459f-a82f-d6d8928c9928')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80e86c1e-ca9c-459f-a82f-d6d8928c9928 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80e86c1e-ca9c-459f-a82f-d6d8928c9928');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8aa3725b-4e8c-4a2d-aac9-2a6ea1fd25c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8aa3725b-4e8c-4a2d-aac9-2a6ea1fd25c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8aa3725b-4e8c-4a2d-aac9-2a6ea1fd25c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset = data[[\"Causal-relation text\", \"BIO_Code_changed\"]].rename(columns={\"Causal-relation text\": \"X\", \"BIO_Code_changed\": \"y\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw, y_raw = [json.loads(tokens) for tokens in dataset.X.values], [json.loads(labels) for labels in dataset.y.values]"
      ],
      "metadata": {
        "id": "8pkUZSDkIdPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(y_raw):\n",
        "    for j, a in enumerate(x):\n",
        "        if 'CT' in a:\n",
        "            y_raw[i][j] = a.replace('CT', 'O')"
      ],
      "metadata": {
        "id": "LwBsuiEBIRmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# RANDOM_SEED = 42\n",
        "# # Our data is split into sentences\n",
        "\n",
        "\n",
        "# # FIXME: we also need test data!\n",
        "# X_train_raw, X_dev_raw, y_train_raw, y_dev_raw = train_test_split(X_raw, y_raw, test_size=.2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "lgpeqMVDIgsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# X_train_final = np.array(X_train_raw)\n",
        "# y_train_final = np.array(y_train_raw)"
      ],
      "metadata": {
        "id": "jS_WVZn0I0Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from sklearn.model_selection import KFold\n",
        "# k=2\n",
        "# seed = 1\n",
        "# kf = KFold(n_splits=k,random_state=seed, shuffle=True)\n",
        "# metrics=[]\n",
        "# i=0\n",
        "# for train_index, val_index in kf.split(X_train_final,y_train_final):\n",
        "#   train_texts, test_texts =X_train_final[train_index].tolist(),X_train_final[val_index].tolist()\n",
        "#   train_labels, test_labels = y_train_final[train_index].tolist(),y_train_final[val_index].tolist()"
      ],
      "metadata": {
        "id": "N-dDCPkUIkq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# X_train_final = np.array(X_raw)\n",
        "# y_train_final = np.array(y_raw)"
      ],
      "metadata": {
        "id": "uRroUNG1IckW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_raw,y_raw):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "xnzlL58MJGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(train,  '/content/train.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZTX9DbGJh1S",
        "outputId": "d83a1182-268e-450c-c256-f2a8d1953d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCvCH7TuKyEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_dev_raw,y_dev_raw):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "jNqtkeTbKGHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(dev,  '/content/dev.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTnCtObmKK6A",
        "outputId": "c08365a8-103f-4950-b3c7-497dca2b483b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(dev,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXQjk_L9KLHy",
        "outputId": "f3ee07db-7b07-47db-fb62-01555630f78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)\n",
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "2wrUoo9gKV9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKeHIQBsJh8m",
        "outputId": "a225031a-6aa6-4daa-dc4b-70286e1aa621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-15 15:06:56.959877: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 15:06:56.959929: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 15:06:56.959973: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 15:06:58.013272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 15:07:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 15:07:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_15-07-00_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 15:07:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 25731.93it/s]\n",
            "Downloading took 0.0 min\n",
            "11/15/2023 15:07:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/15/2023 15:07:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2451.38it/s]\n",
            "Generating train split\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 1787 examples [00:00, 73699.32 examples/s]\n",
            "Generating validation split\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 447 examples [00:00, 101927.47 examples/s]\n",
            "Generating test split\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 447 examples [00:00, 65453.63 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/15/2023 15:07:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/15/2023 15:07:01 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "(…)ert-large-cased/resolve/main/config.json: 100% 414/414 [00:00<00:00, 2.21MB/s]\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:07:02,523 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:07:02,527 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 15:07:02,790 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:07:03,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:07:03,056 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "(…)nbert-large-cased/resolve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 28.5MB/s]\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:07:04,718 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:07:04,718 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:07:04,718 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:07:04,718 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:07:04,718 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:07:04,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:07:04,719 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:07:04,751 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:07:04,752 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 665M/665M [00:02<00:00, 248MB/s]\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 15:07:08,412 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 15:07:11,428 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 15:07:11,428 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1787 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 15:07:11,476 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 15:07:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Running tokenizer on train dataset: 100% 1787/1787 [00:00<00:00, 5870.71 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/447 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49b70748391be4a8.arrow\n",
            "11/15/2023 15:07:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49b70748391be4a8.arrow\n",
            "Running tokenizer on validation dataset: 100% 447/447 [00:00<00:00, 6651.91 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/447 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ef2ab616df33a031.arrow\n",
            "11/15/2023 15:07:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ef2ab616df33a031.arrow\n",
            "Running tokenizer on prediction dataset: 100% 447/447 [00:00<00:00, 6779.07 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 25.7MB/s]\n",
            "[INFO|trainer.py:738] 2023-11-15 15:07:19,218 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 15:07:19,244 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 15:07:19,244 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 15:07:19,244 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 15:07:19,244 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 15:07:19,244 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 15:07:19,244 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 15:07:19,244 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 15:07:19,246 >>   Number of trainable parameters = 332,533,764\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 15:07:19,261 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.4573, 'learning_rate': 4.255952380952381e-05, 'epoch': 4.46}\n",
            " 15% 500/3360 [06:30<33:17,  1.43it/s][INFO|trainer.py:2896] 2023-11-15 15:13:49,415 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:13:49,416 >> Configuration saved in /content/test-ner-2022/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:13:52,066 >> Model weights saved in /content/test-ner-2022/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:13:52,067 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:13:52,067 >> Special tokens file saved in /content/test-ner-2022/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.1865, 'learning_rate': 3.511904761904762e-05, 'epoch': 8.93}\n",
            " 30% 1000/3360 [13:16<36:42,  1.07it/s][INFO|trainer.py:2896] 2023-11-15 15:20:35,872 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:20:35,873 >> Configuration saved in /content/test-ner-2022/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:20:38,397 >> Model weights saved in /content/test-ner-2022/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:20:38,398 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:20:38,398 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.078, 'learning_rate': 2.767857142857143e-05, 'epoch': 13.39}\n",
            " 45% 1500/3360 [20:01<24:43,  1.25it/s][INFO|trainer.py:2896] 2023-11-15 15:27:20,261 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:27:20,262 >> Configuration saved in /content/test-ner-2022/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:27:22,773 >> Model weights saved in /content/test-ner-2022/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:27:22,774 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:27:22,774 >> Special tokens file saved in /content/test-ner-2022/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.0334, 'learning_rate': 2.023809523809524e-05, 'epoch': 17.86}\n",
            " 60% 2000/3360 [26:48<17:22,  1.30it/s][INFO|trainer.py:2896] 2023-11-15 15:34:07,711 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-2000\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:34:07,712 >> Configuration saved in /content/test-ner-2022/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:34:10,399 >> Model weights saved in /content/test-ner-2022/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:34:10,400 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:34:10,400 >> Special tokens file saved in /content/test-ner-2022/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.0178, 'learning_rate': 1.2797619047619047e-05, 'epoch': 22.32}\n",
            " 74% 2500/3360 [33:32<10:34,  1.36it/s][INFO|trainer.py:2896] 2023-11-15 15:40:51,973 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-2500\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:40:51,974 >> Configuration saved in /content/test-ner-2022/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:40:54,607 >> Model weights saved in /content/test-ner-2022/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:40:54,608 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:40:54,608 >> Special tokens file saved in /content/test-ner-2022/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.011, 'learning_rate': 5.357142857142857e-06, 'epoch': 26.79}\n",
            " 89% 3000/3360 [40:20<04:19,  1.39it/s][INFO|trainer.py:2896] 2023-11-15 15:47:39,388 >> Saving model checkpoint to /content/test-ner-2022/checkpoint-3000\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:47:39,389 >> Configuration saved in /content/test-ner-2022/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:47:42,103 >> Model weights saved in /content/test-ner-2022/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:47:42,104 >> tokenizer config file saved in /content/test-ner-2022/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:47:42,104 >> Special tokens file saved in /content/test-ner-2022/checkpoint-3000/special_tokens_map.json\n",
            "100% 3360/3360 [45:15<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 15:52:34,825 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2715.6844, 'train_samples_per_second': 19.741, 'train_steps_per_second': 1.237, 'train_loss': 0.11759778403100513, 'epoch': 30.0}\n",
            "100% 3360/3360 [45:15<00:00,  1.24it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 15:52:34,936 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:52:34,937 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:52:37,551 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:52:37,552 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:52:37,553 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.1176\n",
            "  train_runtime            = 0:45:15.68\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =     19.741\n",
            "  train_steps_per_second   =      1.237\n",
            "11/15/2023 15:52:37 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 15:52:37,565 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 15:52:37,567 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 15:52:37,567 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 15:52:37,567 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  9.11it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.44it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9031\n",
            "  eval_loss               =     0.8952\n",
            "  eval_precision          =     0.8983\n",
            "  eval_recall             =      0.908\n",
            "  eval_runtime            = 0:00:06.84\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.336\n",
            "  eval_steps_per_second   =      8.185\n",
            "11/15/2023 15:52:44 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 15:52:44,412 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 15:52:44,414 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 15:52:44,414 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 15:52:44,414 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.38it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9031\n",
            "  predict_loss               =     0.8952\n",
            "  predict_precision          =     0.8983\n",
            "  predict_recall             =      0.908\n",
            "  predict_runtime            = 0:00:06.82\n",
            "  predict_samples_per_second =     65.467\n",
            "  predict_steps_per_second   =      8.202\n",
            "[INFO|modelcard.py:452] 2023-11-15 15:52:51,603 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.8982915535685774}, {'name': 'Recall', 'type': 'recall', 'value': 0.908005164622337}, {'name': 'F1', 'type': 'f1', 'value': 0.903122240950317}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.849658155434631}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.90      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 15:52:58.123341: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 15:52:58.123401: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 15:52:58.123432: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 15:52:59.174653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 15:53:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 15:53:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_15-53-01_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 15:53:02 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 15:53:03 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 15:53:03 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 15:53:03 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 15:53:03 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 15:53:03 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 15:53:03 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:53:03,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:53:03,657 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 15:53:03,915 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:53:04,177 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:53:04,178 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:53:04,703 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:53:04,703 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:53:04,703 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:53:04,703 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:53:04,704 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:53:04,704 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:53:04,704 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:53:04,737 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:53:04,737 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 15:53:04,828 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 15:53:10,249 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 15:53:10,250 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 15:53:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/447 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 15:53:10,313 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 15:53:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Running tokenizer on validation dataset: 100% 447/447 [00:00<00:00, 5485.59 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ef2ab616df33a031.arrow\n",
            "11/15/2023 15:53:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ef2ab616df33a031.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 15:53:14,089 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 15:53:14,753 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 15:53:16,947 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 15:53:16,947 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 15:53:16,947 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 15:53:16,947 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 15:53:16,947 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 15:53:16,947 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 15:53:16,947 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 15:53:16,949 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 15:53:16,949 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 15:53:16,949 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 15:53:16,949 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 15:53:16,949 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 15:53:16,965 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 15:58:03,253 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.4088, 'train_samples_per_second': 187.18, 'train_steps_per_second': 11.731, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 15:58:03,360 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 15:58:03,360 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 15:58:06,128 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 15:58:06,129 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 15:58:06,129 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.40\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =     187.18\n",
            "  train_steps_per_second   =     11.731\n",
            "11/15/2023 15:58:06 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 15:58:06,142 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 15:58:06,144 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 15:58:06,144 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 15:58:06,144 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  9.03it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.41it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.84\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.348\n",
            "  eval_steps_per_second   =      8.187\n",
            "11/15/2023 15:58:12 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 15:58:12,987 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 15:58:12,989 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 15:58:12,989 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 15:58:12,989 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.36it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.84\n",
            "  predict_samples_per_second =     65.316\n",
            "  predict_steps_per_second   =      8.183\n",
            "[INFO|modelcard.py:452] 2023-11-15 15:58:20,177 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 15:58:24.726839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 15:58:24.726891: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 15:58:24.726919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 15:58:25.788083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 15:58:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 15:58:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_15-58-27_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 15:58:28 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 15:58:29 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 15:58:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 15:58:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 15:58:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 15:58:29 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 15:58:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:58:29,597 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:58:29,601 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 15:58:29,860 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:58:30,123 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:58:30,124 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:58:30,646 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:58:30,646 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:58:30,646 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:58:30,646 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 15:58:30,646 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:58:30,647 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:58:30,647 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 15:58:30,677 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 15:58:30,677 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 15:58:30,708 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 15:58:33,627 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 15:58:33,627 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 15:58:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 15:58:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/447 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 15:58:33,704 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 15:58:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "Running tokenizer on prediction dataset: 100% 447/447 [00:00<00:00, 5974.03 examples/s]\n",
            "[INFO|trainer.py:2078] 2023-11-15 15:58:37,353 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 15:58:38,062 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 15:58:40,239 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 15:58:40,239 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 15:58:40,239 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 15:58:40,239 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 15:58:40,239 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 15:58:40,239 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 15:58:40,239 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 15:58:40,241 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 15:58:40,241 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 15:58:40,241 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 15:58:40,241 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 15:58:40,241 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 15:58:40,257 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:03:26,757 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.6218, 'train_samples_per_second': 187.041, 'train_steps_per_second': 11.723, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:03:26,864 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:03:26,865 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:03:29,583 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:03:29,584 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:03:29,584 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.62\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.041\n",
            "  train_steps_per_second   =     11.723\n",
            "11/15/2023 16:03:29 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:03:29,597 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:03:29,599 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:03:29,599 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:03:29,599 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  8.94it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.41it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.85\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.228\n",
            "  eval_steps_per_second   =      8.172\n",
            "11/15/2023 16:03:36 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:03:36,454 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:03:36,456 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:03:36,456 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:03:36,456 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.35it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.84\n",
            "  predict_samples_per_second =     65.256\n",
            "  predict_steps_per_second   =      8.175\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:03:43,681 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:03:48.060102: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:03:48.060146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:03:48.060172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:03:49.110366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:03:51 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:03:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-03-51_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:03:51 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:03:52 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:03:52 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:03:52 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:03:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:03:52 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:03:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:03:52,988 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:03:52,992 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:03:53,250 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:03:53,510 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:03:53,512 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:03:54,030 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:03:54,030 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:03:54,030 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:03:54,030 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:03:54,030 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:03:54,031 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:03:54,032 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:03:54,062 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:03:54,063 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:03:54,094 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:03:57,071 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:03:57,071 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:03:57 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:03:57 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:03:57 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:04:00,840 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:04:01,488 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:04:03,633 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:04:03,633 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:04:03,633 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:04:03,633 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:04:03,633 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:04:03,633 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:04:03,633 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:04:03,635 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:04:03,635 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:04:03,635 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:04:03,635 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:04:03,635 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:04:03,651 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:08:50,300 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.771, 'train_samples_per_second': 186.944, 'train_steps_per_second': 11.717, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:08:50,408 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:08:50,409 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:08:53,203 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:08:53,204 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:08:53,205 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.77\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    186.944\n",
            "  train_steps_per_second   =     11.717\n",
            "11/15/2023 16:08:53 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:08:53,219 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:08:53,221 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:08:53,221 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:08:53,221 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  9.11it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.46it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.80\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.677\n",
            "  eval_steps_per_second   =      8.228\n",
            "11/15/2023 16:09:00 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:09:00,029 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:09:00,031 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:09:00,031 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:09:00,031 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.38it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.82\n",
            "  predict_samples_per_second =     65.482\n",
            "  predict_steps_per_second   =      8.204\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:09:07,213 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:09:11.779148: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:09:11.779194: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:09:11.779227: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:09:12.870642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:09:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:09:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-09-14_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:09:15 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:09:16 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:09:16 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:09:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:09:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:09:16 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:09:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:09:16,795 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:09:16,798 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:09:17,084 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:09:17,396 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:09:17,397 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:09:18,035 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:09:18,035 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:09:18,035 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:09:18,035 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:09:18,035 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:09:18,035 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:09:18,036 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:09:18,067 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:09:18,068 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:09:18,099 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:09:21,033 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:09:21,033 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:09:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:09:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:09:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:09:24,762 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:09:25,437 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:09:27,574 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:09:27,574 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:09:27,574 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:09:27,574 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:09:27,574 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:09:27,574 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:09:27,574 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:09:27,682 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:09:27,683 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:09:27,683 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:09:27,683 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:09:27,683 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:09:27,703 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:14:14,552 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.9738, 'train_samples_per_second': 186.812, 'train_steps_per_second': 11.708, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.71it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:14:14,659 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:14:14,660 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:14:17,608 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:14:17,609 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:14:17,609 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.97\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    186.812\n",
            "  train_steps_per_second   =     11.708\n",
            "11/15/2023 16:14:17 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:14:17,623 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:14:17,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:14:17,625 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:14:17,625 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  8.92it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.44it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.81\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.595\n",
            "  eval_steps_per_second   =      8.218\n",
            "11/15/2023 16:14:24 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:14:24,442 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:14:24,444 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:14:24,444 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:14:24,444 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.35it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.84\n",
            "  predict_samples_per_second =     65.309\n",
            "  predict_steps_per_second   =      8.182\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:14:31,659 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:14:36.314588: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:14:36.314637: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:14:36.314664: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:14:37.404021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:14:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:14:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-14-39_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:14:39 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:14:41 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:14:41 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:14:41 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:14:41 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:14:41 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:14:41 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:14:41,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:14:41,413 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:14:41,675 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:14:41,924 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:14:41,925 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:14:42,446 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:14:42,446 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:14:42,446 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:14:42,446 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:14:42,446 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:14:42,447 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:14:42,447 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:14:42,479 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:14:42,480 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:14:42,512 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:14:45,603 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:14:45,604 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:14:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:14:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:14:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:14:49,271 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:14:49,960 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:14:52,131 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:14:52,131 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:14:52,131 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:14:52,131 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:14:52,131 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:14:52,131 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:14:52,131 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:14:52,133 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:14:52,133 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:14:52,133 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:14:52,133 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:14:52,133 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:14:52,151 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.34it/s][INFO|trainer.py:1967] 2023-11-15 16:19:38,484 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.4562, 'train_samples_per_second': 187.149, 'train_steps_per_second': 11.73, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:19:38,591 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:19:38,592 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:19:41,382 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:19:41,383 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:19:41,383 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.45\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.149\n",
            "  train_steps_per_second   =      11.73\n",
            "11/15/2023 16:19:41 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:19:41,397 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:19:41,398 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:19:41,399 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:19:41,399 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  8.94it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.37it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.87\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     64.981\n",
            "  eval_steps_per_second   =      8.141\n",
            "11/15/2023 16:19:48 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:19:48,280 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:19:48,282 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:19:48,282 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:19:48,282 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.37it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.83\n",
            "  predict_samples_per_second =     65.422\n",
            "  predict_steps_per_second   =      8.196\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:19:55,462 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:20:00.027223: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:20:00.027270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:20:00.027296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:20:01.109480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:20:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:20:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-20-03_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:20:03 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:20:04 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:20:04 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:20:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:20:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:20:04 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:20:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:20:05,062 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:20:05,066 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:20:05,330 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:20:05,592 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:20:05,593 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:20:06,124 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:20:06,124 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:20:06,124 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:20:06,124 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:20:06,124 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:20:06,124 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:20:06,125 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:20:06,158 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:20:06,159 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:20:06,193 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:20:09,282 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:20:09,282 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:20:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:20:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:20:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:20:13,025 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:20:13,695 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:20:15,858 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:20:15,858 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:20:15,858 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:20:15,858 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:20:15,858 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:20:15,858 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:20:15,858 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:20:15,860 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:20:15,860 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:20:15,860 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:20:15,860 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:20:15,860 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:20:15,876 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:25:02,301 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.5462, 'train_samples_per_second': 187.09, 'train_steps_per_second': 11.726, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:25:02,408 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:25:02,409 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:25:07,418 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:25:07,419 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:25:07,420 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.54\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =     187.09\n",
            "  train_steps_per_second   =     11.726\n",
            "11/15/2023 16:25:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:25:07,433 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:25:07,435 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:25:07,435 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:25:07,435 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  9.03it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.47it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.80\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.713\n",
            "  eval_steps_per_second   =      8.233\n",
            "11/15/2023 16:25:14 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:25:14,240 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:25:14,242 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:25:14,242 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:25:14,242 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.40it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.80\n",
            "  predict_samples_per_second =      65.68\n",
            "  predict_steps_per_second   =      8.228\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:25:21,425 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:25:26.062188: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:25:26.062243: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:25:26.062273: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:25:27.190491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:25:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:25:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-25-29_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:25:29 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:25:30 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:25:30 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:25:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:25:30 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:25:30 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:25:30 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:25:31,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:25:31,184 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:25:31,437 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:25:31,689 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:25:31,690 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:25:32,207 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:25:32,207 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:25:32,207 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:25:32,207 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:25:32,207 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:25:32,207 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:25:32,208 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:25:32,240 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:25:32,241 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:25:32,273 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:25:35,268 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:25:35,268 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:25:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:25:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:25:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:25:38,971 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:25:39,626 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:25:41,817 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:25:41,817 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:25:41,817 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:25:41,817 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:25:41,817 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:25:41,817 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:25:41,817 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:25:41,819 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:25:41,819 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:25:41,819 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:25:41,819 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:25:41,819 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:25:41,838 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:30:28,657 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.9421, 'train_samples_per_second': 186.832, 'train_steps_per_second': 11.71, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.71it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:30:28,763 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:30:28,764 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:30:31,563 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:30:31,564 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:30:31,564 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.94\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    186.832\n",
            "  train_steps_per_second   =      11.71\n",
            "11/15/2023 16:30:31 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:30:31,578 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:30:31,580 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:30:31,580 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:30:31,580 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  9.02it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.42it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.83\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.351\n",
            "  eval_steps_per_second   =      8.187\n",
            "11/15/2023 16:30:38 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:30:38,422 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:30:38,424 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:30:38,424 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:30:38,424 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.37it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.83\n",
            "  predict_samples_per_second =      65.39\n",
            "  predict_steps_per_second   =      8.192\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:30:45,616 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:30:50.302660: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:30:50.302709: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:30:50.302737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:30:51.425247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:30:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:30:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-30-53_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:30:53 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:30:54 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:30:54 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:30:54 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:30:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:30:54 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:30:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:30:55,285 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:30:55,288 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:30:55,571 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:30:55,898 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:30:55,899 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:30:56,419 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:30:56,419 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:30:56,419 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:30:56,419 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:30:56,419 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:30:56,419 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:30:56,420 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:30:56,451 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:30:56,452 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:30:56,483 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:30:59,569 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:30:59,570 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:30:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:30:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:30:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:31:03,434 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:31:04,135 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:31:06,337 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:31:06,337 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:31:06,338 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:31:06,338 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:31:06,338 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:31:06,338 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:31:06,338 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:31:06,339 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:31:06,339 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:31:06,340 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:31:06,340 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:31:06,340 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:31:06,360 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.34it/s][INFO|trainer.py:1967] 2023-11-15 16:35:53,258 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 287.0242, 'train_samples_per_second': 186.779, 'train_steps_per_second': 11.706, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:47<00:00, 11.71it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:35:53,366 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:35:53,367 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:35:56,358 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:35:56,359 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:35:56,360 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:47.02\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    186.779\n",
            "  train_steps_per_second   =     11.706\n",
            "11/15/2023 16:35:56 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:35:56,373 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:35:56,375 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:35:56,376 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:35:56,376 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  9.03it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.40it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.85\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.196\n",
            "  eval_steps_per_second   =      8.168\n",
            "11/15/2023 16:36:03 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:36:03,234 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:36:03,236 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:36:03,236 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:36:03,236 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.35it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.84\n",
            "  predict_samples_per_second =     65.305\n",
            "  predict_steps_per_second   =      8.181\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:36:10,451 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n",
            "2023-11-15 16:36:15.044124: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:36:15.044172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:36:15.044213: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:36:16.138921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:36:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:36:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-36-18_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:36:18 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-b1927ef06d83b4ba\n",
            "11/15/2023 16:36:19 - INFO - datasets.builder - Using custom data configuration default-b1927ef06d83b4ba\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:36:19 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:36:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:36:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:36:19 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:36:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:36:20,019 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:36:20,023 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:36:20,286 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:36:20,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:36:20,539 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:36:21,118 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:36:21,118 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:36:21,118 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:36:21,118 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:36:21,118 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:36:21,119 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:36:21,119 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:36:21,151 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:36:21,152 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:36:21,184 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:36:24,259 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:36:24,260 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "11/15/2023 16:36:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-49641c6dbcd51e27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "11/15/2023 16:36:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43f25ae97dddcb27.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "11/15/2023 16:36:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b1927ef06d83b4ba/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-19b0cfc7a5ca13b6.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:36:28,086 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:36:28,756 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:36:30,922 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:36:30,922 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:36:30,922 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:36:30,922 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:36:30,922 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:36:30,922 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:36:30,922 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:36:30,924 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:36:30,924 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:36:30,924 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:36:30,924 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:36:30,924 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:36:30,941 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:41:17,419 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.6, 'train_samples_per_second': 187.055, 'train_steps_per_second': 11.724, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:41:17,526 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:41:17,527 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:41:22,684 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:41:22,685 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:41:22,685 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.60\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.055\n",
            "  train_steps_per_second   =     11.724\n",
            "11/15/2023 16:41:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:41:22,699 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:41:22,701 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:41:22,701 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:41:22,701 >>   Batch size = 8\n",
            " 98% 55/56 [00:06<00:00,  8.90it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 56/56 [00:06<00:00,  8.41it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.8497\n",
            "  eval_f1                 =     0.9046\n",
            "  eval_loss               =     0.8908\n",
            "  eval_precision          =     0.8982\n",
            "  eval_recall             =     0.9111\n",
            "  eval_runtime            = 0:00:06.85\n",
            "  eval_samples            =        447\n",
            "  eval_samples_per_second =     65.241\n",
            "  eval_steps_per_second   =      8.173\n",
            "11/15/2023 16:41:29 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:41:29,556 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:41:29,558 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:41:29,558 >>   Num examples = 447\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:41:29,558 >>   Batch size = 8\n",
            "100% 56/56 [00:06<00:00,  8.36it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.8497\n",
            "  predict_f1                 =     0.9046\n",
            "  predict_loss               =     0.8908\n",
            "  predict_precision          =     0.8982\n",
            "  predict_recall             =     0.9111\n",
            "  predict_runtime            = 0:00:06.83\n",
            "  predict_samples_per_second =     65.394\n",
            "  predict_steps_per_second   =      8.193\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:41:36,755 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.898170246618934}, {'name': 'Recall', 'type': 'recall', 'value': 0.9110716591349257}, {'name': 'F1', 'type': 'f1', 'value': 0.9045749539299736}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497279196316451}]}\n",
            "{'preds':                                                  preds\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...\n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...\n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C\n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "..                                                 ...\n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...\n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...\n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...\n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[447 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...   \n",
            "2    O,O,O,O,O,C,C,C,C,O,O,O,O,O,O,O,CT,CT,CT,E,E,E...   \n",
            "3            O,O,E,E,E,E,CT,CT,O,C,C,C,C,C,C,C,C,C,C,C   \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "..                                                 ...   \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...   \n",
            "443  E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...   \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,O,C,C,C,...   \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "1    CT,CT,CT,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C...  \n",
            "2    O,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,CT,CT,CT,E,E,O...  \n",
            "3            O,O,E,E,E,E,CT,CT,C,C,C,C,C,C,C,C,C,C,C,C  \n",
            "4    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "..                                                 ...  \n",
            "442  C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,CT,CT,...  \n",
            "443  O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,CT,CT,C,C,C,...  \n",
            "444  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "445  O,O,O,O,O,O,O,O,O,O,O,O,C,C,C,C,C,C,C,C,C,O,C,...  \n",
            "446  O,CT,CT,O,C,C,C,C,C,O,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[447 rows x 2 columns]\n",
            "(14334,)\n",
            "(14334,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.87      0.89      0.88      5694\n",
            "          CT       0.97      0.97      0.97       871\n",
            "           E       0.90      0.91      0.91      5662\n",
            "           O       0.58      0.53      0.55      2107\n",
            "\n",
            "    accuracy                           0.85     14334\n",
            "   macro avg       0.83      0.83      0.83     14334\n",
            "weighted avg       0.85      0.85      0.85     14334\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############Organizational as train and SCITE as test##############################"
      ],
      "metadata": {
        "id": "l9V8tl_W4fgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_excel('/content/test_data.xlsx',sheet_name = \"Sheet3\")\n",
        "data_test.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "s8hCFvWf4lzz",
        "outputId": "616a59f8-6c60-43ce-91d8-deb037d58058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentences  \\\n",
              "0  [\"Various\",\"hormonal\",\",\",\"bacterial\",\"and\",\"i...   \n",
              "1  [\"A\",\"stereo\",\"buss\",\"outputs\",\"the\",\"stereo\",...   \n",
              "2  [\"The\",\"incoming\",\"water\",\"caused\",\"a\",\"stain\"...   \n",
              "3  [\"The\",\"genreal\",\"anesthetic\",\"cause\",\"unconsc...   \n",
              "\n",
              "                                           Bio_label  \n",
              "0              [\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\"]  \n",
              "1  [\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"E\",\"E\",\"O\",\"C\",\"C\",\"...  \n",
              "2  [\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\",\"...  \n",
              "3  [\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b5804a3-dc01-431b-a1c6-478363d74bc4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>Bio_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[\"Various\",\"hormonal\",\",\",\"bacterial\",\"and\",\"i...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"O\",\"E\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\"A\",\"stereo\",\"buss\",\"outputs\",\"the\",\"stereo\",...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"E\",\"E\",\"O\",\"C\",\"C\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[\"The\",\"incoming\",\"water\",\"caused\",\"a\",\"stain\"...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"E\",\"O\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[\"The\",\"genreal\",\"anesthetic\",\"cause\",\"unconsc...</td>\n",
              "      <td>[\"C\",\"C\",\"C\",\"O\",\"E\",\"O\",\"E\",\"O\",\"O\",\"O\",\"O\",\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b5804a3-dc01-431b-a1c6-478363d74bc4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7b5804a3-dc01-431b-a1c6-478363d74bc4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7b5804a3-dc01-431b-a1c6-478363d74bc4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fec79881-6482-494f-a5c1-617a7c110e27\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fec79881-6482-494f-a5c1-617a7c110e27')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fec79881-6482-494f-a5c1-617a7c110e27 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_test[[\"sentences\", \"Bio_label\"]].rename(columns={\"sentences\": \"A\", \"Bio_label\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "X_raw_test, y_raw_test = [json.loads(tokens_test) for tokens_test in dataset_test.A.values], [json.loads(labels_test) for labels_test in dataset_test.B.values]"
      ],
      "metadata": {
        "id": "ND-18Q8a5C8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, x in enumerate(y_raw_test):\n",
        "    for j, a in enumerate(x):\n",
        "        if 'CE' in a:\n",
        "            y_raw_test[i][j] = a.replace('CE', 'CT')"
      ],
      "metadata": {
        "id": "-6SoBu9q5C_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners = []\n",
        "for i,j in zip(X_raw_test,y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners.append(_new_ner)"
      ],
      "metadata": {
        "id": "u8_AZn2_5DC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/dev.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQJ8od305MJI",
        "outputId": "24f612fc-8327-4525-fcf2-134a0091a6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners[i] for i in range(len(new_ners))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QgYe5_c5DGt",
        "outputId": "d146dae1-4bc8-43c7-dcb5-0095aa3a0425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "C0CsvPor5U_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "tghdECZY5Qgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzQNiIdR5DPD",
        "outputId": "f119fbe6-06b8-4509-f0fa-7824caa4073e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-15 16:52:04.673002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:52:04.673052: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:52:04.673082: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:52:05.829841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:52:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:52:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-52-07_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:52:08 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:52:09 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 25523.15it/s]\n",
            "Downloading took 0.0 min\n",
            "11/15/2023 16:52:09 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/15/2023 16:52:09 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1143.38it/s]\n",
            "Generating train split\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 1787 examples [00:00, 108690.98 examples/s]\n",
            "Generating validation split\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 191 examples [00:00, 86569.27 examples/s]\n",
            "Generating test split\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 191 examples [00:00, 96740.98 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/15/2023 16:52:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/15/2023 16:52:09 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:52:09,804 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:52:09,808 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:52:10,061 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:52:10,519 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:52:10,520 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:52:11,037 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:52:11,037 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:52:11,037 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:52:11,037 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:52:11,037 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:52:11,037 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:52:11,038 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:52:11,071 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:52:11,072 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:52:11,105 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:52:14,185 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:52:14,185 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/1787 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 16:52:14,237 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 16:52:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Running tokenizer on train dataset: 100% 1787/1787 [00:00<00:00, 6408.72 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/191 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0234bbed1ff2aa4e.arrow\n",
            "11/15/2023 16:52:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0234bbed1ff2aa4e.arrow\n",
            "Running tokenizer on validation dataset: 100% 191/191 [00:00<00:00, 8576.12 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/191 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b65598bffd965cc.arrow\n",
            "11/15/2023 16:52:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b65598bffd965cc.arrow\n",
            "Running tokenizer on prediction dataset: 100% 191/191 [00:00<00:00, 8697.15 examples/s]\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:52:18,065 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:52:18,750 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:52:20,880 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:52:20,880 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:52:20,880 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:52:20,880 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:52:20,880 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:52:20,880 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:52:20,880 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:52:20,882 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:52:20,882 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:52:20,882 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:52:20,882 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:52:20,882 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:52:20,901 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:36<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 16:56:57,703 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 276.9251, 'train_samples_per_second': 193.59, 'train_steps_per_second': 12.133, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:36<00:00, 12.13it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 16:56:57,809 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 16:56:57,810 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 16:57:00,580 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 16:57:00,581 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 16:57:00,581 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:36.92\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =     193.59\n",
            "  train_steps_per_second   =     12.133\n",
            "11/15/2023 16:57:00 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:57:00,595 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:57:00,597 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:57:00,597 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:57:00,597 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.66it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.17it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.91\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =     99.909\n",
            "  eval_steps_per_second   =     12.554\n",
            "11/15/2023 16:57:02 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 16:57:02,511 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 16:57:02,513 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 16:57:02,513 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 16:57:02,513 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.23it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.86\n",
            "  predict_samples_per_second =    102.612\n",
            "  predict_steps_per_second   =     12.894\n",
            "[INFO|modelcard.py:452] 2023-11-15 16:57:04,724 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 16:57:09.336454: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 16:57:09.336510: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 16:57:09.336549: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 16:57:10.457179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 16:57:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 16:57:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_16-57-12_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 16:57:13 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 16:57:13 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 16:57:13 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 16:57:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:57:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 16:57:14 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 16:57:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:57:14,413 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:57:14,417 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 16:57:14,735 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:57:14,998 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:57:14,998 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:57:15,576 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:57:15,576 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:57:15,576 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:57:15,576 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 16:57:15,576 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:57:15,577 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:57:15,577 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 16:57:15,607 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 16:57:15,608 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 16:57:15,639 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 16:57:18,649 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 16:57:18,649 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 16:57:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/191 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 16:57:18,709 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 16:57:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Running tokenizer on validation dataset: 100% 191/191 [00:00<00:00, 5063.12 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b65598bffd965cc.arrow\n",
            "11/15/2023 16:57:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b65598bffd965cc.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 16:57:22,379 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 16:57:23,058 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 16:57:25,255 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 16:57:25,255 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 16:57:25,255 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 16:57:25,255 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 16:57:25,255 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 16:57:25,255 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 16:57:25,255 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 16:57:25,257 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 16:57:25,257 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 16:57:25,257 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 16:57:25,257 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 16:57:25,257 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 16:57:25,275 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.34it/s][INFO|trainer.py:1967] 2023-11-15 17:02:11,652 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.5009, 'train_samples_per_second': 187.12, 'train_steps_per_second': 11.728, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:02:11,760 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:02:11,761 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:02:14,550 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:02:14,551 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:02:14,551 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.50\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =     187.12\n",
            "  train_steps_per_second   =     11.728\n",
            "11/15/2023 17:02:14 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:02:14,566 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:02:14,568 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:02:14,568 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:02:14,568 >>   Batch size = 8\n",
            " 96% 23/24 [00:01<00:00, 13.14it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 12.96it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.94\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =     98.384\n",
            "  eval_steps_per_second   =     12.362\n",
            "11/15/2023 17:02:16 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:02:16,512 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:02:16,514 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:02:16,514 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:02:16,514 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.98it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.89\n",
            "  predict_samples_per_second =    100.744\n",
            "  predict_steps_per_second   =     12.659\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:02:18,780 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:02:23.244240: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:02:23.244296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:02:23.244325: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:02:24.370533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:02:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:02:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-02-26_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:02:27 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:02:28 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:02:28 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:02:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:02:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:02:28 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:02:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:02:28,501 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:02:28,505 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:02:28,766 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:02:29,082 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:02:29,082 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:02:29,633 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:02:29,633 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:02:29,633 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:02:29,633 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:02:29,633 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:02:29,633 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:02:29,634 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:02:29,664 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:02:29,665 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:02:29,696 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:02:32,742 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:02:32,743 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:02:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:02:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/191 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 17:02:32,817 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:02:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "Running tokenizer on prediction dataset: 100% 191/191 [00:00<00:00, 6423.80 examples/s]\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:02:36,431 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:02:37,103 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:02:39,292 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:02:39,292 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:02:39,292 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:02:39,292 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:02:39,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:02:39,292 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:02:39,292 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:02:39,294 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:02:39,294 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:02:39,294 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:02:39,294 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:02:39,294 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:02:39,311 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.34it/s][INFO|trainer.py:1967] 2023-11-15 17:07:25,820 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.631, 'train_samples_per_second': 187.035, 'train_steps_per_second': 11.722, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:07:25,927 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:07:25,928 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:07:28,744 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:07:28,745 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:07:28,746 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.63\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.035\n",
            "  train_steps_per_second   =     11.722\n",
            "11/15/2023 17:07:28 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:07:28,760 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:07:28,762 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:07:28,762 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:07:28,762 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.53it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.15it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.91\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =      99.77\n",
            "  eval_steps_per_second   =     12.537\n",
            "11/15/2023 17:07:30 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:07:30,678 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:07:30,680 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:07:30,680 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:07:30,680 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.09it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.88\n",
            "  predict_samples_per_second =    101.578\n",
            "  predict_steps_per_second   =     12.764\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:07:32,916 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:07:37.439760: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:07:37.439817: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:07:37.439860: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:07:38.527529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:07:41 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:07:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-07-40_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:07:41 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:07:42 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:07:42 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:07:42 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:07:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:07:42 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:07:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:07:42,459 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:07:42,463 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:07:42,712 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:07:42,963 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:07:42,964 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:07:43,489 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:07:43,489 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:07:43,489 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:07:43,489 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:07:43,489 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:07:43,489 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:07:43,490 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:07:43,521 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:07:43,522 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:07:43,554 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:07:46,664 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:07:46,664 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:07:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:07:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:07:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:07:50,360 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:07:51,043 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:07:53,194 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:07:53,194 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:07:53,194 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:07:53,194 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:07:53,194 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:07:53,194 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:07:53,194 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:07:53,195 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:07:53,195 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:07:53,196 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:07:53,196 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:07:53,196 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:07:53,213 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:12:39,425 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.3353, 'train_samples_per_second': 187.228, 'train_steps_per_second': 11.734, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:12:39,533 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:12:39,534 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:12:43,406 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:12:43,407 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:12:43,407 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.33\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.228\n",
            "  train_steps_per_second   =     11.734\n",
            "11/15/2023 17:12:43 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:12:43,422 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:12:43,424 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:12:43,424 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:12:43,424 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.59it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.14it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.91\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =     99.655\n",
            "  eval_steps_per_second   =     12.522\n",
            "11/15/2023 17:12:45 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:12:45,343 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:12:45,345 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:12:45,345 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:12:45,345 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.11it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.87\n",
            "  predict_samples_per_second =    101.736\n",
            "  predict_steps_per_second   =     12.784\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:12:47,591 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:12:52.095874: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:12:52.095929: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:12:52.095958: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:12:53.193793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:12:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:12:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-12-55_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:12:55 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:12:56 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:12:56 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:12:56 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:12:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:12:56 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:12:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:12:57,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:12:57,059 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:12:57,322 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:12:57,589 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:12:57,590 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:12:58,157 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:12:58,157 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:12:58,157 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:12:58,157 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:12:58,157 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:12:58,157 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:12:58,158 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:12:58,189 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:12:58,190 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:12:58,224 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:13:01,279 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:13:01,279 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:13:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:13:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:13:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:13:05,275 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:13:05,943 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:13:08,145 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:13:08,145 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:13:08,145 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:13:08,145 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:13:08,145 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:13:08,146 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:13:08,146 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:13:08,147 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:13:08,147 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:13:08,147 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:13:08,147 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:13:08,147 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:13:08,166 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:17:54,655 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.6124, 'train_samples_per_second': 187.047, 'train_steps_per_second': 11.723, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:17:54,762 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:17:54,763 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:18:00,216 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:18:00,217 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:18:00,217 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.61\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.047\n",
            "  train_steps_per_second   =     11.723\n",
            "11/15/2023 17:18:00 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:18:00,231 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:18:00,234 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:18:00,234 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:18:00,234 >>   Batch size = 8\n",
            " 96% 23/24 [00:01<00:00, 13.34it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.13it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.91\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =     99.567\n",
            "  eval_steps_per_second   =     12.511\n",
            "11/15/2023 17:18:02 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:18:02,154 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:18:02,156 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:18:02,156 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:18:02,156 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.39it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.83\n",
            "  predict_samples_per_second =    103.906\n",
            "  predict_steps_per_second   =     13.056\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:18:04,345 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:18:08.898921: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:18:08.898981: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:18:08.899008: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:18:09.971148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:18:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:18:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-18-12_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:18:12 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:18:13 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:18:13 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:18:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:18:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:18:13 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:18:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:18:14,047 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:18:14,051 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:18:14,311 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:18:14,582 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:18:14,583 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:18:15,096 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:18:15,096 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:18:15,096 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:18:15,096 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:18:15,096 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:18:15,096 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:18:15,097 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:18:15,128 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:18:15,129 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:18:15,162 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:18:18,191 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:18:18,191 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:18:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:18:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:18:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:18:21,866 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:18:22,550 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:18:24,728 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:18:24,728 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:18:24,728 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:18:24,728 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:18:24,728 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:18:24,728 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:18:24,728 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:18:24,730 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:18:24,730 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:18:24,730 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:18:24,730 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:18:24,730 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:18:24,748 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:23:11,087 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.4624, 'train_samples_per_second': 187.145, 'train_steps_per_second': 11.729, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:23:11,194 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:23:11,195 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:23:14,143 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:23:14,144 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:23:14,144 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.46\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.145\n",
            "  train_steps_per_second   =     11.729\n",
            "11/15/2023 17:23:14 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:23:14,159 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:23:14,161 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:23:14,161 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:23:14,161 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.68it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.19it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.90\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =    100.033\n",
            "  eval_steps_per_second   =      12.57\n",
            "11/15/2023 17:23:16 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:23:16,073 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:23:16,075 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:23:16,075 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:23:16,075 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.22it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.86\n",
            "  predict_samples_per_second =    102.429\n",
            "  predict_steps_per_second   =     12.871\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:23:18,277 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:23:23.090844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:23:23.090893: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:23:23.090927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:23:24.225692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:23:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:23:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-23-26_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:23:26 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:23:27 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:23:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:23:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:23:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:23:27 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:23:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:23:28,079 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:23:28,083 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:23:28,345 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:23:28,598 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:23:28,599 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:23:29,122 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:23:29,122 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:23:29,123 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:23:29,123 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:23:29,123 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:23:29,123 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:23:29,124 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:23:29,154 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:23:29,154 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:23:29,185 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:23:32,203 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:23:32,203 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:23:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:23:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:23:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:23:35,924 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:23:36,618 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:23:38,837 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:23:38,838 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:23:38,838 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:23:38,838 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:23:38,838 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:23:38,838 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:23:38,838 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:23:38,839 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:23:38,839 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:23:38,840 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:23:38,840 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:23:38,840 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:23:38,857 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:28:25,070 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.3347, 'train_samples_per_second': 187.228, 'train_steps_per_second': 11.735, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.73it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:28:25,176 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:28:25,178 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:28:35,073 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:28:35,074 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:28:35,074 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.33\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.228\n",
            "  train_steps_per_second   =     11.735\n",
            "11/15/2023 17:28:35 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:28:35,088 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:28:35,090 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:28:35,090 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:28:35,090 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.00it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.55it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.86\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =    102.667\n",
            "  eval_steps_per_second   =     12.901\n",
            "11/15/2023 17:28:36 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:28:36,953 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:28:36,954 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:28:36,955 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:28:36,955 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.45it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.82\n",
            "  predict_samples_per_second =    104.383\n",
            "  predict_steps_per_second   =     13.116\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:28:39,136 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:28:43.782913: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:28:43.782971: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:28:43.782999: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:28:44.862791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:28:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:28:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-28-46_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:28:47 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:28:48 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:28:48 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:28:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:28:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:28:48 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:28:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:28:48,706 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:28:48,710 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:28:48,987 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:28:49,244 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:28:49,244 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:28:49,771 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:28:49,771 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:28:49,771 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:28:49,771 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:28:49,771 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:28:49,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:28:49,772 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:28:49,803 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:28:49,804 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:28:49,835 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:28:52,833 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:28:52,833 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:28:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:28:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:28:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:28:56,521 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:28:57,207 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:28:59,397 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:28:59,397 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:28:59,397 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:28:59,397 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:28:59,397 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:28:59,397 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:28:59,397 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:28:59,399 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:28:59,399 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:28:59,399 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:28:59,399 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:28:59,399 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:28:59,417 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:33:45,979 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.685, 'train_samples_per_second': 187.0, 'train_steps_per_second': 11.72, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:33:46,086 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:33:46,087 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:33:48,935 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:33:48,936 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:33:48,936 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.68\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =      187.0\n",
            "  train_steps_per_second   =      11.72\n",
            "11/15/2023 17:33:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:33:48,951 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:33:48,953 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:33:48,953 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:33:48,953 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.66it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.20it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.90\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =    100.101\n",
            "  eval_steps_per_second   =     12.578\n",
            "11/15/2023 17:33:50 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:33:50,864 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:33:50,865 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:33:50,865 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:33:50,865 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.15it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.87\n",
            "  predict_samples_per_second =    102.028\n",
            "  predict_steps_per_second   =      12.82\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:33:53,094 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:33:57.773516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:33:57.773568: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:33:57.773596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:33:58.928182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:34:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:34:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-34-01_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:34:01 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:34:02 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:34:02 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:34:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:34:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:34:02 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:34:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:34:02,976 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:34:02,980 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:34:03,241 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:34:03,504 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:34:03,505 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:34:04,507 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:34:04,507 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:34:04,508 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:34:04,508 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:34:04,508 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:34:04,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:34:04,509 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:34:04,539 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:34:04,540 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:34:04,572 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:34:07,560 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:34:07,560 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:34:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:34:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:34:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:34:11,246 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:34:11,930 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:34:14,116 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:34:14,116 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:34:14,116 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:34:14,116 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:34:14,116 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:34:14,116 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:34:14,116 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:34:14,118 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:34:14,118 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:34:14,118 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:34:14,118 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:34:14,118 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:34:14,136 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:39:00,643 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.6298, 'train_samples_per_second': 187.036, 'train_steps_per_second': 11.722, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:39:00,750 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:39:00,751 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:39:04,913 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:39:04,914 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:39:04,914 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.62\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.036\n",
            "  train_steps_per_second   =     11.722\n",
            "11/15/2023 17:39:04 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:39:04,928 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:39:04,930 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:39:04,930 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:39:04,930 >>   Batch size = 8\n",
            " 96% 23/24 [00:01<00:00, 13.34it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.06it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.92\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =     99.099\n",
            "  eval_steps_per_second   =     12.452\n",
            "11/15/2023 17:39:06 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:39:06,860 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:39:06,862 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:39:06,862 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:39:06,862 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.30it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.85\n",
            "  predict_samples_per_second =    103.176\n",
            "  predict_steps_per_second   =     12.965\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:39:09,086 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n",
            "2023-11-15 17:39:13.606361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 17:39:13.606415: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 17:39:13.606448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 17:39:14.722771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 17:39:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 17:39:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_17-39-16_fdcd7d19b2d6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 17:39:17 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-3000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-3b4c78b92216a092\n",
            "11/15/2023 17:39:18 - INFO - datasets.builder - Using custom data configuration default-3b4c78b92216a092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 17:39:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 17:39:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:39:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 17:39:18 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 17:39:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:39:18,760 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:39:18,764 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 17:39:19,018 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:39:19,268 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:39:19,269 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:39:19,792 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:39:19,792 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:39:19,793 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:39:19,793 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 17:39:19,793 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:39:19,793 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:39:19,794 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 17:39:19,824 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 17:39:19,824 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 17:39:19,855 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 17:39:22,821 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 17:39:22,821 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "11/15/2023 17:39:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-45be657709cffc9d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "11/15/2023 17:39:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bad8789103ea0b13.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "11/15/2023 17:39:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-3b4c78b92216a092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4bc1045bb69d6b03.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 17:39:26,506 >> Loading model from /content/test-ner-2022/checkpoint-3000.\n",
            "[INFO|trainer.py:738] 2023-11-15 17:39:27,179 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 17:39:29,354 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 17:39:29,354 >>   Num examples = 1,787\n",
            "[INFO|trainer.py:1726] 2023-11-15 17:39:29,354 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 17:39:29,354 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 17:39:29,354 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 17:39:29,354 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 17:39:29,354 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1733] 2023-11-15 17:39:29,356 >>   Number of trainable parameters = 332,533,764\n",
            "[INFO|trainer.py:1753] 2023-11-15 17:39:29,356 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 17:39:29,356 >>   Continuing training from epoch 26\n",
            "[INFO|trainer.py:1755] 2023-11-15 17:39:29,356 >>   Continuing training from global step 3000\n",
            "[INFO|trainer.py:1757] 2023-11-15 17:39:29,356 >>   Will skip the first 26 epochs then the first 88 batches in the first epoch.\n",
            "  0% 0/3360 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 17:39:29,373 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 3360/3360 [04:46<00:00,  1.35it/s][INFO|trainer.py:1967] 2023-11-15 17:44:15,868 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 286.6178, 'train_samples_per_second': 187.044, 'train_steps_per_second': 11.723, 'train_loss': 0.0009299454944474356, 'epoch': 30.0}\n",
            "100% 3360/3360 [04:46<00:00, 11.72it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 17:44:15,976 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 17:44:15,977 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 17:44:19,051 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 17:44:19,052 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 17:44:19,052 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0009\n",
            "  train_runtime            = 0:04:46.61\n",
            "  train_samples            =       1787\n",
            "  train_samples_per_second =    187.044\n",
            "  train_steps_per_second   =     11.723\n",
            "11/15/2023 17:44:19 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:44:19,066 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:44:19,068 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:44:19,068 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:44:19,068 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 12.66it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 24/24 [00:01<00:00, 13.13it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.3176\n",
            "  eval_f1                 =     0.2894\n",
            "  eval_loss               =      4.762\n",
            "  eval_precision          =     0.1844\n",
            "  eval_recall             =      0.673\n",
            "  eval_runtime            = 0:00:01.91\n",
            "  eval_samples            =        191\n",
            "  eval_samples_per_second =     99.604\n",
            "  eval_steps_per_second   =     12.516\n",
            "11/15/2023 17:44:20 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 17:44:20,989 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 17:44:20,990 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 17:44:20,990 >>   Num examples = 191\n",
            "[INFO|trainer.py:3178] 2023-11-15 17:44:20,991 >>   Batch size = 8\n",
            "100% 24/24 [00:01<00:00, 13.13it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.3176\n",
            "  predict_f1                 =     0.2894\n",
            "  predict_loss               =      4.762\n",
            "  predict_precision          =     0.1844\n",
            "  predict_recall             =      0.673\n",
            "  predict_runtime            = 0:00:01.87\n",
            "  predict_samples_per_second =    101.864\n",
            "  predict_steps_per_second   =       12.8\n",
            "[INFO|modelcard.py:452] 2023-11-15 17:44:23,218 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.18437679494543366}, {'name': 'Recall', 'type': 'recall', 'value': 0.6729559748427673}, {'name': 'F1', 'type': 'f1', 'value': 0.2894499549143372}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3175918254625794}]}\n",
            "{'preds':                                                  preds\n",
            "0                                   C,C,C,C,C,C,C,CT,E\n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E\n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...\n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E\n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E\n",
            "..                                                 ...\n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...\n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...\n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...\n",
            "\n",
            "[191 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0                                    C,C,C,C,C,C,C,O,E   \n",
            "1                    C,C,C,O,E,E,E,E,O,C,C,C,O,E,E,E,E   \n",
            "2    C,C,C,O,E,E,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "3                      C,C,C,O,E,O,E,O,O,O,O,O,O,O,O,O   \n",
            "4                                C,C,C,O,O,E,O,E,O,O,O   \n",
            "..                                                 ...   \n",
            "186  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,C,C,O,...   \n",
            "187  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,O,...   \n",
            "188  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "189  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...   \n",
            "\n",
            "                                                 preds  \n",
            "0                                   C,C,C,C,C,C,C,CT,E  \n",
            "1                    C,C,C,E,E,E,E,E,C,C,E,C,E,E,E,E,E  \n",
            "2    O,C,C,CT,E,E,E,E,E,E,O,O,O,O,O,O,E,E,E,E,E,E,E...  \n",
            "3                     O,C,C,CT,E,E,E,E,E,E,E,E,E,E,E,E  \n",
            "4                              O,C,C,CT,CT,E,E,E,E,E,E  \n",
            "..                                                 ...  \n",
            "186  CT,O,C,C,C,C,C,C,C,C,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "187  O,E,E,E,E,E,E,E,E,O,O,O,O,O,O,O,O,O,O,O,O,E,CT...  \n",
            "188  O,C,C,C,O,C,O,C,O,O,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "189  O,O,O,O,O,O,O,CT,O,E,E,E,E,E,E,E,E,E,E,E,E,E,E...  \n",
            "190  O,O,O,O,O,O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,...  \n",
            "\n",
            "[191 rows x 2 columns]\n",
            "(3621,)\n",
            "(3621,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.29      0.81      0.42       465\n",
            "          CT       0.00      0.00      0.00        25\n",
            "           E       0.21      0.70      0.32       468\n",
            "           O       0.81      0.17      0.28      2663\n",
            "\n",
            "    accuracy                           0.32      3621\n",
            "   macro avg       0.33      0.42      0.26      3621\n",
            "weighted avg       0.66      0.32      0.30      3621\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################Organizational as train and FinCausal as test #######################"
      ],
      "metadata": {
        "id": "lvzSvstYHWLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_fincausal_test = pd.read_json('/content/Fincausal-test.json', lines=True)"
      ],
      "metadata": {
        "id": "Imj46uLMIDBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and rename relevant columns to X (input) and y (output)\n",
        "dataset_test = data_fincausal_test[[\"words\", \"ner\"]].rename(columns={\"words\": \"A\", \"ner\": \"B\"})\n",
        "# load each row's data as a list of strings\n",
        "s1, s2 = [json.dumps(tokens_test) for tokens_test in dataset_test.A.values], [json.dumps(labels_test) for labels_test in dataset_test.B.values]"
      ],
      "metadata": {
        "id": "qZa6aQC7JD_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw_test, y_raw_test = [json.loads(token_test) for token_test in s1], [json.loads(label_test) for label_test in s2]\n"
      ],
      "metadata": {
        "id": "kQ6VtchpJEFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ners2 = []\n",
        "for i,j in zip(X_raw_test, y_raw_test):\n",
        "  _new_ner = {'words': i, 'ner': j}\n",
        "  new_ners2.append(_new_ner)"
      ],
      "metadata": {
        "id": "0c0CQlFvJNBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners2[i] for i in range(len(new_ners2))]\n",
        "dump_json_lines(test,  '/content/dev.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgpiW14RJSzV",
        "outputId": "96789c25-ec39-4267-f3dd-5bd1b5eaf991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = [new_ners2[i] for i in range(len(new_ners2))]\n",
        "dump_json_lines(test,  '/content/test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwOe14MKJPnw",
        "outputId": "fc7f301f-0689-4708-dc88-90856e6d2d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_json('/content/test.json',lines=True)"
      ],
      "metadata": {
        "id": "ZocfRkFNJVqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['liststring'] = [','.join(map(str, l)) for l in data_test['ner']]"
      ],
      "metadata": {
        "id": "-tPh2ihcJX0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  !python run_ner.py --model_name_or_path SpanBERT/spanbert-large-cased --train_file /content/train.json --validation_file /content/dev.json --test_file /content/test.json  \\\n",
        "           --output_dir /content/test-ner-2022 --do_train --do_eval --do_predict --learning_rate 5e-5 --num_train_epochs 30 --weight_decay 0 --per_device_train_batch_size 16\n",
        "\n",
        "  data_preds = pd.read_csv('/content/test-ner-2022/predictions.txt', sep='\\t',names=['preds'])\n",
        "\n",
        "  data_preds = data_preds.apply(lambda x: x.str.replace(' ',','))\n",
        "  preds = {'preds':data_preds}\n",
        "  print(preds)\n",
        "\n",
        "  #data_test = pd.read_json('/content/test.json',lines=True)\n",
        "\n",
        "  data_test_final = pd.concat([data_test['liststring'],data_preds],axis=1)\n",
        "\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.replace('[',''))\n",
        "  # data_test_final = data_test_final.apply(lambda x: x.str.replace(']',''))\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace('[','')\n",
        "  # data_test_final['ner'] = data_test_final['ner'].replace(']','')\n",
        "  print(data_test_final)\n",
        "\n",
        "  data_spanbert_gold_preds = data_test_final\n",
        "\n",
        "  flat_list = [i.strip() for i in data_spanbert_gold_preds['liststring'] for i in i.split(',')]\n",
        "\n",
        "  flat_list_preds = [i.strip() for i in data_spanbert_gold_preds['preds'] for i in i.split(',')]\n",
        "\n",
        " # flat_list_words = [i.strip() for i in data_spanbert_gold_preds['words'] for i in i.split(',')]\n",
        "\n",
        "  print(np.shape(flat_list))\n",
        "  print(np.shape(flat_list_preds))\n",
        " # print(np.shape(flat_list_words))\n",
        "\n",
        "  df_flatlist = pd.DataFrame(flat_list,columns=['gold'])\n",
        "  df_finallist = pd.DataFrame(flat_list_preds,columns=['preds'])\n",
        " # df_finallist_words = pd.DataFrame(flat_list_words,columns=['words'])\n",
        "\n",
        "  df_combine = pd.concat([df_flatlist,df_finallist],axis=1)\n",
        "\n",
        "  print(classification_report(flat_list, flat_list_preds, zero_division = 1)) #, target_names=target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGZSGCQDJEQV",
        "outputId": "eedcdf26-b55d-491b-dce7-92830ce7cb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-15 19:50:49.753767: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 19:50:49.753813: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 19:50:49.753860: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 19:50:50.851135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 19:50:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 19:50:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_19-50-53_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 19:50:53 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 19:50:54 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 19:50:54 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 19:50:54 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:50:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 19:50:54 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:50:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:50:55,325 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:50:55,333 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 19:50:55,594 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:50:55,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:50:55,846 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:50:56,360 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:50:56,360 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:50:56,361 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:50:56,361 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:50:56,361 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:50:56,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:50:56,362 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:50:56,393 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:50:56,394 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 19:50:56,484 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 19:51:01,865 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 19:51:01,865 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 19:51:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/265 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 19:51:01,930 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 19:51:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Running tokenizer on validation dataset: 100% 265/265 [00:00<00:00, 3545.51 examples/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4c4175a836d2e814.arrow\n",
            "11/15/2023 19:51:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4c4175a836d2e814.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 19:51:05,670 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 19:51:06,308 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 19:51:08,558 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 19:51:08,558 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 19:51:08,558 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 19:51:08,558 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 19:51:08,558 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 19:51:08,558 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 19:51:08,558 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 19:51:08,560 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 19:51:08,560 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 19:51:08,560 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 19:51:08,560 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 19:51:08,560 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 19:51:08,580 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4196/4200 [02:18<00:01,  2.04it/s][INFO|trainer.py:1967] 2023-11-15 19:53:29,999 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 141.5453, 'train_samples_per_second': 473.488, 'train_steps_per_second': 29.672, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:21<00:00, 29.67it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 19:53:30,108 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 19:53:30,109 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 19:53:33,048 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 19:53:33,049 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 19:53:33,049 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:21.54\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    473.488\n",
            "  train_steps_per_second   =     29.672\n",
            "11/15/2023 19:53:33 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 19:53:33,062 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 19:53:33,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 19:53:33,064 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 19:53:33,065 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.02it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.85\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.226\n",
            "  eval_steps_per_second   =      5.803\n",
            "11/15/2023 19:53:38 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 19:53:38,926 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 19:53:38,928 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 19:53:38,928 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 19:53:38,928 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.02it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.83\n",
            "  predict_samples_per_second =     45.451\n",
            "  predict_steps_per_second   =      5.831\n",
            "[INFO|modelcard.py:452] 2023-11-15 19:53:45,033 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 19:53:49.523498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 19:53:49.523552: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 19:53:49.523580: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 19:53:50.592822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 19:53:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 19:53:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_19-53-52_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 19:53:53 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 19:53:54 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 19:53:54 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 19:53:54 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:53:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 19:53:54 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:53:54 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:53:54,527 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:53:54,532 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 19:53:54,792 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:53:55,050 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:53:55,051 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:53:55,573 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:53:55,573 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:53:55,573 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:53:55,573 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:53:55,573 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:53:55,574 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:53:55,574 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:53:55,604 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:53:55,605 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 19:53:55,636 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 19:53:58,595 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 19:53:58,595 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 19:53:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 19:53:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Running tokenizer on prediction dataset:   0% 0/265 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2698] 2023-11-15 19:53:58,671 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 19:53:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "Running tokenizer on prediction dataset: 100% 265/265 [00:00<00:00, 4193.18 examples/s]\n",
            "[INFO|trainer.py:2078] 2023-11-15 19:54:02,348 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 19:54:03,025 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 19:54:05,300 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 19:54:05,300 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 19:54:05,300 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 19:54:05,300 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 19:54:05,300 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 19:54:05,300 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 19:54:05,301 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 19:54:05,302 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 19:54:05,302 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 19:54:05,302 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 19:54:05,302 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 19:54:05,302 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 19:54:05,322 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4197/4200 [02:20<00:01,  2.02it/s][INFO|trainer.py:1967] 2023-11-15 19:56:27,787 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.5909, 'train_samples_per_second': 470.016, 'train_steps_per_second': 29.455, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.46it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 19:56:27,895 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 19:56:27,896 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 19:56:30,915 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 19:56:30,916 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 19:56:30,916 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.59\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    470.016\n",
            "  train_steps_per_second   =     29.455\n",
            "11/15/2023 19:56:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 19:56:30,930 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 19:56:30,932 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 19:56:30,932 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 19:56:30,932 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.08it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.06it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.85\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.265\n",
            "  eval_steps_per_second   =      5.808\n",
            "11/15/2023 19:56:36 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 19:56:36,789 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 19:56:36,791 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 19:56:36,791 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 19:56:36,791 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.05it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.79\n",
            "  predict_samples_per_second =     45.717\n",
            "  predict_steps_per_second   =      5.866\n",
            "[INFO|modelcard.py:452] 2023-11-15 19:56:42,859 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 19:56:47.389323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 19:56:47.389381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 19:56:47.389410: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 19:56:48.466971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 19:56:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 19:56:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_19-56-50_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 19:56:50 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 19:56:51 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 19:56:51 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 19:56:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:56:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 19:56:51 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:56:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:56:52,348 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:56:52,352 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 19:56:52,632 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:56:52,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:56:52,896 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:56:53,407 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:56:53,407 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:56:53,407 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:56:53,407 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:56:53,407 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:56:53,408 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:56:53,408 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:56:53,438 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:56:53,439 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 19:56:53,469 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 19:56:56,492 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 19:56:56,492 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 19:56:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 19:56:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 19:56:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 19:57:00,207 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 19:57:00,855 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 19:57:02,980 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 19:57:02,980 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 19:57:02,980 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 19:57:02,980 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 19:57:02,980 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 19:57:02,980 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 19:57:02,980 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 19:57:02,982 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 19:57:02,982 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 19:57:02,982 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 19:57:02,982 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 19:57:02,982 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 19:57:03,001 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4194/4200 [02:18<00:02,  2.22it/s][INFO|trainer.py:1967] 2023-11-15 19:59:25,572 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.6958, 'train_samples_per_second': 469.67, 'train_steps_per_second': 29.433, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.43it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 19:59:25,680 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 19:59:25,681 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 19:59:28,749 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 19:59:28,750 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 19:59:28,750 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.69\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =     469.67\n",
            "  train_steps_per_second   =     29.433\n",
            "11/15/2023 19:59:28 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 19:59:28,764 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 19:59:28,766 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 19:59:28,766 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 19:59:28,766 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.03it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.06it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.87\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.128\n",
            "  eval_steps_per_second   =       5.79\n",
            "11/15/2023 19:59:34 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 19:59:34,641 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 19:59:34,642 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 19:59:34,643 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 19:59:34,643 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.01it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.83\n",
            "  predict_samples_per_second =     45.444\n",
            "  predict_steps_per_second   =      5.831\n",
            "[INFO|modelcard.py:452] 2023-11-15 19:59:40,750 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 19:59:45.370964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 19:59:45.371020: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 19:59:45.371060: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 19:59:46.472822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 19:59:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 19:59:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_19-59-48_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 19:59:49 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 19:59:50 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 19:59:50 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 19:59:50 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:59:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 19:59:50 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 19:59:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:59:50,460 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:59:50,464 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 19:59:50,724 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:59:50,984 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:59:50,985 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:59:51,506 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:59:51,506 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:59:51,506 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:59:51,506 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 19:59:51,506 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:59:51,507 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:59:51,507 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 19:59:51,539 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 19:59:51,540 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 19:59:51,574 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 19:59:54,521 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 19:59:54,522 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 19:59:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 19:59:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 19:59:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 19:59:58,308 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 19:59:58,950 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:00:01,112 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:00:01,112 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:00:01,112 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:00:01,112 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:00:01,112 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:00:01,112 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:00:01,112 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:00:01,114 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:00:01,114 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:00:01,114 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:00:01,114 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:00:01,114 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:00:01,135 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4198/4200 [02:21<00:00,  2.22it/s][INFO|trainer.py:1967] 2023-11-15 20:02:23,721 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7119, 'train_samples_per_second': 469.618, 'train_steps_per_second': 29.43, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.43it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:02:23,828 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:02:23,829 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:02:26,534 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:02:26,535 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:02:26,535 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.71\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.618\n",
            "  train_steps_per_second   =      29.43\n",
            "11/15/2023 20:02:26 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:02:26,549 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:02:26,551 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:02:26,551 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:02:26,551 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.08it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.85\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.239\n",
            "  eval_steps_per_second   =      5.804\n",
            "11/15/2023 20:02:32 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:02:32,411 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:02:32,413 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:02:32,413 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:02:32,413 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.01it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.83\n",
            "  predict_samples_per_second =     45.448\n",
            "  predict_steps_per_second   =      5.831\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:02:38,524 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 20:02:43.349785: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 20:02:43.349850: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 20:02:43.349890: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 20:02:44.487536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 20:02:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 20:02:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_20-02-46_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 20:02:47 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 20:02:48 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 20:02:48 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 20:02:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:02:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 20:02:48 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:02:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:02:48,550 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:02:48,554 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 20:02:48,818 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:02:49,077 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:02:49,077 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:02:49,600 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:02:49,600 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:02:49,600 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:02:49,600 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:02:49,600 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:02:49,600 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:02:49,601 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:02:49,632 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:02:49,633 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 20:02:49,666 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 20:02:52,765 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 20:02:52,765 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 20:02:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 20:02:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 20:02:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 20:02:56,476 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 20:02:57,133 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:02:59,344 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:02:59,344 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:02:59,344 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:02:59,344 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:02:59,344 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:02:59,344 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:02:59,344 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:02:59,345 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:02:59,345 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:02:59,346 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:02:59,346 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:02:59,346 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:02:59,365 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4198/4200 [02:21<00:00,  2.22it/s][INFO|trainer.py:1967] 2023-11-15 20:05:21,944 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7038, 'train_samples_per_second': 469.644, 'train_steps_per_second': 29.432, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.43it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:05:22,051 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:05:22,053 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:05:25,055 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:05:25,056 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:05:25,056 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.70\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.644\n",
            "  train_steps_per_second   =     29.432\n",
            "11/15/2023 20:05:25 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:05:25,071 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:05:25,073 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:05:25,073 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:05:25,073 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.05it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.85\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.261\n",
            "  eval_steps_per_second   =      5.807\n",
            "11/15/2023 20:05:30 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:05:30,931 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:05:30,933 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:05:30,933 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:05:30,933 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  5.99it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.85\n",
            "  predict_samples_per_second =     45.231\n",
            "  predict_steps_per_second   =      5.803\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:05:37,062 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 20:05:41.843857: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 20:05:41.843916: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 20:05:41.843947: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 20:05:42.999324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 20:05:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 20:05:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_20-05-45_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 20:05:45 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 20:05:46 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 20:05:46 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 20:05:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:05:46 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 20:05:46 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:05:46 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:05:46,976 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:05:46,980 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 20:05:47,232 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:05:47,483 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:05:47,484 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:05:48,004 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:05:48,004 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:05:48,004 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:05:48,004 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:05:48,004 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:05:48,005 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:05:48,005 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:05:48,037 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:05:48,038 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 20:05:48,071 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 20:05:51,083 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 20:05:51,083 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 20:05:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 20:05:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 20:05:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 20:05:54,857 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 20:05:55,516 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:05:57,775 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:05:57,775 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:05:57,775 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:05:57,775 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:05:57,775 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:05:57,775 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:05:57,775 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:05:57,777 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:05:57,777 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:05:57,777 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:05:57,777 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:05:57,777 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:05:57,800 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4194/4200 [02:18<00:02,  2.22it/s][INFO|trainer.py:1967] 2023-11-15 20:08:20,448 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7763, 'train_samples_per_second': 469.406, 'train_steps_per_second': 29.417, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.42it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:08:20,555 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:08:20,556 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:08:23,528 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:08:23,529 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:08:23,530 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.77\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.406\n",
            "  train_steps_per_second   =     29.417\n",
            "11/15/2023 20:08:23 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:08:23,544 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:08:23,546 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:08:23,547 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:08:23,547 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.04it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.86\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.179\n",
            "  eval_steps_per_second   =      5.797\n",
            "11/15/2023 20:08:29 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:08:29,414 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:08:29,416 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:08:29,417 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:08:29,417 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.01it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.83\n",
            "  predict_samples_per_second =     45.406\n",
            "  predict_steps_per_second   =      5.826\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:08:35,531 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 20:08:40.345429: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 20:08:40.345486: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 20:08:40.345524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 20:08:41.509105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 20:08:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 20:08:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_20-08-43_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 20:08:44 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 20:08:45 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 20:08:45 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 20:08:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:08:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 20:08:45 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:08:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:08:45,554 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:08:45,558 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 20:08:45,820 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:08:46,081 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:08:46,082 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:08:46,601 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:08:46,601 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:08:46,601 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:08:46,601 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:08:46,601 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:08:46,601 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:08:46,602 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:08:46,633 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:08:46,634 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 20:08:46,666 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 20:08:49,680 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 20:08:49,680 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 20:08:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 20:08:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 20:08:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 20:08:53,474 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 20:08:54,134 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:08:56,341 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:08:56,341 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:08:56,341 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:08:56,341 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:08:56,341 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:08:56,341 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:08:56,341 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:08:56,342 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:08:56,343 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:08:56,343 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:08:56,343 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:08:56,343 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:08:56,363 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4194/4200 [02:18<00:02,  2.22it/s][INFO|trainer.py:1967] 2023-11-15 20:11:18,988 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7499, 'train_samples_per_second': 469.493, 'train_steps_per_second': 29.422, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.42it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:11:19,095 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:11:19,096 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:11:22,165 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:11:22,166 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:11:22,167 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.74\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.493\n",
            "  train_steps_per_second   =     29.422\n",
            "11/15/2023 20:11:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:11:22,184 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:11:22,187 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:11:22,187 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:11:22,187 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.02it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.05it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.86\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.171\n",
            "  eval_steps_per_second   =      5.796\n",
            "11/15/2023 20:11:28 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:11:28,056 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:11:28,058 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:11:28,058 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:11:28,058 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.01it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.83\n",
            "  predict_samples_per_second =     45.396\n",
            "  predict_steps_per_second   =      5.824\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:11:34,232 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 20:11:39.184251: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 20:11:39.184309: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 20:11:39.184338: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 20:11:40.320917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 20:11:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 20:11:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_20-11-42_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 20:11:42 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 20:11:43 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 20:11:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 20:11:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:11:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 20:11:43 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:11:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:11:44,305 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:11:44,309 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 20:11:44,570 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:11:44,830 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:11:44,831 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:11:45,354 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:11:45,354 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:11:45,354 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:11:45,354 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:11:45,355 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:11:45,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:11:45,355 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:11:45,386 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:11:45,387 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 20:11:45,419 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 20:11:48,458 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 20:11:48,459 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 20:11:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 20:11:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 20:11:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 20:11:52,295 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 20:11:52,967 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:11:55,203 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:11:55,203 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:11:55,203 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:11:55,203 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:11:55,203 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:11:55,203 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:11:55,203 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:11:55,204 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:11:55,205 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:11:55,205 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:11:55,205 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:11:55,205 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:11:55,224 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4194/4200 [02:18<00:02,  2.23it/s][INFO|trainer.py:1967] 2023-11-15 20:14:17,811 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7117, 'train_samples_per_second': 469.618, 'train_steps_per_second': 29.43, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.43it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:14:17,918 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:14:17,919 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:14:20,928 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:14:20,929 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:14:20,929 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.71\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.618\n",
            "  train_steps_per_second   =      29.43\n",
            "11/15/2023 20:14:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:14:20,944 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:14:20,946 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:14:20,946 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:14:20,946 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.03it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.05it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.89\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     44.965\n",
            "  eval_steps_per_second   =      5.769\n",
            "11/15/2023 20:14:26 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:14:26,843 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:14:26,845 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:14:26,845 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:14:26,845 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.02it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.82\n",
            "  predict_samples_per_second =     45.488\n",
            "  predict_steps_per_second   =      5.836\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:14:32,943 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 20:14:37.566839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 20:14:37.566896: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 20:14:37.566930: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 20:14:38.681951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 20:14:41 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 20:14:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_20-14-40_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 20:14:41 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 20:14:42 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 20:14:42 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 20:14:42 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:14:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 20:14:42 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:14:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:14:42,709 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:14:42,713 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 20:14:42,965 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:14:43,224 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:14:43,225 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:14:43,752 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:14:43,752 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:14:43,752 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:14:43,752 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:14:43,752 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:14:43,752 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:14:43,753 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:14:43,784 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:14:43,785 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 20:14:43,818 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 20:14:46,929 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 20:14:46,929 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 20:14:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 20:14:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 20:14:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 20:14:50,800 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 20:14:51,448 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:14:53,631 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:14:53,631 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:14:53,631 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:14:53,631 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:14:53,631 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:14:53,631 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:14:53,631 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:14:53,632 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:14:53,633 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:14:53,633 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:14:53,633 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:14:53,633 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:14:53,652 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4198/4200 [02:21<00:00,  2.22it/s][INFO|trainer.py:1967] 2023-11-15 20:17:16,297 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7703, 'train_samples_per_second': 469.425, 'train_steps_per_second': 29.418, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.42it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:17:16,405 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:17:16,406 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:17:19,415 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:17:19,416 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:17:19,416 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.77\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.425\n",
            "  train_steps_per_second   =     29.418\n",
            "11/15/2023 20:17:19 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:17:19,430 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:17:19,432 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:17:19,432 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:17:19,432 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.06it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.87\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.083\n",
            "  eval_steps_per_second   =      5.784\n",
            "11/15/2023 20:17:25 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:17:25,312 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner, words. If ner, words are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:17:25,314 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:17:25,314 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:17:25,314 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.01it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.83\n",
            "  predict_samples_per_second =     45.449\n",
            "  predict_steps_per_second   =      5.831\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:17:31,419 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n",
            "2023-11-15 20:17:35.942214: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 20:17:35.942263: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 20:17:35.942291: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 20:17:37.007112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/15/2023 20:17:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/15/2023 20:17:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-ner-2022/runs/Nov15_20-17-39_68af0c1e1fc4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/test-ner-2022,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-ner-2022,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/15/2023 20:17:39 - INFO - __main__ - Checkpoint detected, resuming training at /content/test-ner-2022/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "Using custom data configuration default-f9a6527f6cbfdc82\n",
            "11/15/2023 20:17:40 - INFO - datasets.builder - Using custom data configuration default-f9a6527f6cbfdc82\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/15/2023 20:17:40 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/15/2023 20:17:40 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:17:40 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/15/2023 20:17:40 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/15/2023 20:17:40 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:17:40,998 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:17:41,002 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-15 20:17:41,267 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:17:41,528 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:17:41,529 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:17:42,067 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:17:42,067 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:17:42,067 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:17:42,067 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2028] 2023-11-15 20:17:42,067 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:17:42,067 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:17:42,068 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-11-15 20:17:42,098 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-15 20:17:42,099 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"SpanBERT/spanbert-large-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3121] 2023-11-15 20:17:42,132 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--SpanBERT--spanbert-large-cased/snapshots/a49cba45de9565a5d3e7b089a94dbae679e64e79/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3950] 2023-11-15 20:17:45,199 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-15 20:17:45,199 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "11/15/2023 20:17:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8e59b4ea2b52024d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "11/15/2023 20:17:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb7f80053e89b268.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "11/15/2023 20:17:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f9a6527f6cbfdc82/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-372281f0f69efc2c.arrow\n",
            "[INFO|trainer.py:2078] 2023-11-15 20:17:49,073 >> Loading model from /content/test-ner-2022/checkpoint-4000.\n",
            "[INFO|trainer.py:738] 2023-11-15 20:17:49,716 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-15 20:17:51,929 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-15 20:17:51,930 >>   Num examples = 2,234\n",
            "[INFO|trainer.py:1726] 2023-11-15 20:17:51,930 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1727] 2023-11-15 20:17:51,930 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1730] 2023-11-15 20:17:51,930 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1731] 2023-11-15 20:17:51,930 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-15 20:17:51,930 >>   Total optimization steps = 4,200\n",
            "[INFO|trainer.py:1733] 2023-11-15 20:17:51,931 >>   Number of trainable parameters = 332,532,739\n",
            "[INFO|trainer.py:1753] 2023-11-15 20:17:51,931 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1754] 2023-11-15 20:17:51,931 >>   Continuing training from epoch 28\n",
            "[INFO|trainer.py:1755] 2023-11-15 20:17:51,931 >>   Continuing training from global step 4000\n",
            "[INFO|trainer.py:1757] 2023-11-15 20:17:51,931 >>   Will skip the first 28 epochs then the first 80 batches in the first epoch.\n",
            "  0% 0/4200 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-11-15 20:17:51,951 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 4194/4200 [02:18<00:02,  2.23it/s][INFO|trainer.py:1967] 2023-11-15 20:20:14,492 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.6662, 'train_samples_per_second': 469.768, 'train_steps_per_second': 29.439, 'train_loss': 0.00045399194671994164, 'epoch': 30.0}\n",
            "100% 4200/4200 [02:22<00:00, 29.44it/s]\n",
            "[INFO|trainer.py:2896] 2023-11-15 20:20:14,600 >> Saving model checkpoint to /content/test-ner-2022\n",
            "[INFO|configuration_utils.py:461] 2023-11-15 20:20:14,601 >> Configuration saved in /content/test-ner-2022/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-15 20:20:17,637 >> Model weights saved in /content/test-ner-2022/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-15 20:20:17,638 >> tokenizer config file saved in /content/test-ner-2022/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-15 20:20:17,639 >> Special tokens file saved in /content/test-ner-2022/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.0005\n",
            "  train_runtime            = 0:02:22.66\n",
            "  train_samples            =       2234\n",
            "  train_samples_per_second =    469.768\n",
            "  train_steps_per_second   =     29.439\n",
            "11/15/2023 20:20:17 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:20:17,653 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:20:17,655 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:20:17,655 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:20:17,655 >>   Batch size = 8\n",
            " 97% 33/34 [00:05<00:00,  7.08it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "100% 34/34 [00:05<00:00,  6.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.6263\n",
            "  eval_f1                 =      0.645\n",
            "  eval_loss               =     2.1367\n",
            "  eval_precision          =     0.6943\n",
            "  eval_recall             =     0.6023\n",
            "  eval_runtime            = 0:00:05.83\n",
            "  eval_samples            =        265\n",
            "  eval_samples_per_second =     45.387\n",
            "  eval_steps_per_second   =      5.823\n",
            "11/15/2023 20:20:23 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:738] 2023-11-15 20:20:23,496 >> The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3173] 2023-11-15 20:20:23,498 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3175] 2023-11-15 20:20:23,498 >>   Num examples = 265\n",
            "[INFO|trainer.py:3178] 2023-11-15 20:20:23,498 >>   Batch size = 8\n",
            "100% 34/34 [00:05<00:00,  6.04it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.6263\n",
            "  predict_f1                 =      0.645\n",
            "  predict_loss               =     2.1367\n",
            "  predict_precision          =     0.6943\n",
            "  predict_recall             =     0.6023\n",
            "  predict_runtime            = 0:00:05.80\n",
            "  predict_samples_per_second =     45.614\n",
            "  predict_steps_per_second   =      5.852\n",
            "[INFO|modelcard.py:452] 2023-11-15 20:20:29,578 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6942805265546982}, {'name': 'Recall', 'type': 'recall', 'value': 0.60228391415633}, {'name': 'F1', 'type': 'f1', 'value': 0.6450184501845019}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6262897067186611}]}\n",
            "{'preds':                                                  preds\n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...\n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...\n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E\n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C\n",
            "..                                                 ...\n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...\n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...\n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...\n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...\n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n",
            "\n",
            "[265 rows x 1 columns]}\n",
            "                                            liststring  \\\n",
            "0    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "1    E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,C,C,C,...   \n",
            "3    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C   \n",
            "4          E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,C,C,C,C,C   \n",
            "..                                                 ...   \n",
            "260  E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...   \n",
            "261  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "262  E,E,E,E,O,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...   \n",
            "263  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...   \n",
            "264  E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...   \n",
            "\n",
            "                                                 preds  \n",
            "0    O,O,O,O,O,O,E,E,E,E,E,E,E,E,E,E,O,E,E,E,E,E,E,...  \n",
            "1    O,E,E,E,E,E,E,E,O,O,O,O,C,C,C,O,O,O,C,C,C,C,C,...  \n",
            "2    E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "3    C,C,C,O,O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,E,E,E,E,E  \n",
            "4          O,O,O,O,O,O,O,O,O,E,E,E,E,E,E,O,O,C,C,C,C,C  \n",
            "..                                                 ...  \n",
            "260  O,O,O,E,E,E,E,E,E,E,E,E,E,E,O,O,O,O,C,C,C,C,C,...  \n",
            "261  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,...  \n",
            "262  E,E,E,E,E,O,O,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,C,...  \n",
            "263  O,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,E,O,...  \n",
            "264  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
            "\n",
            "[265 rows x 2 columns]\n",
            "(12309,)\n",
            "(12309,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.91      0.62      0.74      6590\n",
            "           E       0.70      0.61      0.65      5079\n",
            "           O       0.15      0.81      0.26       640\n",
            "\n",
            "    accuracy                           0.63     12309\n",
            "   macro avg       0.59      0.68      0.55     12309\n",
            "weighted avg       0.78      0.63      0.68     12309\n",
            "\n"
          ]
        }
      ]
    }
  ]
}